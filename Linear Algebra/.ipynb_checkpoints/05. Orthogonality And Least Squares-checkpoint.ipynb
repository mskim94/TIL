{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Inner Product, Length, and Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 정리\n",
    "- Inner Product: $u$ $\\cdot$ $v$ = ${u}^{T} \\cdot v$\n",
    "- unit vector: $\\left\\lVert{u}\\right\\rVert$ = 1\n",
    "- normalizing:  $u$ = $\\frac{1}{\\left\\lVert{v}\\right\\rVert}v$\n",
    "- Orthogonal Vector: 직교 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Theorem 1.\n",
    "Let $u$, $v$ and $w$ be vectors in $\\mathbb{R}^{n}$, and let $c$ be a scalar.\n",
    "- a. $u$ $\\cdot$ $v$ = $v$ $\\cdot$ $u$\n",
    "- b. ($u$ + $v$) $\\cdot$ $w$ = $u \\cdot w$ + $v \\cdot w$\n",
    "- c. ($cu$) $\\cdot$ $v$ = $c$ ($u \\cdot v$)\n",
    "- d. $u \\cdot u$ $\\geq 0$ and $u \\cdot u$ = 0 $\\Longleftrightarrow$ $u$ = 0\n",
    "\n",
    "#### Length of a Vector\n",
    "The **length** (or **norm**) of $v$ in $\\mathbb{R}^{n}$ is the nonnegative scalar $\\left\\lVert{v}\\right\\rVert$ defined by \n",
    "$\\left\\lVert{v}\\right\\rVert$ = $\\sqrt{v \\cdot v}$ = $\\sqrt{{v}_1^{2} + {v}_2^{2} + \\dots + {v}_n^{2}}$\n",
    "\n",
    "#### Distance in $\\mathbb{R}^n$\n",
    "$u$ 와 $v$가 $\\mathbb{R}^{n}$ 에 있고 $u$와 $v$사이의 거리는 **dist(u,v)** 쓰고 $u-v$의 **length**이다.\n",
    "- dist(u, v) = $\\left\\lVert{u - v}\\right\\rVert$\n",
    "$\\Longrightarrow$\n",
    "$u$ 와 $v$의 거리 = $u - v$의 length\n",
    "\n",
    "#### Orthogonal Vectors\n",
    "- Two vectors $u$ and $v$ in $\\mathbb{R}^{n}$ are **orthogonal** if $u \\cdot v = 0$\n",
    "- The **zero vector** is **orthogonal to every vector** in $\\mathbb{R}^{n}$\n",
    "\n",
    "### Theorem 2. Pythagorean Theorem\n",
    "Two vectors $u$ and $v$ are **orthogonal** $\\Longleftrightarrow$ ${\\left\\lVert{u+v}\\right\\rVert}^2$ = ${\\left\\lVert{u}\\right\\rVert}^2$ + ${\\left\\lVert{v}\\right\\rVert}^2$\n",
    "(${u} \\cdot v = 0$)\n",
    "\n",
    "#### Orthogonal Complements\n",
    "- $W$를 공간 $\\mathbb{R}^{3}$안의 원점을 지나는 평면이라고 하고, $L$을 $W$와 직교하하고 원점을 지나는 직선이라고 하면,\n",
    "if vector $z$는 공간 $\\mathbb{R}^{3}$ 안의 subspace $W$의 모든 vector에 대해 orthogonal하고, $z$는 orthogonal to $W$라고 한다.\n",
    "- $W$에 orthogonal한 vector z의 set인 set fof all vectors $z$는 **orthogonal complement of $W$** 라고 하고 ${W}^{\\perp}$로 표기한다. \n",
    "\n",
    "\n",
    "### Theorem 3.\n",
    "${(Row A)}^{\\perp}$ = $Nul A$ and ${(Col A)}^{\\perp}$ = $Nul {A}^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Orthogonal Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "#### Orthogonal Set\n",
    ": 공간 $\\mathbb{R}^{n}$의 orthogonal한 vector들의 set, set안의 모든 vector들이 orthogonal하다.\n",
    "\n",
    "\n",
    "### Theorem 4.\n",
    "$S$ = \\{${u}_1,...,{u}_p$\\}가 $\\mathbb{R}^n$안의 **nonzero vectors**의 orthogonal set이라면, S는 linearly independent이고 S에 의해 span되는 subspace의 **basis**이다.\n",
    "\n",
    "#### Orthogonal Basis\n",
    "공간 $\\mathbb{R}^n$의 subspace $W$에 **Orthogonal basis**는 $W$의 basis이고 orthogonal set이다.\n",
    "\n",
    "### Theorem 5.\n",
    "\\{${u}_1,...,{u}_p$\\}가 $\\mathbb{R}^n$의 subspace $W$에 대한 orthogonal basis 라 하자. $W$ 안의 $y$ 는 linear combination으로 표현할 수 있다.\n",
    "- $y = {c}_1{u}_1 + \\dots + {c}_p{u}_p$, ${c}_{j} = \\frac{y \\cdot {u}_{j}}{{u}_j \\cdot {u}_j}$\n",
    "- Orthogonal basis면 ${c}_j$(weight)가 주어져서 nice하다.\n",
    "\n",
    "#### Orthogonal Projection\n",
    "$B$ = \\{${u}_1,...,{u}_n$\\}이 공간 $\\mathbb{R}^n$의 orthogonal basis라 하자. \n",
    "- $y = {c}_1{u}_1 + \\dots + {c}_n{u}_n$으로 표현할수 있고\n",
    "$\\hat{y} = {proj}_Ly = \\frac{y \\cdot {u}_1}{{u}_1 \\cdot {u}_1}{u}_1$, $y = \\hat{y} + z$ ($z$: the component of y orthogonal to ${u}_1$)\n",
    "\n",
    "#### Orthonormal Sets\n",
    "$A$ = \\{${u}_1,...,{u}_n$\\}이 unit vectors의 orthogonal set이면, A를 **orthonormal set** 이라고 한다.\n",
    "\n",
    "### Theorem 6.\n",
    "m x n matrix $U$ 가 orthonormal columns을 가진다. $\\Longleftrightarrow$ ${U}^{T}U = I$\n",
    "\n",
    "### Theorem 7.\n",
    "Let $U$ be an m x n matrix with **orthonormal columns** , and let $x$ and $y$ be in $\\mathbb{R}^n$ 하면.\n",
    "- a. ${\\left\\lVert{Ux}\\right\\rVert}$ = ${\\left\\lVert{x}\\right\\rVert}$\n",
    "- b. ($Ux$) $\\cdot$ ($Uy$) = $x$ $\\cdot$ $y$\n",
    "- c. ($Ux$) $\\cdot$ ($Uy$) = 0 $\\Longleftrightarrow$ $x$ $\\cdot$ $y$ = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Orthogonal Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Theorem 8. The Orthogonal Decomposition Theorem\n",
    "$W$를 공간 $\\mathbb{R}^n$의 subspace라고 하자. 공간 $\\mathbb{R}^n$의 each $y$는 **uniquely**하게 $y = \\hat{y} + z$의 형태로 쓸 수 있다.\n",
    "- {${u}_1,...,{u}_p$}이 $W$의 orthogonal basis이면, $\\hat{y} = \\frac{y \\cdot {u}_1}{{u}_1 \\cdot {u}_1}{u}_1 + \\dot + \\frac{y \\cdot {u}_p}{{u}_1p \\cdot {u}_p}{u}_p$ and $z = y - \\hat{y}$\n",
    "- $\\hat{y}$ = ${proj}_{W}y$: the **orthogonal projection** fof $y$ onto $W$\n",
    "- If y is in $W$ = Span {{${u}_1,...,{u}_p$}}, then ${proj}_{W}y = y$\n",
    "\n",
    "#### A Geometric Interpretation of the Orthogonal Projection\n",
    "$\\mathbb{R}^3$ 공간에서 $y$를 ${u}_1$, ${u}_2$ 평면에 Projection 시킨 $\\hat{y}$는 $y$를 ${u}_2$에 projection 시킨 $\\hat{y}_2$과 $y$를 ${u}_1$에 projection 시킨 $\\hat{y}_1$ 시킨 vector들의 합이다. $\\Longrightarrow$ $\\hat{y} = \\hat{y}_1 + \\hat{y}_2$\n",
    "\n",
    "### Theorem 9. The Best Approximation Theorem\n",
    "$W$를 $\\mathbb{R}^n$의 subspace라고 하고, $\\mathbb{R}^n$ 공간 안의 어느 $y$ vector라 하면, $\\hat{y}$는 orthogonal projection of $y$ onto $W$. Then $\\hat{y}$ 에서 $y$까지의 **closest point**는 ${\\left\\lVert{y - \\hat{y}}\\right\\rVert}$이다.\n",
    "- ${\\left\\lVert{y - \\hat{y}}\\right\\rVert}$ < ${\\left\\lVert{y - v}\\right\\rVert}$\n",
    "\n",
    "### Theorem 10.\n",
    "$\\mathbb{R}^n$의 subspace $W$에 대한 orthonormal basis가 {${u}_1,...,{u}_p$}라면, ${proj}_{W}y$ = ($y \\cdot {u}_1){u}_1$ + ($y \\cdot {u}_2){u}_2$ + $\\dot$ + ($y \\cdot {u}_p){u}_p$로 linear equation으로 표현 가능하다.\n",
    "If U = \\[${u}_1 {u}_2 \\cdot {u}_p$\\], then ${proj}_{W}y$ = $U$ ${U}^{T}$ $y$ for all y in $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 The Gram-Schmidt Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Theorem 11. The Gram-Schmidt Process\n",
    "$\\mathbb{R}^n$의 subspace에 대한 basis를 \\{${x}_1,...,{x}_p$\\}로 주어지면, \n",
    "- ${v}_1 = {x}_1$\n",
    "- ${v}_2 = {x}_2 - \\frac{{x}_2 - {v}_1}{{x}_1 \\cdot {x}_1}{v}_1$\n",
    "- ${v}_3 = {x}_3 - \\frac{{x}_3 - {v}_1}{{x}_1 \\cdot {x}_1}{v}_1 - \\frac{{x}_3 - {v}_2}{{x}_2 \\cdot {x}_2}{v}_2$\n",
    "- ${v}_p = {x}_p - \\frac{{x}_p - {v}_1}{{x}_1 \\cdot {x}_1}{v}_1 - \\frac{{x}_p - {v}_2}{{x}_2 \\cdot {x}_2}{v}_2 - \\dots - \\frac{{x}_p - {v}_{p-1}}{{x}_{p-1} \\cdot {x}_{p-1}}{v}_{p-1}$\n",
    "\n",
    "Then \\{${v}_1,...,{v}_p$\\} is an **orthogonal basis** for $W$\n",
    "\n",
    "#### Orthonormal Bases\n",
    "Given \\{${v}_1,...,{v}_p$\\} orthogonal basis for $W$ **Normalize!**\n",
    "\n",
    "### Theorem 12. The $QR$ Factorization\n",
    "If A is an m x n matrix with linearly independent columns, A can be factored as $A = QR$, Q는 $Col A$의 basis: \\{${x}_1,...,{x}_n$\\}를 Gram-Schmidt를 이용하여 orthogonal basis로 만든 것이고 $R$은 n x n **upper triangular invertible matrix**이다.\n",
    "\n",
    "#### $QR$ Factorization Steps\n",
    "- 1. A의 column이 linear independent 확인\n",
    "- 2. $Col A$에 대한 orthonormal basis 찾는다. by the **Gram-Schmidt** algorithm\n",
    "- 3. Construct $Q$\n",
    "- 4. Find $R$ by ${Q}^{T}A = {Q}^{T}(QR) = IR = R$\n",
    "- if ${r}_{kk} < 0$, switch the sign of ${u}_k$ $\\to$ (${-u}_k$) and ${r}_{kk}$ to ${r}_{kn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Least-Squares Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "#### Least-Squares Solution\n",
    "${\\left\\lVert{b - A\\hat{x}}\\right\\rVert}$ $\\leq$ ${\\left\\lVert{b - Ax}\\right\\rVert}$를 만족하는 solution x가 least-square solution이다.\n",
    "\n",
    "### Theorem 13.\n",
    "The set of **least-squares solutions of Ax = b** coincides with the nonempty set of solutions of the **normal equation** ${A}^{T}Ax = {A}^Tb$\n",
    "- $\\Longrightarrow$ least-squares solutions of ($Ax = b$) = (${A}^{T}Ax = {A}^Tb$)\n",
    "\n",
    "\n",
    "#### Least-Squares Error\n",
    "${\\left\\lVert{b - A\\hat{x}}\\right\\rVert}$ (the distance from **b** to $A\\hat{x}$)\n",
    "\n",
    "\n",
    "### Theorem 14.\n",
    "${A}^TA$ is invertible $\\Longleftrightarrow$ columns of $A$ are linearly independent\n",
    "- $Ax = b$ has only one least-squares solution $\\hat{x}$, $\\hat{x} = $(${A}^TA)^{-1}{A}^{T}b$\n",
    "\n",
    "### Theorem 15.\n",
    "\n",
    "m x n matrix A가 Linearly independent columns를 가지고, $A = QR$라 하면, $Ax = b$는 unique least-squares solution $\\hat{x} = {R}^{-1}{Q}^{T}b$를 가진다.\n",
    "- $R\\hat{x} =  {Q}^{T}b$를 더 자주 사용 \n",
    "\n",
    "#### Least-Squares Lines\n",
    "Least-Squares solution을 이용하여 실험을 통해 얻은 data를 $y = {\\beta}_0 + {\\beta}_1x$에 fitting 시킨다.\n",
    "\n",
    "#### Least-Squares Fitting of Other Curves\n",
    "- 1차식 뿐만 아니라 $y = {\\beta}_0 + {\\beta}_1x + {\\beta}_2{x}^2$ x에 관한 2차식도 fitting 가능하다.\n",
    "- $y = {\\beta}_0 + {\\beta}_1{f(x)}_1 + \\dots + {\\beta}_k{f(x)}_k$의 함수 형태도 가능\n",
    "\n",
    "#### Weighted Least-Squares (Advanced Topic)\n",
    "${\\left\\lVert{y - \\hat{y}}\\right\\rVert}$ = $\\sqrt{({y}_1 - \\hat{y}_1)^2 + \\dots +({y}_n - \\hat{y}_n)^2}$에 데이터에 가중치를 줄 수 있다. $\\Longrightarrow$ $\\sqrt{{w}_1^2({y}_1 - \\hat{y}_1)^2 + \\dots +{w}_n^2({y}_n - \\hat{y}_n)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
