{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 05. Similarity and Ambiguity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNQ73+ujVHn+LinXrBNFr6x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"u7_ydyQf3zRG"},"source":["# Chapter 05. 유사성 & 모호성\n","\n","## INDEX\n","\n","### 5.1 단어의 의미\n","\n","### 5.2 One-hot Encoding\n","\n","### 5.3 시소러스를 활용한 단어 의미 파악\n","\n","### 5.4 특징\n","\n","### 5.5 특징 추출하기: TF-IDF\n","\n","### 5.6 특징 벡터 만들기\n","\n","### 5.7 벡터 유사도 구하기\n","\n","### 5.8 단어 중의성 해소\n","\n","### 5.9 선택 선호도\n","\n","\n","## 5.1 단어의 의미\n","\n","**단어의 의미(word sense)**: 단어는 겉으로 보이는 형태 내에 의미를 갖고 효율성을 추구(당연한 정보들은 생략)하기에 모호성이 나타남.\n","\n","### 5.1.1 단어와 의미의 관계\n","\n","**표제어(lemma)**: 단어가 겉으로 보이는 형태\n","\n","일상 대화에서 **주변 정보(context)** 를 활용해 숨겨진 의미를 파악, 이해\n","\n","주변 정보가 부족시 모호성 증가\n","\n","단어는 형태를 공유하면서 서로 다른 뜻을 가진 의미로 구성될 수 있다. (서로 비슷 or 다른 의미로 사용될 수 있다.) \n","\n","**중의성**: 한 가지 형태에 여러 의미가 포함\n","\n","정리) 단어는 표제어라고 하는 겉으로 보이는 형태가 같지만 상황에 따라 의미가 다를 수 있다. 같은 형태의 단어가 서로 의미를 가질 때 발생하는 문제를 **중의성 문제**라고 한다. 이것을 해결하기 위해 **주변 정보(context)**를 잘 활용해야 한다. \n","\n","인공 지능(ML, DL)에서는 단어는 형태를 잘 이해하고 의미를 잘 파악해서 되어 사용되야 한다.\n","\n","\n","### 5.1.2 동형어 & 다의어\n","\n","**동형어(homonym)**: 형태는 같고 서로 뜻이 서로 다른 단어, 의미들이 서로 다른 의미(관련)을 가진다. \n","\n","ex) 다리 $\\to$ 사람 다리, 책상 다리\n","\n","**다의어(polysemy)**: 한 형태의 단어가 여러 의미를 지니고 그 의미들이 서로 관련되어 있는 뜻을 가지는 단어\n","\n","ex) 차 $\\to$ 마시는 차(tea), 달리는 차(car)\n","\n","단어가 동형어 or 다의어이라면 **단어 중의성 해소(Word Senese Disambiguation, WSD)** 방법을 통해 의미를 명확히 파악하는 과정이 필요\n","\n","DL 이전의 전통적인 NLP 과정에서는 중의성 해소를 위해 **주변 문맥** 통해 단어의 의미를 파악하는 방법 가장 많이 사용되었다. \n","\n","DL의 end-to-end 방법이 선호되고 모델 구조가 RNN으로 넘어오면서 단어 중의성 해소에 대한 필요성이 낮아졌다. BUT, 여전히 단어의 모호한 의미로 인한 문제 해결이 어려운 경우가 많다. \n","\n","### 5.1.3 동의어\n","\n","**동의어(Synonym)**: 다른 형태지만 의미가 같은 단어 (어느 정도 비슷한 의미를 가진다면 동의어라고 칭한다.) \n","\n","**동의어 집합(synset)**: 동의어가 여러 개 존재할 때 이들의 집합\n","\n","ex) 동의어 집합:{dwelling, domicile, abode, habitation} $\\to$ 의미: home\n","\n","### 5.1.4 상위어 & 하위어\n","\n","사람이 사용하는 단어는 하나의 추상적 개념을 나타냄.\n","\n","- 상위어 개념: 개념들을 포함하는 상위 개념을 가리키는 단어\n","\n","- 하위어 개념: 개념들을 포함하는 하위 개념을 가리키는 단어\n","\n","ex) \n","- 상위어: 동물 / 하위어: 포유류\n","- 상위어: 포유류 / 하위어: 코끼리\n","- 상위어: 코끼리 / 하위어: 아프리카 코끼리\n","\n","상, 하위어는 **어휘 분류(taxonomy)** 에 따라 단어 간 관계를 계층화 가능\n","\n","### 5.1.5 모호성 해소\n","\n","사람의 언어는 불연속적 심볼이며 내부 의미를 지닌다. 컴퓨터는 사람의 언어를 텍스트로 받아 처리를 하는데 텍스트가 가진 내부 의미를 파악하는 과정이 필요하다.\n","\n","결국 텍스트의 겉 형태만 파악하는 것이 아닌 내포된 의미를 파악하여 모호성을 제거하여 성능을 높인다. \n","\n","**단어 중의성 해소(Word-Sense Disambiguation, WSD)**: 단어가 가지는 모호성을 제거하는 과정\n","\n","\n","## 5.2 One-hot Encoding\n","\n","- 동형어: 같은 형태, 관련 없는 다른 의미들을 지닌 단어\n","\n","- 다의어: 같은 형태, 관련된 의미들을 갖는 단어\n","\n","- 동의어: 다른 형태, 비슷한 의미를 갖는 단어들\n","\n","단어는 불연속적 심볼, 동형어를 제외한 단어는 내부 의미는 유사, 겉 형태는 다른 경우가 많다.\n","\n","사람이 상식을 이용해 유사한 단어들로 부터 부족한 정보를 보완하는 것 처럼 불연속적인 단어 간 유사도를 효과적으로 구하는 것을 연구되어왔다. DL에서는 유사도를 효율적으로 구하기 위해 다양한 임베딩 방법들을 사용한다.\n","\n","컴퓨터가 단어(텍스트)를 인식하기 위해서는 데이터를 숫자로 변형시켜야 한다. 즉, 벡터로 표현을 해야 한다. 가장 쉬운 방법은 one-hot encoding 방식이다. \n","\n","#### one-hot encoding 표현\n","- 0, 1로만 구성된 벡터로 단어를 표현\n","\n","- 벡터의 차원: 전체 어휘(vocabulary)의 개수\n","\n","\n","단어는 불연속적인 심볼 $\\to$ 이산 확률 변수로 표현.\n","\n","one-hot vector는 이산 확률 분포로 부터 뽑아낸 샘플이라고 할 수 있다. 멀티눌리 확률 분포가 된다.\n","\n","사전(dictionary) 내의 각 단어를 one-hot encoding으로 나타낼 시 \n","- 벡터의 차원이 너무 커진다.\n","- 해당하는 단어는 1로 나머지는 0으로 표현되기에 많은 부분이 0으로 채워진 희소 벡터(sparse vector)이다.\n","- **희소 벡터를 통해 유사도를 구할 시 결과값이 0이다.** $\\to$ 선형대수의 관점으로 볼때 내적의 결과값이 0이므로 두 벡터는 **직교(orthogonal)**이다. \n","($[0,0,...,1,0] \\times [0,1,0,...,0]^T = 0$)\n","('개'와 '강아지'는 서로 유사한 단어지만 one-hot encoding의 결과로 나온 단어에 대한 벡터끼리의 유사도를 위한 연산을 하게되면 0이 나올 것이다.)\n","\n","### 5.2.1 차원의 저주(Curse of Dimensionality)\n","\n","희소 벡터(sparse vector)는 ML에서 큰 어려움으로 작용\n","\n","ex) 정보 표현을 위해 큰 차원이 사용되었다면, 작은 차원으로 같은 정보를 표현한 것에 비해 같은 크기에 공간에 표현되는 정보가 상대적으로 작을 것이다. (밀도가 낮다. 같은 크기로 나뉜 단위 공간이 비어 있을 경우가 많다.)\n","\n","정보를 표현하는 점(벡터)는 낮은 밀도로 희소하게 퍼져있을 것이다.\n","\n","차원이 증가할수록 희소성은 지수적(exponential)으로 증가한다.\n","$\\to$ 이 문제를 차원의 저주라고 한다.\n","\n","$\\Rightarrow$ 차원 축소하여 단어를 표현할 필요\n","\n","정리)\n","같은 데이터에 대해 데이터를 표현하는 공간이 커진다면 희소성 문제(빈 공간이 많이 생긴다) 발생한다.(이는 관측치 보다 변수가 많아지고 변수들의 값들은 0으로 채워져있다. 데이터가 많을수록 유리한 머신러닝에서 이는 모델의 성능 저하를 야기한다.)\n","\n","\n","## 5.3 시소러스를 활용한 단어 의미 파악\n","\n","단어는 내부에 의미를 지니고 있고, 의미는 계층적 구조를 갖고 있다. \n","이 계층적 구조를 분류, 분석하여 DB 구축하면 NLP 도움될 것이다. \n","\n","**시소러스(어휘분류사전, thesaurus):** 단어가 지니는 내부 의미의 계층적 구조를 분석, 분류하여 구축한 데이터베이스\n","\n","대표적인 시소러스: 워드넷(WordNet)\n","\n","### 5.3.1 WordNet\n","\n","WordNet: Machine Translation을 돕기 위한 목적으로 1985년 Professor. George Amiltage Miler의 지도 아래 프린스턴 대학에서 만들어진 프로그램\n","\n","- 특징: 동의어, 상위어, 하위어에 관한 정보 구축이 잘 구축되어 있다.\n","$\\to$ 유향 비순환 그래프(Directed Acyclic Graph, DAG) 이루게 됨\n","(한 노드가 여러 상위 노드를 가질 수 있기에 트리구조가 아닌 Graph구조를 채택함)\n","\n","다운받아 사용 가능하고 NLTK로 랩핑되어 있다. 의미별 비슷한 뜻의 동의어를 링크를 통해 동의어 집합을 제공\n","\n","WordNet이 제공하는 이들 데이터를 바탕으로 supervised learning을 통해 단어 중의성 해소 문제를 풀 수 있다.\n","\n","### 5.3.2 한국어 워드넷\n","\n","한국어를 위한 몇개의 워드넷도 존재하지만, 표준이 정해지지 않았다.\n","- KorLex (ORG: 부산대학교)\n","- Korean WordNet(KWN) (ORG: KAIST)\n","\n","### 5.3.3 WordNet을 활용한 단어간 유사도 비교\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"PGc7nC2k3Ges"},"source":["from nltk.corpus import wordnet as wn\n","\n","def hypernyms(word):\n","    current_node = wn.synsets(word)[0]\n","    yield current_node\n","\n","    while True:\n","        try:\n","            current_node = current_node.hypernyms()[0]\n","            yield current_node\n","\n","        except IndexError:\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPkKIHWHIhdE","executionInfo":{"status":"ok","timestamp":1623835625145,"user_tz":-540,"elapsed":417,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"98b7b210-d9f8-4a4b-d9eb-be58e779ef36"},"source":["for h in hypernyms('policeman'):\n","    print(h)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Synset('policeman.n.01')\n","Synset('lawman.n.01')\n","Synset('defender.n.01')\n","Synset('preserver.n.03')\n","Synset('person.n.01')\n","Synset('causal_agent.n.01')\n","Synset('physical_entity.n.01')\n","Synset('entity.n.01')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGh_TM7iI0jv","executionInfo":{"status":"ok","timestamp":1623836262945,"user_tz":-540,"elapsed":402,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"66894aab-7004-4a44-9aba-0f5ec3e38fd0"},"source":["for h in hypernyms('sheriff'):\n","    print(h)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Synset('sheriff.n.01')\n","Synset('lawman.n.01')\n","Synset('defender.n.01')\n","Synset('preserver.n.03')\n","Synset('person.n.01')\n","Synset('causal_agent.n.01')\n","Synset('physical_entity.n.01')\n","Synset('entity.n.01')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPxlNNzVLvtM","executionInfo":{"status":"ok","timestamp":1623836292418,"user_tz":-540,"elapsed":476,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"d1f607c4-26e8-4a67-a2cc-c3b07092fa23"},"source":["for h in hypernyms('student'):\n","    print(h)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Synset('student.n.01')\n","Synset('enrollee.n.01')\n","Synset('person.n.01')\n","Synset('causal_agent.n.01')\n","Synset('physical_entity.n.01')\n","Synset('entity.n.01')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PbTCZSzL3E0","executionInfo":{"status":"ok","timestamp":1623836325327,"user_tz":-540,"elapsed":355,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"413ebe23-04d3-4caf-8877-b67077efa21a"},"source":["for h in hypernyms('fireman'):\n","    print(h)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Synset('fireman.n.01')\n","Synset('play.n.08')\n","Synset('diversion.n.01')\n","Synset('activity.n.01')\n","Synset('act.n.02')\n","Synset('event.n.01')\n","Synset('psychological_feature.n.01')\n","Synset('abstraction.n.06')\n","Synset('entity.n.01')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7jdb8q_L_Ig","executionInfo":{"status":"ok","timestamp":1623836337905,"user_tz":-540,"elapsed":589,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"0877fcb1-df3a-4f9b-82d6-401a9f4178eb"},"source":["for h in hypernyms('mailman'):\n","    print(h)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Synset('mailman.n.01')\n","Synset('deliveryman.n.01')\n","Synset('employee.n.01')\n","Synset('worker.n.01')\n","Synset('person.n.01')\n","Synset('causal_agent.n.01')\n","Synset('physical_entity.n.01')\n","Synset('entity.n.01')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vmwhv3BkJm-O"},"source":["'police'와 'sheriff'는 비슷한 경로를 가진다.\n","\n","위 경로를 통해 특정 단어의 최상위 부모 노드까지 경로를 구할 수 있다.\n","\n","entity(.n.01) - physical_entity(.n.01) - causal_agent(.n.01) - person(.n.01) - preserver(.n.03) - defender(.n.01) - lawman(.n.01) - sheriff(.n.01)\n","\n","각 노드간 거리를 구할 수 있다. \n","\n","'student' - 'fireman' 최단 거리: 5, 각 최하단 노드 간 최단 거리를 구할 수 있고, 이것은 유사도로 치환하여 활용 가능하다. (거리 멀수록 유사도 낮아)\n","\n","$similarity(w, w') = -log\\ distance(w, w')$\n","\n","시소러스 통해 유사도(거리)를 구할 수 있다. \n","\n","- 단점: 사전 구축에 시간, 비용 소요\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwADrjJMT-DG","executionInfo":{"status":"ok","timestamp":1623838469854,"user_tz":-540,"elapsed":291,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"010776c3-6990-4969-806f-a10461506325"},"source":["import numpy as np\n","\n","def distance(word1, word2):\n","    word1_hypernyms = [h for h in hypernyms(word1)]\n","    \n","    for i, word2_hypernym in enumerate(hypernyms(word2)):\n","        try:\n","            return i + word1_hypernyms.index(word2_hypernym)\n","        except ValueError:\n","            continue\n","\n","distance('sheriff', 'student')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"J_hH0_mdT-3w"},"source":["def similarity(word1, word2):\n","    return -np.log(distance(word1, word2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10C2xJJDUOkL","executionInfo":{"status":"ok","timestamp":1623838507583,"user_tz":-540,"elapsed":273,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"b01c69a5-0d6b-4fbd-a34f-b58672cf4554"},"source":["similarity('sheriff', 'student')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1.791759469228055"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"APDxWqQ-Tnpf"},"source":["## 5.4 특징\n","\n","one-hot vector는 간단하지만 많은 문제가 있기에 효과적이지 않다.\n","\n","효과적인 표현 방식?\n","\n","효과적으로 정보를 추출하고 학습하려면 대상의 특징(feature)를 잘 표현해야 함.\n","\n","각 특징은 수치로 표현되고, 그 특징은 최대한 많은 샘플의 설명할 수 있어야 한다.\n","\n","샘플들은 수치가 서로 다른 공통된 특징 지니며 특징으로 표현된 샘플들은 최대한 다양한게 표현되야 좋다.\n","\n","ex) MNIST dataset\n","- features\n","    - 굵기\n","    - 기울기\n","    - 각 위치 별 직선, 곡선 여부\n","    - etc\n","    \n","**Feature vector:** 각 샘플의 특징들마다 수치를 가지고, 이런 특징별 수치를 모아 표현한 벡터\n","\n","\n","### 5.4.1 단어의 특징 벡터 구성을 위한 가정\n","\n","1. 의미가 비슷한 단어면 쓰임새가 비슷할 것\n","2. 쓰임새가 비슷하기에, 비슷한 문장 안에서 비슷한 역할로 사용될 것\n","3. 함께 나타나는 단어들이 유사할 것\n","\n","## 5.5 특징 추출하기: TF-IDF\n","\n","텍스트 마이닝에서 TF-IDF는 중요하게 사용된다.\n","- **TF(Term Frequency)**: 단어가 문서 내에 출현한 횟수(count)\n","- **IDF(Inverse Document Frequency):** 단어가 등장한 문서의 수의 역수\n","\n","TF-IDF(w, d) = ${TF(w, d)} \\times log\\frac{N}{1+DF(w)}$\n","\n","'the'와 같은 관사는 중요하지 않지만 많이 등장한다 이는 TF('the', d)가 높은 값을 갖게 되는 것에 패널티를 주기 위해 DF의 역수(IDF)를 곱하여 중요도를 낮춘다.\n","\n","TF-IDF가 높은 값을 갖는다면 특정 문서에만 많이 등장하는 단어이다. (TF-IDF: 특정 문서에서 얼마나 중요한 역할을 차지하는지 나타내는 수치)\n","\n","### 5.5.1 TF-IDF Example\n","\n","분절(tokenize)된 문서가 주어졌을 때 TF-IDF의 예제\n"]},{"cell_type":"code","metadata":{"id":"DEBeJpOxU6KN"},"source":["doc1 = '''\n","지능 지수 라는 말 들 어 보 셨 을 겁니다 . 여러분 의 지성 을 일컫 는 말 이 죠 . 그런데 심리 지수 란 건 뭘까요 ? 사람 들 이 특정 한 식 으로 행동 하 는 이유 에 대해 여러분 은 얼마나 알 고 계시 나요 ? 또 타인 이나 심지어 여러분 의 행동 을 예측 하 는 일 은 얼마나 잘 하 시 나요 ? 또 , 심리학 에 대해 갖춘 지식 중 에서 어느 정도 나 잘못 된 것 일까요 ? 심리학 에 관한 열 가지 신화 를 통해 잘못 된 것 들 을 알아보 도록 하 죠 . 여러분 은 한 번 쯤 들 어 보 셨 을 법 한 것 은 자신 들 의 심리학 에 대해 고려 할 때 , 거의 항상 남자 는 화성 에서 왔 고 , 여자 는 금성 에서 온 것 같 다고 합니다 . 하지만 실제로 남자 와 여자 는 얼마나 다른 걸까요 ? 이 를 알아보 기 위해 , 일단 남녀 사이 에 확실 하 게 차이 나 는 것 을 살펴보 고 심리학 적 인 성별 간 의 차이점 을 동일 한 척도 상 에서 대비 해 보 도록 하 겠 습니다 . 남자 와 여자 간 에 실제로 차이 나 는 능력 중 하나 는 그 들 이 공 을 얼마나 멀리 던질 수 있 느냐 하 는 것 입니다 . 여기 남자 들 의 데 이타 를 보 시 면 , 정상 분포 곡선 이 라는 걸 볼 수 있 습니다 . 남자 들 소수 는 정말 멀리 던지 고 , 남자 들 소수 는 멀리 던지 지 못하 지만 , 남자 들 대부분 은 평균 적 인 거리 를 던졌 습니다 . 여자 들 도 역시 비슷 한 분포 상태 를 보입니다 만 사실 남녀 사이 엔 커다란 차이 가 있 습니다 . 사실 , 평균 수준 의 남자 라면 모든 여성 중 대략 98 % 보다 더 멀리 던질 수 있 거든요 . 이 와 동일 하 게 표준 화 된 척도 상 에서 심리학 에서 말 하 는 성별 간 의 차이 를 살펴 봅시다 . 심리학자 라는 여러분 에게 말 하 길 남자 들 의 공간 지각 능력 이 여자 들 보다 뛰어나 다고 할 겁니다 . 예 를 들 어 , 지도 읽 는 능력 같 은 건데 , 맞 는 말 입니다 . 하지만 그 차이 의 정도 를 살펴봅시다 . 아주 작 죠 . 두 선 이 너무 근접 해서 거의 겹칠 정도 입니다 .\n","'''\n","\n","doc2 = '''\n","최상 의 제시 유형 은 학습 자 에 좌우 되 는 것 이 아니 라 학습 해야 할 내용 에 따라 좌우 됩니다 . 예 를 들 어 여러분 이 운전 하 기 를 배울 때 실제로 몸 으로 체감 하 는 경험 없이 누군가 가 어떻게 할 지 이야기 하 는 것 을 듣 는 것 만 으로 배울 수 있 습니까 ? 연립 방정식 을 풀 어야 하 는데 종이 에 쓰 지 않 고 머리 속 에서 말 하 는 것 으로 풀 수 가 있 을까요 ? 또는 만일 여러분 이 체감 형식 의 학습 자 유형 이 라면 , 건축학 시험 을 해석 적 춤 을 이용 하 여 수정 할 수 있 을까요 ? 아니 죠 ! 배워야 할 내용 을 제시 된 유형 에 맞추 어야 합니다 , 당신 에게 맞추 는 게 아니 라요 . 여러분 들 상당수 가 \" A \" 급 의 우등 생 이 라는 걸 아 는데 , 조만간 중등 학력 인증 시험 ( GCSE ) 결과 를 받 게 되 시 겠 네요 . 그런데 , 만일 , 여러분 들 이 희망 했 던 성적 을 받 지 못하 게 된다 해도 여러분 들 의 학습 방식 을 탓 해서 는 안 되 는 겁니다 . 여러분 이 비난 할 수 있 는 한 가지 는 바로 유전자 입니다 . 이건 최근 에 런던 대학교 ( UCL ) 에서 수행 했 던 연구 결과 는 여러 학생 들 과 그 들 의 중등 학력 인증 시험 결과 사이 의 차이 중 58 % 는 유전 적 인 요인 으로 좁혀졌 습니다 . 매우 정밀 한 수치 처럼 들립니다 . 그러면 어떻게 알 수 있 을까요 ? 유전 적 요인 과 환경 적 요인 의 상대 적 기여 도 를 알 고 싶 을 때 우리 가 사용 할 수 있 는 방식 은 바로 쌍둥이 연구 입니다 . 일 란 성 쌍생아 의 경우 환경 적 요인 과 유전 적 요인 모두 를 100 % 똑같이 공유 하 게 되 지만 이란 성 쌍생아 의 경우 는 100 % 동일 한 환경 을 공유 하 지만 유전자 의 경우 여타 의 형제자매 들 처럼 50 % 만 공유 하 게 됩니다 . 따라서 일 란 성 쌍둥이 와 이란 성 쌍둥이 사이 의 인증 시험 결과 가 얼마나 비슷 한지 비교 해 보 고 여기 에 약간 의 수학 적 계산 을 더하 게 되 면 그 수행 능력 의 차이 중 어느 정도 가 환경 적 요인 의 탓 이 고 어느 정도 가 유전자 탓 인지 를 알 수 있 게 됩니다 .\n","'''\n","\n","doc3 = '''\n","그러나 이 이야기 는 세 가지 이유 로 인해 신화 입니다 . 첫째 , 가장 중요 한 건 실험실 가운 은 흰색 이 아니 라 회색 이 었 다 라는 점 이 죠 . 둘째 , 참 여자 들 은 실험 하 기 전 에 와 참여 자 들 이 걱정 을 표현 할 때 마다 상기 시키 는 말 을 들 었 는데 , 전기 충격 이 고통 스럽 기 는 하 지만 , 치명 적 이 지 는 않 으며 실제로 영구 적 인 손상 을 남기 는 일 은 없 을 거 라는 것 이 었 습니다 . 셋째 , 참 여자 들 은 단지 가운 을 입 은 사람 이 시켜 전기 충격 을 주지 는 않 았 죠 . 실험 이 끝나 고 그 들 의 인터뷰 를 했 을 때 모든 참여 자 들 은 강한 신념 을 밝혔 는데 , ' 학습 과 처벌 ' 연구 가 과학 적 으로 가치 있 는 목적 을 수행 했 기 때문 에 비록 동료 참여 자 들 에게 가해진 순간 적 인 불편 함 에 반해서 과학 을 위해서 오래 남 을 성과 를 얻 을 것 이 라고 말 이 죠 . 그러 다 보 니 제 가 이야기 를 한 지 벌써 12 분 이 되 었 습니다 . 여러분 들 중 에 는 아마 거기 앉 아서 제 이야기 를 들으시는 동안 저 의 말투 와 몸짓 을 분석 하 면서 제 가 말 하 는 어떤 것 을 인지 해야 할까 해결 하 려고 하 셨 을 겁니다 , 제 가 진실 을 이야기 하 는 지 , 또는 거짓말 을 하 고 있 는 것 인지 말 이 죠 . 만일 그러 셨 다면 , 아마 지금 쯤 완전히 실패 하 셨 을 겁니다 . 왜냐하면 우리 모두 가 사람 이 말 하 는 패턴 과 몸짓 으로 도 거짓말 여부 를 알아내 는 것 이 가능 하 다고 생각 하 지만 , 오랜 세월 수백 회 에 걸쳐 행해진 실제 심리 검사 의 결과 를 보 면 우리 들 모두 는 , 심지어 경찰관 이나 탐정 들 을 포함 해서 도 기본 적 으로 몸짓 과 언어 적 패턴 으로 거짓말 을 탐지 하 는 것 은 운 에 맞 길 수 밖 에 는 없 는 것 입니다 . 흥미 롭 게 도 한 가지 예외 가 있 는데요 : 실종 된 친척 을 찾 아 달 라고 호소 하 는 TV 홍보 입니다 .\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tjWj8V6JzZE"},"source":["# 문서 내 단어의 출현 빈도 카운트 함수\n","import pandas as pd\n","\n","def get_term_frequency(document, word_dict=None):\n","    if word_dict is None:\n","        word_dict = {}\n","    words = document.split()\n","\n","    for w in words:\n","        word_dict[w] = 1 + (0 if word_dict.get(w) is None else word_dict[w])\n","\n","    return pd.Series(word_dict).sort_values(ascending=False)\n","\n","# 단어가 등장한 문서의 수 카운트 함수\n","def get_document_frequency(documents):\n","    dicts = []\n","    vocab = set([])\n","    df = {}\n","\n","    for d in documents:\n","        tf = get_term_frequency(d)\n","        dicts += [tf]\n","        vocab = vocab | set(tf.keys())\n","\n","    for v in list(vocab):\n","        df[v] = 0 \n","        for dict_d in dicts:\n","            if dict_d.get(v) is not None:\n","                df[v] += 1\n","\n","    return pd.Series(df).sort_values(ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJmZ1-viU-mV","executionInfo":{"status":"ok","timestamp":1623838696323,"user_tz":-540,"elapsed":282,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"3780af8f-41cb-494e-ebb1-1bc9de0ca17c"},"source":["get_term_frequency(doc1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":[".     16\n","는     15\n","들     14\n",",     10\n","하     10\n","      ..\n","정상     1\n","면      1\n","이타     1\n","데      1\n","지능     1\n","Length: 186, dtype: int64"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"Ff0MhYXISXet"},"source":["def get_tfidf(docs):\n","    vocab = {}\n","    tfs = []\n","    for d in docs:\n","        vocab = get_term_frequency(d, vocab)\n","        tfs += [get_term_frequency(d)]\n","    df = get_document_frequency(docs)\n","\n","    from operator import itemgetter\n","    import numpy as np\n","\n","    stats = []\n","    for word, freq in vocab.items():\n","        tfidfs = []\n","        for idx in range(len(docs)):\n","            if tfs[idx].get(word) is not None:\n","                tfidfs += [tfs[idx][word] * np.log(len(docs) / df[word])]\n","            else:\n","                tfidfs += [0]\n","\n","        stats.append((word, freq, *tfidfs, max(tfidfs)))\n","\n","    return pd.DataFrame(stats, columns=('word',\n","                                        'frequency', \n","                                        'doc1', \n","                                        'doc2', \n","                                        'doc3', \n","                                        'max')).sort_values('max', ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"iggGm17bV1NV","executionInfo":{"status":"ok","timestamp":1623838928250,"user_tz":-540,"elapsed":789,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}},"outputId":"bc30d27f-ede8-4963-8d04-2750026bbc97"},"source":["get_tfidf([doc1, doc2, doc3])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>frequency</th>\n","      <th>doc1</th>\n","      <th>doc2</th>\n","      <th>doc3</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>23</th>\n","      <td>남자</td>\n","      <td>9</td>\n","      <td>9.887511</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>9.887511</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>요인</td>\n","      <td>6</td>\n","      <td>0.000000</td>\n","      <td>6.591674</td>\n","      <td>0.000000</td>\n","      <td>6.591674</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>심리학</td>\n","      <td>5</td>\n","      <td>5.493061</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>5.493061</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>었</td>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.394449</td>\n","      <td>4.394449</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>제</td>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.394449</td>\n","      <td>4.394449</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>라는</td>\n","      <td>6</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>중</td>\n","      <td>6</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>습니다</td>\n","      <td>7</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>보</td>\n","      <td>7</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>는</td>\n","      <td>47</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>437 rows × 6 columns</p>\n","</div>"],"text/plain":["   word  frequency      doc1      doc2      doc3       max\n","23   남자          9  9.887511  0.000000  0.000000  9.887511\n","37   요인          6  0.000000  6.591674  0.000000  6.591674\n","51  심리학          5  5.493061  0.000000  0.000000  5.493061\n","57    었          4  0.000000  0.000000  4.394449  4.394449\n","63    제          4  0.000000  0.000000  4.394449  4.394449\n","..  ...        ...       ...       ...       ...       ...\n","36   라는          6  0.000000  0.000000  0.000000  0.000000\n","33    중          6  0.000000  0.000000  0.000000  0.000000\n","30  습니다          7  0.000000  0.000000  0.000000  0.000000\n","29    보          7  0.000000  0.000000  0.000000  0.000000\n","0     는         47  0.000000  0.000000  0.000000  0.000000\n","\n","[437 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"QrFSQfJQV-H0"},"source":["- doc1에서 가장 중요한 단어: '남자'\n","- doc2에서 가장 중요한 단어: '요인'"]},{"cell_type":"markdown","metadata":{"id":"Hc_iuR4DWP6K"},"source":["## 5.6 특징 벡터(feature vector) 만들기\n","\n","### 5.6.1 TF-IDF\n","\n","문서들(doc1, doc2, doc3) 있다고 가정하고 TF(w, d1), TF(w, d2), TF(w, d3)가 column이라고 하면 '문서에 대한 단어별 등장 횟수'를 활용한 특징 벡터가 될 거다. \n","\n","BUT. 문서의 수가 많다면 벡터의 차원 역시 커질 것이다. (문서가 1000개 라면, 단어당 1000차원의 벡터가 만들어질 것이다. TF(w, doc1) ~ TF(w, doc1000))\n","\n","단순 등장 횟수를 카운트 하여 feature vector를 구성하면 다른 많은 정보들이 유실된다.\n","\n","\n","\\\\\n","\n","### 5.6.2 콘텍스트 윈도우로 함께 출현한 단어들의 정보 활용하기\n","\n","**함께 등장하는 동시발생(co-occurrence) 단어들을 활용한 방법**\n","\n","가정: 의미 비슷하면 쓰임새도 비슷할 것이고 쓰임새가 비슷하기에 비슷한 문장 안에서 비슷한 역할로 사용될 것이다. 따라서 함께 나타나는 단어들이 서로 유사할 것이다.\n","\n","$\\to$ 함께 등장하는 서로 유사한 단어들이 유사한 형태의 벡터를 갖게 해야한다.\n","\n","함께 나타나는 단어들을 조사하기 위한 방법: **윈도잉(windowing)**\n","\n","**Windowing:** window를 움직이며 윈도우 안에 있는 유닛들의 정보를 취합하는 방법\n","(이때 사용되는 윈도우를 컨텍스트 윈도우(context window)라 한다.)\n","\n","$\\to$ 단어별 윈도우 내에 속해 있는 이웃 단어들의 출현 빈도를 세어 행렬로 표현\n","\n","윈도우는 Hyperparameter, 사이즈 정하는 것 중요\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"EV7VGAjgQgL-","executionInfo":{"status":"ok","timestamp":1623922069881,"user_tz":-540,"elapsed":470,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}}},"source":["from collections import defaultdict\n","import pandas as pd\n","\n","def get_context_counts(lines, w_sizse=2):\n","    co_dict = defaultdict(int)\n","\n","    for line in lines:\n","        words = line.split()\n","\n","        for i, w in enumerate(words):\n","            for c in words[i - w_size: i + w_size]:\n","                if w != c:\n","                    co_dict[(w, c)] += 1\n","\n","        return pd.Series(co_dict)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rSddRxBTExp","executionInfo":{"status":"ok","timestamp":1623922197442,"user_tz":-540,"elapsed":438,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}}},"source":["def co_occurrence(co_dict, vocab):\n","    data = []\n","    \n","    for word1 in vocab:\n","        row = []\n","        for word2 in vocab:\n","            try:\n","                count = co_dict[(word1, word2)]\n","\n","            except KeyError:\n","                count = 0 \n","                row.append(count)\n","\n","        data.append(row)\n","    \n","    return pd.DataFrame(data, index=vocab, columns=vocab)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKS3qfXfT-BH"},"source":["context window를 사용해도 희소성 문제에 대해 완벽히 해결하지 못한다.\n","\n","\n","one-hot vector $\\to$ co-occurrence $\\to$ word embedding\n","\n","\\\\\n","\n","## 5.7 벡터 유사도 구하기\n","\n","자연어 처리에 머신러닝을 사용하기 위해서는 데이터를 최대한 효율적으로 활용해 모델을 학습시키는 것이 중요\n","\n","단어로 부터 특징 벡터를 추출해 유사한 단어(단어들의 집합)에서 더 많은 정보를 추출해 학습에 활용해야한다.\n","\n","### 5.7.1 L1 거리\n","\n","**L1 거리(L1 distance):** L1 norm을 사용한 것(Manhattan Distance 라고도 함), 값의 차이의 절대값을 모두 합친 값\n","\n","$d_{L1}(w, v) = \\sum^d_{i=1}|w_i - v_i|, where\\ w, v \\in \\mathbb{R}^d$\n"]},{"cell_type":"code","metadata":{"id":"ZEtTL7ZpTjxH","executionInfo":{"status":"ok","timestamp":1623922759386,"user_tz":-540,"elapsed":3642,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}}},"source":["import torch\n","\n","def get_l1_distance(x1, x2):\n","    return ((x1 - x2).abs().sum())"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXnvvgNdVtIt"},"source":["### 5.7.2 L2 거리\n","**L2 거리(L2 distance):** 유클리디안 거리(Euclidean distance)\n","\n","$d_{L2}(w, v) = \\sqrt{\\sum^d_{i=1}(w_i - v_i)^2}, where\\ w, v \\in \\mathbb{R}^d$\n"]},{"cell_type":"code","metadata":{"id":"D722Vvm2VsWu","executionInfo":{"status":"ok","timestamp":1623922928260,"user_tz":-540,"elapsed":2,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}}},"source":["def get_l2_distance(x1, x2):\n","    return ((x1 - x2)**2).sum()**.5"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hThzz3iUWdhC"},"source":["### 5.7.3 Infinity Norm($L_{\\infty}$)\n","\n","**infinity norm을 이용한 거리는 차원별 값의 차이 중 가장 큰 값을 나타낸다.**\n","\n","$d_{\\infty}(w, v) = max(|w_1 - v_1|, |w_2 - v_2|, \\cdots, |w_d - v_d|), where\\ w, v \\in \\mathbb{R}^d$\n"]},{"cell_type":"code","metadata":{"id":"c6bsqgMBWVun","executionInfo":{"status":"ok","timestamp":1623923126920,"user_tz":-540,"elapsed":457,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}}},"source":["def get_infinity_distance(x1, x2):\n","    return ((x1 - x2).abs()).max()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KqTN9Cv7XOl1"},"source":["### 5.7.3 코사인 유사도\n","\n","**코사인 유사도(cosine similarity):** 두 벡터 사이의 방향과 크기 모두 고려한 유사도 방법\n","\n","$sim_{cos}(w, v) = \\frac{w \\cdot v}{|w||v|} = \\frac{w}{|w|} \\cdot \\frac{v}{|v|}$\n","\n","$ = \\frac{\\sum^d_{i=1}w_iv_i}{\\sqrt{\\sum^d_{i=1}w^2_i}\\sqrt{\\sum^d_{i=1}w^2_i}}, where\\ w, v \\in \\mathbb{R}^d$\n","\n","코사인 유사도의 결과: -1 ~ 1 (1에 가까울수록 방향은 일치, 0에 가까울수록 직교, -1에 가까울수록 반대 방향\n","\n","희소 벡터일 경우, 0들이 많기에 해당 차원이 직교하며 곱의 값이 0이 되므로, 정확한 유사도 or 거리 반영 불가\n"]},{"cell_type":"code","metadata":{"id":"GJKDOS68XG5X","executionInfo":{"status":"ok","timestamp":1623924968871,"user_tz":-540,"elapsed":433,"user":{"displayName":"김민성","photoUrl":"","userId":"11106110681894598136"}}},"source":["def get_cosine_similarity(x1, x2):\n","    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lARkhPaPeKJm"},"source":["### 5.7.5 자카드 유사도\n","\n","**자카드 유사도(jacard similarity):** 두 집합 간의 유사도를 구하는 방법\n","\n"]},{"cell_type":"code","metadata":{"id":"ytrdm5kKeImi"},"source":[""],"execution_count":null,"outputs":[]}]}