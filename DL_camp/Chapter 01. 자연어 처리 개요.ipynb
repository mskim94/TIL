{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "온라인에는 많고 다양한 양질의 정보들이 존재한다. 자연어처리 공부를 하며 충족되지 않는 공부를 하면서도 먼가 핵심을 공부하지 않는 느낌이 들었다.\n",
    "자연처 처리 딥러닝 캠프 책의 지은이의 말 중 가장 공감되는 말이 있었다.\n",
    "\"지금까지 자연어처리를 공부하면서 단편적으로 흩어져 있는 자료들로 공부하다 보니, 체계가 없어 기초가 부족해 어려움을 겪었던 경험이 있다.\"\n",
    "자연어 처리를 조금 더 체계적으로 공부하기 위해 책을 읽는다. \n",
    "\n",
    "# 자연어 처리 개요\n",
    "## 1.1 자연어 처리란?\n",
    "\n",
    "자연어 처리(Natural Language Processing, NLP): 인공지능의 한 분야, **자연어 처리 인공지능은** 사람의 언어를 컴퓨터가 알아듣도록 처리하는 인터페이스\n",
    "\n",
    "## 1.2 딥러닝 소개\n",
    "### 1.2.1 딥러닝의 역사\n",
    "\n",
    "- 1950년: 인공지능의 첫 유행\n",
    "- 1980년: 역전파 알고리즘 제안 됨\n",
    "- 2006년: 제프리 힌튼 Deep Belief Network(DBN)통한 은닉층(hidden layer)을 효과적 사전훈련(pretraining) 방법 제안\n",
    "- 2012년: ImageNet, AlexNet(Convolutional Layer) 우승\n",
    "- 2015년: ResNet(Residual connection) 우승\n",
    "\n",
    "- 음성인식(speech recognition): ASR(Automatic Speech Recognition) 시스템: AM + LM, WFST(Weighted Finite State Transeducer) 결합\n",
    "    - 음향 모델(Aucostic Model, AM): GMM + HMM\n",
    "        - GMM(Gausian Mixture Model): 음소(phone)을 인식\n",
    "        - HMM(Hidden Markov Model): sequential 정보를 반영\n",
    "    - n-gram 기반의 언어 모델(Language Model, LM)\n",
    "\n",
    "$\\to$ 음향 모델 전체를 LSTM으로 대체, end-to-end 방식 사용이 추세\n",
    "\n",
    "- 기계번역: \n",
    "    - 규칙 기반 기계번역(Rule-based machine Translation, RBMT)\n",
    "    - 통계 기반 기계번역(Statistical Machine Translation, SMT)\n",
    "    - 신경망 기반 기계번역(Neural Network Translation, NMT)\n",
    "\n",
    "\n",
    "- 생성 모델 학습: 데이터X의 분포를 배우는 것에 집중\n",
    "    - 적대적 학습(Adversarial Learning)\n",
    "    - 변분 오토인코더(Variational AutoEncoder, VAE)\n",
    "    \n",
    "### 1.2.2 자연어 처리의 패러다임 변화\n",
    "end-to-end 모델로 대체, 딥러닝이 자연어 처리에서도 주류가 되면서, 앞에서와 같은 기존의 접식과는 다른 여러 변화 나타남. 사람의 언어는 불연속적인 이산(discrete) 심벌로 이뤄짐. 전통적 자연어 처리서는 데이터를 불연속적 심벌로 취급. $\\to$ 사람이 데이터 보고 해석 쉬움, **모호성, 유의성 어려움**\n",
    "$\\to$ 단어 임베딩을 통해 연속적인(continuous) 벡터로 나타내어 해결\n",
    "\n",
    "## 1.3 자연어 처리가 어려운 이유\n",
    "언어는 사람의 생각과 지식을 내포\n",
    "### 1.3.1 모호성\n",
    "단어의 중의성, 문장 내 정보의 부족 때문에 문장을 해석하는데 모호함이 발생\n",
    "\n",
    "\n",
    "### 1.3.2 다양한 표현\n",
    "문장의 표현 형식이 다양, 비슷한 의미의 단어가 존재하기 때문에 다양한 표현의 문제가 존재\n",
    "\n",
    "### 1.3.3 불연속적 데이터\n",
    "데이터는 불연속적 데이터 과거에는 처리가 쉬운 편이였지만 딥러닝에 적용을 위해서는 연속적인 값으로 변환해주어야 한다. 임베딩을 사용한다. 하지만 구현시에 제약이 존재한다.\n",
    "\n",
    "- 차원의 저주: 불연속적인 데이터기 때문에 많은 종류의 데이터 표현 위해서는 데이터의 종류만큼 큰 차원이 필요. (각 단어를 불연속적인 심벌로 다룬 만큼, 어휘의 크기만큼 차원이 있는 것과 같다.) 이런 희소성(sparseness)문제를 해결 위해 단어를 분절(segmentation)등의 노력. 적절한 임베딩을 통해 차원 축소(dimension reduction)으로 문제 해결\n",
    "\n",
    "- 노이즈 & 정규화: 데이터에서 노이즈를 분리하는 것은 어려움, 잘못 처리하면 데이터 본래의 의미가 변질될 수 있다. (Image에서 RGB 값 중 하나가 1 바뀐다고 이미지 전체 의미는 큰 변화가 없을 것, 하지만 단어는 불연속적 심벌이라 살짝만 바뀌어도 문장 의미가 완전 달라질 수 있다.)\n",
    "    - 정제(Normalization)?\n",
    "\n",
    "## 1.4 한국어 자연어 처리는 어렵다.\n",
    "\n",
    "### 1.4.1 교착어\n",
    "- 교착어: 어간에 접사가 붙어 단어를 이루고 의미와 문법적 기능 정해짐 (한국어, 일본어, 몽골어)\n",
    "- 굴절어: 단어의 형태 변화로 문법적 기능이 정해짐 (라틴어, 독일어, 러시아어)\n",
    "- 고립어: 어순에 따라 단어의 문법적 기능 정해짐 (영어, 중국어)\n",
    "\n",
    "교착어라는 특징때문에 다양한 단어가 파생되어 파싱(parsing), 형태소 분석(POS tagging)부터 언어 모델까지 어렵게 한다. 추가적인 분절 과정이 필요\n",
    "\n",
    "\n",
    "### 1.4.2 띄어쓰기\n",
    "추가적인 분절을 통해 띄어쓰기를 정제해주는 과정이 필요\n",
    "\n",
    "\n",
    "### 1.4.3 평서문과 의문문\n",
    "한국어는 의문문과 평성문이 같은 형태의 문장 구조를 가진다. 마침표나 물음표가 붙지 않아면 구분이 힘들다.(특히 음성 인식의 결과로 나오는 텍스트에서)\n",
    "\n",
    "\n",
    "### 1.4.4 주어 생략\n",
    "영어는 주어가 중요, 한국어는 동사를 중시 주어가 자주 생략됨. 문장의 주어가 생략되면 컴퓨터가 문장의 정확한 뜻을 파악하기 매우 어렵다.\n",
    "\n",
    "\n",
    "### 1.4.5 한자 기반의 언어\n",
    "한국어는 한자의 영향을 많이 받은 언어, 각 글자가 의미를 갖고 합쳐서 하나의 단어의 뜻을 이루는 단어들(sub-word)이 존재. \n",
    "\n",
    "서브워드 단위로 분절 경우 문제가 어려워짐\n",
    "\n",
    "- 원문: 한 가지 문제가 있다고 생각\n",
    "\n",
    "- 형태소 분절: 한 가지 문제 있 다고 생각\n",
    "\n",
    "- 출현 빈도 기반 서브워드 분절: 한 가지 문 제 있 다고 생각\n",
    "\n",
    "출현 빈도 기반 서브워드 분절을 보면 \"문제\"에 \"제\"는 같은 소리를 가지는 다양한 한자들이 존재. 신경망에서 토큰은 임베딩 벡터로 변환되는데 다양한 의미의 \"제\"에 대해서 하나의 글자를 갖고 임베딩 할 것이고, 벡터의 평균값으로 애매하게 임베딩하게 될 것이다.\n",
    "\n",
    "## 1.5 자연어 처리의 최근 추세\n",
    "\n",
    "### 1.5.1 딥러닝의 자연어 처리 정복 과정\n",
    "\n",
    "- 2010년: RNN 활용 언어 모델 시도, 기존 n-gram 기반 언어 모델 한계 극복 위해 노력, 기존 n-gram의 결합(보간, interpolation)을 통해 더 나은 성능의 언어 모델을 만들어 냄, 하지만 구조적인 한계가 존재했고 연산량이 높아 큰 성과는 낼 수 없었음.\n",
    "\n",
    "\n",
    "- 2013년: Google의 토마스 미코로프 word2vec 발표. 단순한 구조 신경망 사용 효과적으로 단어들을 잠재 공간(latent space)에 투사시킴. (고차원 공간상에 단어가 latent space배치 될 때, 비슷한 의미의 단어일수록 저차원의 잠재 공간에서 가깝게 위치 한다.)\n",
    "\n",
    "- 2014년: 문장이란 sequential data인데 순환 신경망(RNN, Recurrent Neural Network)을 통해 해결한다는 고정관념 존재 (이미지는 CNN, 텍스트는 RNN). 윤 김은 CNN만 활용 기존 보다 성능 높은 텍스트 분류 모델 제시. 자연어 처리에 대한 시각응ㄹ 한 단계 넓히게 됨\n",
    "\n",
    "### 1.5.2 자연어 생성의 시작\n",
    "\n",
    "2014년 자연어 처리가 크게 발전함, seq2seq, attention 기법이 개발, 기계번역에 적용되어 큰 성과를 거둠. 이를 통해 주어진 데이터를 기반으로 문장을 생성하는 자연어 생성 (NLG, Natural Language Generation) 가능해짐. \n",
    "\n",
    "\n",
    "### 1.5.3 메모리를 활용한 심화 연구\n",
    "Attention의 성과로 연속적인 방식으로 정보를 읽고 쓰는 기법에 관심이 커짐. 뉴럴 튜링 머신(NTM, Neural Turing Machine)이 주목을 받음 (특정 주소에서만 정보를 읽는 방법 떠나 여러 주소서 연속적으로 정보를 읽고 쓰는 방법 제시). 디퍼런셜 뉴럴 컴퓨터(Differential Neural Computer, DNC)제시되며 활용 방법에 관심 높아짐\n",
    "\n",
    "신경망을 통해 메모리를 활용하는 기법을 **메모리 증강 신경망(MANN, Memory Augmented Neural Network)** 라 한다. \n",
    "\n",
    "\n",
    "### 1.5.4 강화학습의 자연어 처리 분야의 성공적 적용\n",
    "\n",
    "Computer vision에서 기존 판별 모델 학습 방식에서 VAE, GAN을 통해 생성 모델 학습으로 관심이 이동함. 하지만 자연어 처리는 언어 모델 자체가 문장에 대한 생성 모델 학습이기 때문에 그렇지 않았다. \n",
    "\n",
    "하지만, 딥러닝에서 사용하는 손실 함수(loss function)과 실제 기계번역을 위한 목적 함수(objective function)의 괴리가 존재. 자연어 처리에서도 강화학습을 활용 seqGAN 같이 GAN이 구현하는 방법 제안되기도 했음.\n",
    "\n",
    "이때 강화학습의 **폴리시 그래디언트(policy gradients) 방식** 을 자연어 생성에 성공적으로 적용하여 비전의 적대적 학습과 같은 방법을 자연어 처리에서도 흉내낼 수 있게 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
