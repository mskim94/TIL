{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Deep Learning 구현 장비\n",
    "\n",
    "### 3.1.1 장비 구성\n",
    "\n",
    "- CPU: Core의 개수 보다 단일 클럭 높아야 함.\n",
    "    - 권장: i7\n",
    "\n",
    "- RAM: 메모리 다다익선\n",
    "    - 권장: 64GB\n",
    "    \n",
    "- GPU: 메모리 클수록 좋음(비용 비쌈)\n",
    "    - 권장: RTX 2080Ti\n",
    "    \n",
    "- Power Supply: 비싸고 검증된 브랜드\n",
    "\n",
    "- Cooling: GPU 발열 심해 쿨링 시스템 중요.\n",
    "\n",
    "\n",
    "### Why Pytorch?\n",
    "\n",
    "비슷한 레벨의 구현 난도를 가정 시, Pytorch가 TF 비해 뛰어난 생산성 \n",
    "\n",
    "장점:\n",
    "\n",
    "- Python First, 깔끔한 코드\n",
    "\n",
    "- Numpy와 뛰어난 호환성\n",
    "\n",
    "- Autograd\n",
    "\n",
    "- Dynamic graph\n",
    "\n",
    "\n",
    "## 3.3 Pytorch Tutorial\n",
    "\n",
    "### 3.3.1 Tensor\n",
    "\n",
    "Pytorch's tensor == Numpy's ndarray (같은 개념)\n",
    "\n",
    "파이토치 연산 수행의 가장 기본적인 객체\n",
    "\n",
    "### 3.3.2 Autograd\n",
    "\n",
    "**Autograd**: 자동으로 미분 및 역전파 수행 함수\n",
    "\n",
    "Pytorch는 tensor들 간에 연산을 수행할 때마다 동적으로 연산 그래프(computation graph) 생성하여 연산 결과물이 어떤 tensor로 부터 어떤 연산을 통해 왔는지 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor([1,2])\n",
    "y = torch.FloatTensor([1,2])\n",
    "y.requires_grad_(True)  \n",
    "\n",
    "z = (x+y) + torch.FloatTensor([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras, TF는 **미리 정의한 연산들을 컴파일 통해 고정 후**, **정해진 입력**에 맞춰 tensor를 feedforward 해야한다.\n",
    "\n",
    "Pytorch는 **정해진 연산이 없고**, Model은 학습해야할 parameter tensor만 미리 알고 있음, 그 weight parameter들이 어떠한 연산을 통해 학습 또는 연산에 관여하는지는 알 수 없다. (연산 수행된 직후에 알 수 있다.)\n",
    "\n",
    "**기울기 구할 필요 없는 연산의 경우:** **with torch.no_grad()**로 수행 가능\n",
    "\n",
    "$\\Rightarrow$ 역전파 알고리즘 수행이 필요 없는 비 학습 과정, 예측(prediction), 추론(inference) 수행시 유용 $\\to$ 연산 속도, 메모리 측면 이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor(2,2)\n",
    "y = torch.FloatTensor(2,2)\n",
    "y.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = (x+y) + torch.FloatTensor(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Feedforward\n",
    "\n",
    "선형 계층(Linear laye) or 완전연결계층(Fully-connected layer) 구현\n",
    "\n",
    "$y = xW + b\\\\\n",
    "where\\ x \\in \\mathbb{R}^{M\\times N}, W \\in \\mathbb{R}^{N\\times P} and\\ b\\in \\mathbb{R}^P.\\\\\n",
    "Thus, y \\in \\mathbb{R}^{M\\times P}$\n",
    "- input matrix $x$: $M \\times N$\n",
    "- weight matrix W: $M \\times P$\n",
    "- bias vector b: P\n",
    "\n",
    "위 수식에서는 $x$의 표기는 벡터지만, 딥러닝 수행 시 **미니배치(mini-batch)** 기준으로 수행, x가 2차원 행렬이라고 가정\n",
    "\n",
    "$y = f(x;\\theta)\\ where\\ \\theta=\\{W,b\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def linear(x, W, b):\n",
    "    y = torch.mm(x,W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10)\n",
    "W = torch.FloatTensor(10, 5)\n",
    "b = torch.FloatTensor(5)\n",
    "\n",
    "y = linear(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 nn.Module\n",
    "\n",
    "Pytorch **nn.Module** class 제공, 사용자 필요한 모델 구조 구현 도움\n",
    "\n",
    "특징: \n",
    "- nn.Module 상속 객체 안에 nn.Module 상속 객체를 선언하여 변수로 사용 가능\n",
    "- nn.Module의 forward() 함수를 override 하여 feedforward를 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = torch.FloatTensor(input_size, output_size)\n",
    "        self.b = torch.FloatTensor(output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        y = torch.mm(X, self.W) + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parameters()** 함수는 모듈 내에 선언된 학습이 필요한 parameters을 변환하는 **iterator**이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 필요한 파라미터가 없기 때문에 빈 리스트가 반환된다.\n",
    "\n",
    "**신경망의 학습 파라미터는 단순한 tensor가 아니기에 파라미터로 등록되어야 한다.**\n",
    "\n",
    "파라미터로 등록하기 위해서는 **Parameter class** 로 래핑하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_size, output_size), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_size), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([10, 5]), torch.Size([5])]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinear(\n",
      "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 역전파 수행\n",
    "\n",
    "**Back-Propagation Algorithm: feedforward를 통해 얻은 값을 실제 정답값과의 차이를 계산해 오류(손실)을 뒤로 전달(back-propagation)**\n",
    "\n",
    "$x \\in \\mathbb{R}^{16 \\times 10}, \\theta=\\{W, b\\}\\ and\\ W\\in \\mathbb{R}^{10 \\times 5}, b\\in \\mathbb{R}^{5} \\\\\n",
    "\\hat{y} = x\\cdot W + b \\\\\n",
    "where\\ \\hat{y}\\in \\mathbb{R}^{16\\times 5}\\\\\n",
    "\\\\\n",
    "\\mathcal{L}(\\theta) = ||y - \\hat{y}||_2^2 \\\\\n",
    "\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\nabla_\\theta ||y - \\hat{y}||_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 100\n",
    "\n",
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10, 5)\n",
    "y = linear(x)\n",
    "loss = (objective - y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 train() & eval()\n",
    "\n",
    "train(), eval()을 사용하면 훈련, 추론 모드를 쉽게 전환할 수 있다.\n",
    "\n",
    "nn.Module을 상속받아 생성된 객체 기본적으로 **훈련 모드**\n",
    "\n",
    "eval()을 사용하여 추론 모드로 바꾸면, Dropout or Batch-Normalization 같은 학습과 추론시 다른 forward() 동작을 하는 Module들에 대해서도 각 상황에 따라 올바르게 동작한다. \n",
    "\n",
    "**추론(inference)가 끝나면 train()을 선언하여 다시 train mode로 돌아가게 해야한다.**\n",
    "\n",
    "\n",
    "### 3.3.7 선형회귀분석 예제\n",
    "\n",
    "1. 임의로 생성된 tensor들을\n",
    "2. 근사하고자 하는 정답 함수에 넣어 정답(y) 구함\n",
    "3. 그 정답과 신경망을 통과한 $\\hat{y}$와의 차이(error)를 MSE(평균제곱오차)통해 구해\n",
    "4. Stochastic Gradient Descent(SGD, 확률적 경사 하강법)을 통해 최적화 진행\n",
    "\n",
    "MSE: \n",
    "\n",
    "$\\mathcal{L}_{MSE}(\\hat{y}, y) = \\frac{1}{N} \\sum^{N}_{i=1}(\\hat{y}_i - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.linear = Linear(input_size, output_size) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 함수 $f$가 동작한다고 가정, 함수 $f$가 내부적으로 어떻게 동작하는지 파악하려면 **손실함수를 최소로 만드는 parameter $\\theta$를 찾아 함수 $f$를 근사해야 한다.**\n",
    "\n",
    "$y = f(x_1, x_2, x_3) = 3x_1 + x_2 -2x_3\\\\\n",
    "\\hat{y} = \\hat{f}(x_1, x_2, x_3;\\theta)\\\\\n",
    "\\hat{\\theta} = argmax_{\\theta \\in \\Theta} \\mathcal{L}(\\hat{y}, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
