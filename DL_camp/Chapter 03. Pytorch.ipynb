{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Deep Learning 구현 장비\n",
    "\n",
    "### 3.1.1 장비 구성\n",
    "\n",
    "- CPU: Core의 개수 보다 단일 클럭 높아야 함.\n",
    "    - 권장: i7\n",
    "\n",
    "- RAM: 메모리 다다익선\n",
    "    - 권장: 64GB\n",
    "    \n",
    "- GPU: 메모리 클수록 좋음(비용 비쌈)\n",
    "    - 권장: RTX 2080Ti\n",
    "    \n",
    "- Power Supply: 비싸고 검증된 브랜드\n",
    "\n",
    "- Cooling: GPU 발열 심해 쿨링 시스템 중요.\n",
    "\n",
    "\n",
    "### Why Pytorch?\n",
    "\n",
    "비슷한 레벨의 구현 난도를 가정 시, Pytorch가 TF 비해 뛰어난 생산성 \n",
    "\n",
    "장점:\n",
    "\n",
    "- Python First, 깔끔한 코드\n",
    "\n",
    "- Numpy와 뛰어난 호환성\n",
    "\n",
    "- Autograd\n",
    "\n",
    "- Dynamic graph\n",
    "\n",
    "\n",
    "## 3.3 Pytorch Tutorial\n",
    "\n",
    "### 3.3.1 Tensor\n",
    "\n",
    "Pytorch's tensor == Numpy's ndarray (같은 개념)\n",
    "\n",
    "파이토치 연산 수행의 가장 기본적인 객체\n",
    "\n",
    "### 3.3.2 Autograd\n",
    "\n",
    "**Autograd**: 자동으로 미분 및 역전파 수행 함수\n",
    "\n",
    "Pytorch는 tensor들 간에 연산을 수행할 때마다 동적으로 연산 그래프(computation graph) 생성하여 연산 결과물이 어떤 tensor로 부터 어떤 연산을 통해 왔는지 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor([1,2])\n",
    "y = torch.FloatTensor([1,2])\n",
    "y.requires_grad_(True)  \n",
    "\n",
    "z = (x+y) + torch.FloatTensor([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras, TF는 **미리 정의한 연산들을 컴파일 통해 고정 후**, **정해진 입력**에 맞춰 tensor를 feedforward 해야한다.\n",
    "\n",
    "Pytorch는 **정해진 연산이 없고**, Model은 학습해야할 parameter tensor만 미리 알고 있음, 그 weight parameter들이 어떠한 연산을 통해 학습 또는 연산에 관여하는지는 알 수 없다. (연산 수행된 직후에 알 수 있다.)\n",
    "\n",
    "**기울기 구할 필요 없는 연산의 경우:** **with torch.no_grad()**로 수행 가능\n",
    "\n",
    "$\\Rightarrow$ 역전파 알고리즘 수행이 필요 없는 비 학습 과정, 예측(prediction), 추론(inference) 수행시 유용 $\\to$ 연산 속도, 메모리 측면 이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor(2,2)\n",
    "y = torch.FloatTensor(2,2)\n",
    "y.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = (x+y) + torch.FloatTensor(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Feedforward\n",
    "\n",
    "선형 계층(Linear laye) or 완전연결계층(Fully-connected layer) 구현\n",
    "\n",
    "$y = xW + b\\\\\n",
    "where\\ x \\in \\mathbb{R}^{M\\times N}, W \\in \\mathbb{R}^{N\\times P} and\\ b\\in \\mathbb{R}^P.\\\\\n",
    "Thus, y \\in \\mathbb{R}^{M\\times P}$\n",
    "- input matrix $x$: $M \\times N$\n",
    "- weight matrix W: $M \\times P$\n",
    "- bias vector b: P\n",
    "\n",
    "위 수식에서는 $x$의 표기는 벡터지만, 딥러닝 수행 시 **미니배치(mini-batch)** 기준으로 수행, x가 2차원 행렬이라고 가정\n",
    "\n",
    "$y = f(x;\\theta)\\ where\\ \\theta=\\{W,b\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def linear(x, W, b):\n",
    "    y = torch.mm(x,W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10)\n",
    "W = torch.FloatTensor(10, 5)\n",
    "b = torch.FloatTensor(5)\n",
    "\n",
    "y = linear(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 nn.Module\n",
    "\n",
    "Pytorch **nn.Module** class 제공, 사용자 필요한 모델 구조 구현 도움\n",
    "\n",
    "특징: \n",
    "- nn.Module 상속 객체 안에 nn.Module 상속 객체를 선언하여 변수로 사용 가능\n",
    "- nn.Module의 forward() 함수를 override 하여 feedforward를 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = torch.FloatTensor(input_size, output_size)\n",
    "        self.b = torch.FloatTensor(output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        y = torch.mm(X, self.W) + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parameters()** 함수는 모듈 내에 선언된 학습이 필요한 parameters을 변환하는 **iterator**이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 필요한 파라미터가 없기 때문에 빈 리스트가 반환된다.\n",
    "\n",
    "**신경망의 학습 파라미터는 단순한 tensor가 아니기에 파라미터로 등록되어야 한다.**\n",
    "\n",
    "파라미터로 등록하기 위해서는 **Parameter class** 로 래핑하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_size, output_size), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_size), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinear(\n",
      "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 역전파 수행\n",
    "\n",
    "**Back-Propagation Algorithm: feedforward를 통해 얻은 값을 실제 정답값과의 차이를 계산해 오류(손실)을 뒤로 전달(back-propagation)**\n",
    "\n",
    "$x \\in \\mathbb{R}^{16 \\times 10}, \\theta=\\{W, b\\}\\ and\\ W\\in \\mathbb{R}^{10 \\times 5}, b\\in \\mathbb{R}^{5} \\\\\n",
    "\\hat{y} = x\\cdot W + b \\\\\n",
    "where\\ \\hat{y}\\in \\mathbb{R}^{16\\times 5}\\\\\n",
    "\\\\\n",
    "\\mathcal{L}(\\theta) = ||y - \\hat{y}||_2^2 \\\\\n",
    "\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\nabla_\\theta ||y - \\hat{y}||_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 100\n",
    "\n",
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10, 5)\n",
    "y = linear(x)\n",
    "loss = (objective - y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 train() & eval()\n",
    "\n",
    "train(), eval()을 사용하면 훈련, 추론 모드를 쉽게 전환할 수 있다.\n",
    "\n",
    "nn.Module을 상속받아 생성된 객체 기본적으로 **훈련 모드**\n",
    "\n",
    "eval()을 사용하여 추론 모드로 바꾸면, Dropout or Batch-Normalization 같은 학습과 추론시 다른 forward() 동작을 하는 Module들에 대해서도 각 상황에 따라 올바르게 동작한다. \n",
    "\n",
    "**추론(inference)가 끝나면 train()을 선언하여 다시 train mode로 돌아가게 해야한다.**\n",
    "\n",
    "\n",
    "### 3.3.7 선형회귀분석 예제\n",
    "\n",
    "1. 임의로 생성된 tensor들을\n",
    "2. 근사하고자 하는 정답 함수에 넣어 정답(y) 구함\n",
    "3. 그 정답과 신경망을 통과한 $\\hat{y}$와의 차이(error)를 MSE(평균제곱오차)통해 구해\n",
    "4. Stochastic Gradient Descent(SGD, 확률적 경사 하강법)을 통해 최적화 진행\n",
    "\n",
    "MSE: \n",
    "\n",
    "$\\mathcal{L}_{MSE}(\\hat{y}, y) = \\frac{1}{N} \\sum^{N}_{i=1}(\\hat{y}_i - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 함수 $f$가 동작한다고 가정, 함수 $f$가 내부적으로 어떻게 동작하는지 파악하려면 **손실함수를 최소로 만드는 parameter $\\theta$를 찾아 함수 $f$를 근사해야 한다.**\n",
    "\n",
    "$y = f(x_1, x_2, x_3) = 3x_1 + x_2 -2x_3\\\\\n",
    "\\hat{y} = \\hat{f}(x_1, x_2, x_3;\\theta)\\\\\n",
    "\\hat{\\theta} = argmax_{\\theta \\in \\Theta} \\mathcal{L}(\\hat{y}, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3 * x[:, 0] + x[:, 1] - 2 * x[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optim):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    y_hat = model(x)\n",
    "    loss = ((y - y_hat)**2).sum() / x.size(0)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter\n",
    "batch_size = 1\n",
    "n_epochs = 1000\n",
    "n_iter = 1000\n",
    "\n",
    "model = MyModel(3, 1)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0311) tensor(0.9000) tensor(0.2590)\n",
      "tensor(1.2999) tensor(0.9000) tensor(0.4634)\n",
      "tensor(0.9590) tensor(0.9000) tensor(0.5817)\n",
      "tensor(0.8494) tensor(0.9000) tensor(0.6696)\n",
      "tensor(0.7670) tensor(0.9000) tensor(0.7283)\n",
      "tensor(0.6828) tensor(0.9000) tensor(0.7737)\n",
      "tensor(0.6433) tensor(0.9000) tensor(0.8029)\n",
      "tensor(0.6332) tensor(0.9000) tensor(0.8310)\n",
      "tensor(0.6373) tensor(0.9000) tensor(0.8564)\n",
      "tensor(0.5910) tensor(0.9000) tensor(0.8588)\n",
      "tensor(0.5825) tensor(0.9000) tensor(0.8602)\n",
      "tensor(0.5835) tensor(0.9000) tensor(0.8697)\n",
      "tensor(0.5441) tensor(0.9000) tensor(0.8718)\n",
      "tensor(0.5101) tensor(0.9000) tensor(0.8761)\n",
      "tensor(0.5125) tensor(0.9000) tensor(0.8849)\n",
      "tensor(0.4567) tensor(0.9000) tensor(0.8919)\n",
      "tensor(0.4952) tensor(0.9000) tensor(0.8985)\n",
      "tensor(0.4560) tensor(0.9000) tensor(0.9006)\n",
      "tensor(0.4325) tensor(0.9000) tensor(0.9025)\n",
      "tensor(0.4230) tensor(0.9000) tensor(0.9093)\n",
      "tensor(0.3637) tensor(0.9000) tensor(0.9124)\n",
      "tensor(0.3961) tensor(0.9000) tensor(0.9209)\n",
      "tensor(0.3794) tensor(0.9000) tensor(0.9347)\n",
      "tensor(0.3608) tensor(0.9000) tensor(0.9213)\n",
      "tensor(0.3278) tensor(0.9000) tensor(0.9195)\n",
      "tensor(0.3480) tensor(0.9000) tensor(0.9239)\n",
      "tensor(0.3154) tensor(0.9000) tensor(0.9294)\n",
      "tensor(0.3210) tensor(0.9000) tensor(0.9278)\n",
      "tensor(0.3101) tensor(0.9000) tensor(0.9312)\n",
      "tensor(0.2918) tensor(0.9000) tensor(0.9358)\n",
      "tensor(0.2842) tensor(0.9000) tensor(0.9349)\n",
      "tensor(0.2658) tensor(0.9000) tensor(0.9349)\n",
      "tensor(0.2730) tensor(0.9000) tensor(0.9372)\n",
      "tensor(0.2498) tensor(0.9000) tensor(0.9472)\n",
      "tensor(0.2463) tensor(0.9000) tensor(0.9446)\n",
      "tensor(0.2393) tensor(0.9000) tensor(0.9446)\n",
      "tensor(0.2167) tensor(0.9000) tensor(0.9463)\n",
      "tensor(0.2172) tensor(0.9000) tensor(0.9469)\n",
      "tensor(0.2018) tensor(0.9000) tensor(0.9449)\n",
      "tensor(0.2134) tensor(0.9000) tensor(0.9416)\n",
      "tensor(0.2063) tensor(0.9000) tensor(0.9462)\n",
      "tensor(0.1895) tensor(0.9000) tensor(0.9462)\n",
      "tensor(0.1851) tensor(0.9000) tensor(0.9428)\n",
      "tensor(0.1687) tensor(0.9000) tensor(0.9463)\n",
      "tensor(0.1599) tensor(0.9000) tensor(0.9502)\n",
      "tensor(0.1720) tensor(0.9000) tensor(0.9506)\n",
      "tensor(0.1644) tensor(0.9000) tensor(0.9542)\n",
      "tensor(0.1558) tensor(0.9000) tensor(0.9556)\n",
      "tensor(0.1469) tensor(0.9000) tensor(0.9607)\n",
      "tensor(0.1454) tensor(0.9000) tensor(0.9585)\n",
      "tensor(0.1466) tensor(0.9000) tensor(0.9606)\n",
      "tensor(0.1264) tensor(0.9000) tensor(0.9573)\n",
      "tensor(0.1317) tensor(0.9000) tensor(0.9577)\n",
      "tensor(0.1212) tensor(0.9000) tensor(0.9580)\n",
      "tensor(0.1183) tensor(0.9000) tensor(0.9617)\n",
      "tensor(0.1190) tensor(0.9000) tensor(0.9619)\n",
      "tensor(0.1138) tensor(0.9000) tensor(0.9578)\n",
      "tensor(0.1067) tensor(0.9000) tensor(0.9564)\n",
      "tensor(0.1040) tensor(0.9000) tensor(0.9585)\n",
      "tensor(0.0999) tensor(0.9000) tensor(0.9604)\n",
      "tensor(0.1037) tensor(0.9000) tensor(0.9642)\n",
      "tensor(0.0934) tensor(0.9000) tensor(0.9605)\n",
      "tensor(0.0951) tensor(0.9000) tensor(0.9679)\n",
      "tensor(0.0833) tensor(0.9000) tensor(0.9670)\n",
      "tensor(0.0873) tensor(0.9000) tensor(0.9675)\n",
      "tensor(0.0835) tensor(0.9000) tensor(0.9678)\n",
      "tensor(0.0764) tensor(0.9000) tensor(0.9667)\n",
      "tensor(0.0769) tensor(0.9000) tensor(0.9678)\n",
      "tensor(0.0761) tensor(0.9000) tensor(0.9668)\n",
      "tensor(0.0715) tensor(0.9000) tensor(0.9688)\n",
      "tensor(0.0699) tensor(0.9000) tensor(0.9713)\n",
      "tensor(0.0677) tensor(0.9000) tensor(0.9711)\n",
      "tensor(0.0651) tensor(0.9000) tensor(0.9676)\n",
      "tensor(0.0621) tensor(0.9000) tensor(0.9693)\n",
      "tensor(0.0577) tensor(0.9000) tensor(0.9689)\n",
      "tensor(0.0558) tensor(0.9000) tensor(0.9712)\n",
      "tensor(0.0556) tensor(0.9000) tensor(0.9712)\n",
      "tensor(0.0538) tensor(0.9000) tensor(0.9707)\n",
      "tensor(0.0504) tensor(0.9000) tensor(0.9688)\n",
      "tensor(0.0490) tensor(0.9000) tensor(0.9687)\n",
      "tensor(0.0483) tensor(0.9000) tensor(0.9710)\n",
      "tensor(0.0450) tensor(0.9000) tensor(0.9699)\n",
      "tensor(0.0436) tensor(0.9000) tensor(0.9658)\n",
      "tensor(0.0457) tensor(0.9000) tensor(0.9680)\n",
      "tensor(0.0429) tensor(0.9000) tensor(0.9685)\n",
      "tensor(0.0422) tensor(0.9000) tensor(0.9692)\n",
      "tensor(0.0371) tensor(0.9000) tensor(0.9703)\n",
      "tensor(0.0388) tensor(0.9000) tensor(0.9709)\n",
      "tensor(0.0361) tensor(0.9000) tensor(0.9692)\n",
      "tensor(0.0364) tensor(0.9000) tensor(0.9692)\n",
      "tensor(0.0370) tensor(0.9000) tensor(0.9700)\n",
      "tensor(0.0319) tensor(0.9000) tensor(0.9667)\n",
      "tensor(0.0313) tensor(0.9000) tensor(0.9659)\n",
      "tensor(0.0335) tensor(0.9000) tensor(0.9662)\n",
      "tensor(0.0301) tensor(0.9000) tensor(0.9678)\n",
      "tensor(0.0289) tensor(0.9000) tensor(0.9680)\n",
      "tensor(0.0294) tensor(0.9000) tensor(0.9678)\n",
      "tensor(0.0255) tensor(0.9000) tensor(0.9696)\n",
      "tensor(0.0266) tensor(0.9000) tensor(0.9673)\n",
      "tensor(0.0259) tensor(0.9000) tensor(0.9645)\n",
      "tensor(0.0249) tensor(0.9000) tensor(0.9672)\n",
      "tensor(0.0248) tensor(0.9000) tensor(0.9660)\n",
      "tensor(0.0224) tensor(0.9000) tensor(0.9669)\n",
      "tensor(0.0219) tensor(0.9000) tensor(0.9680)\n",
      "tensor(0.0213) tensor(0.9000) tensor(0.9667)\n",
      "tensor(0.0201) tensor(0.9000) tensor(0.9670)\n",
      "tensor(0.0192) tensor(0.9000) tensor(0.9670)\n",
      "tensor(0.0189) tensor(0.9000) tensor(0.9667)\n",
      "tensor(0.0190) tensor(0.9000) tensor(0.9643)\n",
      "tensor(0.0181) tensor(0.9000) tensor(0.9649)\n",
      "tensor(0.0186) tensor(0.9000) tensor(0.9644)\n",
      "tensor(0.0164) tensor(0.9000) tensor(0.9643)\n",
      "tensor(0.0162) tensor(0.9000) tensor(0.9646)\n",
      "tensor(0.0153) tensor(0.9000) tensor(0.9647)\n",
      "tensor(0.0148) tensor(0.9000) tensor(0.9652)\n",
      "tensor(0.0152) tensor(0.9000) tensor(0.9652)\n",
      "tensor(0.0147) tensor(0.9000) tensor(0.9662)\n",
      "tensor(0.0139) tensor(0.9000) tensor(0.9653)\n",
      "tensor(0.0137) tensor(0.9000) tensor(0.9634)\n",
      "tensor(0.0124) tensor(0.9000) tensor(0.9627)\n",
      "tensor(0.0128) tensor(0.9000) tensor(0.9622)\n",
      "tensor(0.0124) tensor(0.9000) tensor(0.9634)\n",
      "tensor(0.0120) tensor(0.9000) tensor(0.9623)\n",
      "tensor(0.0115) tensor(0.9000) tensor(0.9629)\n",
      "tensor(0.0110) tensor(0.9000) tensor(0.9628)\n",
      "tensor(0.0107) tensor(0.9000) tensor(0.9624)\n",
      "tensor(0.0105) tensor(0.9000) tensor(0.9627)\n",
      "tensor(0.0094) tensor(0.9000) tensor(0.9628)\n",
      "tensor(0.0085) tensor(0.9000) tensor(0.9625)\n",
      "tensor(0.0094) tensor(0.9000) tensor(0.9616)\n",
      "tensor(0.0093) tensor(0.9000) tensor(0.9603)\n",
      "tensor(0.0085) tensor(0.9000) tensor(0.9595)\n",
      "tensor(0.0088) tensor(0.9000) tensor(0.9587)\n",
      "tensor(0.0078) tensor(0.9000) tensor(0.9599)\n",
      "tensor(0.0081) tensor(0.9000) tensor(0.9585)\n",
      "tensor(0.0082) tensor(0.9000) tensor(0.9587)\n",
      "tensor(0.0073) tensor(0.9000) tensor(0.9589)\n",
      "tensor(0.0073) tensor(0.9000) tensor(0.9579)\n",
      "tensor(0.0072) tensor(0.9000) tensor(0.9584)\n",
      "tensor(0.0066) tensor(0.9000) tensor(0.9574)\n",
      "tensor(0.0066) tensor(0.9000) tensor(0.9579)\n",
      "tensor(0.0062) tensor(0.9000) tensor(0.9566)\n",
      "tensor(0.0064) tensor(0.9000) tensor(0.9560)\n",
      "tensor(0.0058) tensor(0.9000) tensor(0.9561)\n",
      "tensor(0.0059) tensor(0.9000) tensor(0.9565)\n",
      "tensor(0.0056) tensor(0.9000) tensor(0.9552)\n",
      "tensor(0.0055) tensor(0.9000) tensor(0.9555)\n",
      "tensor(0.0053) tensor(0.9000) tensor(0.9548)\n",
      "tensor(0.0051) tensor(0.9000) tensor(0.9541)\n",
      "tensor(0.0051) tensor(0.9000) tensor(0.9527)\n",
      "tensor(0.0049) tensor(0.9000) tensor(0.9527)\n",
      "tensor(0.0048) tensor(0.9000) tensor(0.9525)\n",
      "tensor(0.0044) tensor(0.9000) tensor(0.9521)\n",
      "tensor(0.0041) tensor(0.9000) tensor(0.9515)\n",
      "tensor(0.0043) tensor(0.9000) tensor(0.9515)\n",
      "tensor(0.0039) tensor(0.9000) tensor(0.9519)\n",
      "tensor(0.0038) tensor(0.9000) tensor(0.9522)\n",
      "tensor(0.0041) tensor(0.9000) tensor(0.9517)\n",
      "tensor(0.0039) tensor(0.9000) tensor(0.9511)\n",
      "tensor(0.0038) tensor(0.9000) tensor(0.9506)\n",
      "tensor(0.0035) tensor(0.9000) tensor(0.9500)\n",
      "tensor(0.0034) tensor(0.9000) tensor(0.9491)\n",
      "tensor(0.0035) tensor(0.9000) tensor(0.9491)\n",
      "tensor(0.0032) tensor(0.9000) tensor(0.9486)\n",
      "tensor(0.0031) tensor(0.9000) tensor(0.9488)\n",
      "tensor(0.0030) tensor(0.9000) tensor(0.9482)\n",
      "tensor(0.0032) tensor(0.9000) tensor(0.9482)\n",
      "tensor(0.0028) tensor(0.9000) tensor(0.9480)\n",
      "tensor(0.0028) tensor(0.9000) tensor(0.9474)\n",
      "tensor(0.0026) tensor(0.9000) tensor(0.9473)\n",
      "tensor(0.0026) tensor(0.9000) tensor(0.9470)\n",
      "tensor(0.0025) tensor(0.9000) tensor(0.9471)\n",
      "tensor(0.0023) tensor(0.9000) tensor(0.9465)\n",
      "tensor(0.0024) tensor(0.9000) tensor(0.9459)\n",
      "tensor(0.0021) tensor(0.9000) tensor(0.9461)\n",
      "tensor(0.0023) tensor(0.9000) tensor(0.9455)\n",
      "tensor(0.0022) tensor(0.9000) tensor(0.9451)\n",
      "tensor(0.0021) tensor(0.9000) tensor(0.9446)\n",
      "tensor(0.0020) tensor(0.9000) tensor(0.9447)\n",
      "tensor(0.0020) tensor(0.9000) tensor(0.9445)\n",
      "tensor(0.0019) tensor(0.9000) tensor(0.9440)\n",
      "tensor(0.0020) tensor(0.9000) tensor(0.9436)\n",
      "tensor(0.0019) tensor(0.9000) tensor(0.9432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0018) tensor(0.9000) tensor(0.9429)\n",
      "tensor(0.0016) tensor(0.9000) tensor(0.9423)\n",
      "tensor(0.0016) tensor(0.9000) tensor(0.9422)\n",
      "tensor(0.0016) tensor(0.9000) tensor(0.9420)\n",
      "tensor(0.0016) tensor(0.9000) tensor(0.9414)\n",
      "tensor(0.0015) tensor(0.9000) tensor(0.9413)\n",
      "tensor(0.0015) tensor(0.9000) tensor(0.9411)\n",
      "tensor(0.0015) tensor(0.9000) tensor(0.9413)\n",
      "tensor(0.0014) tensor(0.9000) tensor(0.9406)\n",
      "tensor(0.0015) tensor(0.9000) tensor(0.9404)\n",
      "tensor(0.0014) tensor(0.9000) tensor(0.9400)\n",
      "tensor(0.0013) tensor(0.9000) tensor(0.9395)\n",
      "tensor(0.0012) tensor(0.9000) tensor(0.9390)\n",
      "tensor(0.0013) tensor(0.9000) tensor(0.9389)\n",
      "tensor(0.0012) tensor(0.9000) tensor(0.9382)\n",
      "tensor(0.0012) tensor(0.9000) tensor(0.9381)\n",
      "tensor(0.0011) tensor(0.9000) tensor(0.9380)\n",
      "tensor(0.0011) tensor(0.9000) tensor(0.9378)\n",
      "tensor(0.0011) tensor(0.9000) tensor(0.9372)\n",
      "tensor(0.0010) tensor(0.9000) tensor(0.9372)\n",
      "tensor(0.0010) tensor(0.9000) tensor(0.9367)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        x = torch.rand(batch_size, 3)\n",
    "        y = ground_truth(x.data)\n",
    "\n",
    "        loss = train(model, x, y, optim)\n",
    "\n",
    "        avg_loss += loss\n",
    "    avg_loss = avg_loss / n_iter\n",
    "\n",
    "    # simple test sample to check the network.\n",
    "    x_valid = torch.FloatTensor([[.3, .2, .1]])\n",
    "    y_valid = ground_truth(x_valid.data)\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "\n",
    "    print(avg_loss, y_valid.data[0], y_hat.data[0, 0])\n",
    "\n",
    "    if avg_loss < .001: # finish the training if the loss is smaller than .001.\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. nn.Module Class 상속받아 (forward 함수를 통해) 모델 아키텍처 클래스 선언\n",
    "\n",
    "2. 해당 클래스 객체 생성\n",
    "\n",
    "3. SGD나 Adam 등의 옵티마이저를 생성하고, 생성한 모델의 파라미터를 최적화 대상으로 등록\n",
    "\n",
    "4. 데이터로 미니배치를 구성하여 피드포워드 연산 그래프 생성\n",
    "\n",
    "5. 손실 함수를 통해 최종 결과값(scalar)과 손실값(loss) 계산\n",
    "\n",
    "6. 손실에 대해서 backward() 호출 $\\to$ 연산 그래프 상의 tensors의 기울기가 채워짐\n",
    "\n",
    "7. 3번의 옵티마이저에 step()을 호출하여 gradient descent 1step 수행\n",
    "\n",
    "8. 4번으로 돌아가 수렴 조건이 만족할 때가지 반복 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
