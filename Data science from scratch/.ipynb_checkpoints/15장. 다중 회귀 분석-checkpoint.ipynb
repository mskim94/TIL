{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 회귀 분석\n",
    "앞의 단순선형회귀에 추가적인 데이터를 사용해서 모델의 성능을 높여보자. \\\n",
    "시간(분) = $\\alpha$ + ${\\beta}_1 친구수$ + ${\\beta}_2 근무시간$ + ${\\beta}_3 박사 학위 취득 여부$ + $\\epsilon$ \\\n",
    "박사 학위 취득 여부는 숫자 데이터가 아니다. 하지만 앞서 11장 '기계학습'에서 다룬 바와 같이 **가변수(dummy variable)** 를 만들어서 박사 학위를 가진 사람을 1, 그렇지 않은 사람을 0으로 표기하면 숫자로 표기할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 모델\n",
    "14장. ' 단순 회귀 분석' 에서는 아래와 같은 형태의 모델을 다뤘다. \\\n",
    "$y_i = \\alpha + \\beta x_i + \\epsilon_i$ \\\n",
    "각 입력값 $x_i$는 숫자 하나가 아니라 k개의 숫자인 $x_i1, ..., x_ik$ 라고 한다면 다중 회귀(multiple regression)모델은 다음과 같은 형태를 띈다. \\\n",
    "$y_i = \\alpha + \\beta_1 x_i + ... + \\beta_k x_ik + \\epsilon_i$ \\\n",
    "다중 회귀 분석에서는 보통 파라미터 벡터를 $\\beta$라고 부른다. 여기에 상수항까지 포함시키기 위해 데이터의 앞부분에 1로 구성된 열을 덧붙이면 된다. \\\n",
    "beta = [alpha, beta_1, ..., beta_k] \\\n",
    "그리고 각 데이터는 다음과 같다. \\\n",
    "x_i = [1, x_i1, ..., x_ik] \\\n",
    "이렇게 하면 모델을 다음과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import math\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w),  \"vectors must be same length\"\n",
    "    \n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"각 x_i의 첫 번째 항목은 1이라고 가정\"\"\"\n",
    "    return dot(x, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 49, 4, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 경우 독립 변수 x는 각각 다음과 같은 벡터들의 열로 표현할 수 있다.\n",
    "[1, # 상수항\n",
    "49, # 친구의 수\n",
    "4, # 하루 근무 시간\n",
    "0] # 박사 학위 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 최소자승법에 대한 몇 가지 추가 가정\n",
    "이 모델(그리고 답)이 의미가 있으려면 몇 가지 추가적인 가정이 필요하다. \\\n",
    "첫번째로 x의 열은 서로 **선형독립(linear independence)** 해야 한다. \\\n",
    "선형독립이란? \\\n",
    "어떤 벡터도 다른 벡터의 선형결합으로 만들어질 수 없다는 것을 의미한다. 만약 이 가정이 성립하지 않는다면 베타를 추정할 수 없다. 극단적인 예로 num_friends와 동일한 num_acquaintances가 데이터에 추가되었다고 해보자. \\\n",
    "그렇다면 어떠한 beta를 사용해도 num_friends 계수에 임의의 값을 더하고 num_acquaintances 계수에서 똑같은 값을 빼주면 모델의 예측값은 변하지 않을 것이다. 즉, num_friends의 정확한 계수를 계산할 수 없다는 것의 의미한다.(보통은 이 가정이 위배되었는지 확인하는 것은 쉽지 않다.) \\\n",
    "두번째로 중요한 가정은 $x$의 모든 열의 오류 $\\epsilon$과 아무런 상관 관계가 없다는 것이다. 만약 이 가정이 위배되는 경우 아예 잘못된 beta가 추정될 것이다. \\\n",
    "예를 들어, 14장 '단순 회귀 분석'에서 만든 모델을 살펴보면 친구 수 가 한 명씩 증가할 때 사용자가 하루 평균 사이트에서 보내는 시간이 0.90분씩 증가한다고 예측되었다. 또한 다음과 같은 경우가 있다고 생각해 보자. \n",
    "- 근무 시간이 더 긴 사람은 더 적은 시간을 사이트에서 보낼 것이다.\n",
    "- 친구 수가 많은 사람일수록 근무 시간이 더 길다.\n",
    "\n",
    "이 경우 '실제' 모델은 다음과 같다.\\\n",
    "사이트에서 보내는 시간(분) = $\\alpha$  + $\\beta_1 친구수$ + $\\beta_2 근무 시간$ + $\\epsilon$ \n",
    "\n",
    "만약 실제 모델의 $\\beta_1$(즉, '실제' 모델의 오류를 최소화하는 $\\beta_1$)을 사용하는 단일 변수 모델로 예측 성능을 평가해 보자. $\\beta_2$ < 0 이지만 모델에 포함시키지 않았기 때문에 근무 시간이 긴 사용자의 예측값은 너무 크게 계산될 것이고, 그눔 시간이 짧은 사용자의 예측값은 조금 더 크게 계산될 것이다. 또한 근무 시간과 친구의 수는 양의 상관관계를 지니기 때문에 친구 수가 많은 사용자의 예측값은 너무 크게 계산될 것이고, 친구 수가 적은 사용자의 예측값은 조금만 크게 계산될 것이다. \\\n",
    "결국 단일 변수 모델의 오류를 줄이기 위해서는 추정된 $\\beta_1$을 줄여야 한다. 즉, 오류를 최소화하는 $\\beta_1$은 실제 값보다 작아진다는 것을 의미한다. 결국 단일 변수 모델 안의 최소자승법의 결과는 $\\beta_1$을 과소평가하도록 편향된다. 일반적으로 이렇게 독립 변수와 오류 사이에 상관관계가 존재한다면, 최소자승법으로 만들어지는 모델을 $\\beta_1$를 추정해 준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 모델 학습하기\n",
    "단순 회귀 분석 모델처럼 오류를 제곱한 값의 합을 최소화해 주는 beta를 찾을 것이다. 경사 하강법을 사용하자. 오류의 제곱 합을 최소화할 것이며 이 경우 오류 함수는 14장 '단순 회귀 분석'에서 사용한 것과 같지만 [alpha, beta]를 받는 대신 임의의 벡터를 받도록 수정할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "y = 30\n",
    "beta = [4,4,4]  # 예측 결과 = 4 + 8 + 12 = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert error(x, y, beta) == -6\n",
    "assert squared_error(x, y, beta) == 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 데이터를 다룰 수 있는 least-squares_fit 함수를 작성하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"모든 성분을 c로 곱하기\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"모든 벡터의 각 성분들끼리 더한다.\"\"\"\n",
    "    # vectors가 비어있는지 확인\n",
    "    assert vectors, \"no vectors provided\"\n",
    "    \n",
    "    # 모든 벡터의 길이가 동일한지 확인\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "    \n",
    "    # i번째 결과값은 모든 벡터의 i번째 성분을 더한 값\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"각 성분별 평균을 계산\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"v에서 step_size만큼 이동하기\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(xs: List[Vector],\n",
    "                     ys: List[float],\n",
    "                     learning_rate: float = 0.001,\n",
    "                     num_steps: int = 1000,\n",
    "                     batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    오류의 제곱 합을 최소화하는 beta를 찾자.\n",
    "    모델은 y = dot(x, beta)라 가정하자.\n",
    "    \"\"\"\n",
    "    # 임의의 위치에서 출발\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start: start+batch_size]\n",
    "            batch_ys = ys[start: start+batch_size]\n",
    "            \n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "    \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5장. 통계 import daily_minutes_good\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"v에서 step_size만큼 이동하기\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-39765675280d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;36m30.50\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m30.70\u001b[0m  \u001b[0;31m# 상수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;36m0.96\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.00\u001b[0m  \u001b[0;31m# 친구 수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "# 시행착오를 통해 num_iters와 step_size를 선택하였다.\n",
    "# 실행하는 데 시간이 좀 걸릴 것이다.\n",
    "learning_rate = 0.001\n",
    "\n",
    "beta = least_squares_fit(inputs, daily_minutes_good, learning_rate, 5000, 25)\n",
    "assert 30.50 < beta[0] < 30.70  # 상수\n",
    "assert 0.96 < beta[1] < 1.00  # 친구 수\n",
    "assert -1.89 < beta[2] < -1.85  # 일 업무 시간\n",
    "assert 0.91 < beta[3] < 0.94  # 박사 학위 여부\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로는 경사 하강법으로 선형 회귀 모델을 추정하지 않는다. 선형대수 기법을 사용하면 정확한 계수를 구할 수 있지만 이 책에서 다룰 내용은 아니다. 만약 선형대수 방식을 사용했다면 다음과 같은 수식을 찾았을 것이다. \\\n",
    "사이트에서 보낸 시간(분) = 30.58 + 0.972 친구 수 - 1.87 근무 시간 + 0.923 박사 학위 취득 여부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 모델 해석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 계수는 다른 모든 것이 동일할 때 해당 항목의 영향력을 나타낸다. \\\n",
    "다른 모든 것이 동일할 때 친구 수가 한 명 증가하면 사용자가 하루 평균 사이트에서 보내는 시간은 1분 증가한다. \\\n",
    "다른 모든 것이 동일할 때 근무 시간이 한 시간 증가하면 사용자가 하루 평균 사이트에서 보내는 시간은 대략 2분 감소한다. 다른 모든 것이 동이할 때 박사 학위를\n",
    "취득했다면 사용자는 하루 평균 사이트를 1분 더 사용한다. \\\n",
    "위의 해석은 변수 간의 관계를 직접적으로 설명해 주지 못한다. \\\n",
    "\n",
    "e.g) \\\n",
    "친구의 수가 많은 사용자의 근무 시간과, 적은 사용자들의 근무 시간은 서로 다른 방식으로 동작할 수도 있다. 이 모델은 이러한 관계를 잡아내지 못한다. \\\n",
    "이러한 문제를 다루는 방법 중 하나는 친구 수와 근무 시간을 곱한 새로운 변수를 도입하는 것이다. 이를 통해 친구의 수가 증가할 수록 근무 시간의 계수를 증가(혹은 감소)시킬 수 있다. \\\n",
    "혹은 친구 수가 증가할수록 사이트에서 보내는 시간은 어느 일정한 수준까지 증가하고, 그 이후로는 사이트에서 보내는 시간이 감소할 수도 있다. 이러한 현상은 친구수를 제곱한 값을 모델의 변수로 사용해서 잡아낼 수 있다. \\\n",
    "변수가 점점 추가되기 시작하면 각 계수가 유의미한지 살펴봐야 한다. 변수끼리 곱한 값, 변수의 log값, 변수의 제곱 값 등 수많은 새로운 변수를 추가할 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5 적합성(Goodness of fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"x의 모든 데이터 포인트에서 평균을 뺌(평균을 0으로 만들기 위해)\"\"\"\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "def total_sum_of_squares(y: Vector) -> float:\n",
    "    \"\"\"평균을 기준으로 y_i의 변화량을 제곱한 값의 총합\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, beta) ** 2 for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-678fc2a5c357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m0.67\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmultiple_r_squared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.68\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta) < 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 회귀 분석 모델에 새로운 변수를 추가하면 R 제곱 값이 어쩔 수 없이 증가할 수 밖에 없다는 것을 기억! \\\n",
    "단순 회귀 모델은 근무 시간과 박사 학위 취득 여부의 계수가 0인 다중 회귀 분석 모델과 동일하다. \\\n",
    "즉, 최적의 다중 회귀 분석 모델은 언제나 단순 회귀 모델보다 같거나 작은 오류값을 가질 것이다. \\\n",
    "이러한 이유 때문에 다중 회귀 분석 모델에서는 각 계수의 **표준 오차(standard errors)** 를 살펴봐야 한다. \\\n",
    "계수이 표준 오차는 추정된 $\\beta_i$의 계수가 얼마나 확실한지 알려준다. 모델 자체는 주어진 데이터에 적합(fit)할 수도 있지만, 몇몇 독립 변수 간에 어떠한 상관관계가 있따면 이 변수들의 계수는 무의미할 수 있다. \\\n",
    "오차를 측정하는 일반적인 방법은 각 오류는 $\\epsilon_i$는 독립이며, 평균은 0이고 표준 편차는 $\\sigma$인 정규분포의 확률변수라는 추가적인 가정에서 시작 \\\n",
    "이런 한 가정을 기반으로 우리는 선형대수를 이용해서 각 계수의 표준 오차를 계산해 준다. 표준 오차가 클수록 모델이 해당 계수에 대해 확신을 갖지 못한다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6 여담: 부트스트랩\n",
    "알 수 없는 분포에서 생성된 표본 데이터 n개가 주어졌다고 상상해보자. \n",
    "\n",
    "data = get_sample(num_points=n) \n",
    "\n",
    "앞 장의 통계에서 표본 데이터의 중앙값을 계산해 주는 함수를 만들어 봤다. 이 중앙값을 분포 자체의 중앙값에 대한 추정치로 사용할 수 있다. \\\n",
    "하지만 과연 이 추정치를 어느 정도로 믿을 수 있을까? 만약 표본 데이터가 모두 100 근처에 위치하고 있다면 중앙값 또한 아마 100 근처에 위치하곻 있을 것이다. 표본 데이터의 반은 0 근처에 위치하고 나머지 반은 200 근처에 위치한다면 추정된 중앙값을 앞선 경우처럼 신뢰하기는 힘들다. \\\n",
    "만약 새로운 표본을 계속 얻을 수 있다면 각 표본의 중앙값을 계산해 보고 이 값들의 분포를 살펴볼 수 있다. 하지만 보통 새로운 표본을 계속 얻을 수는 없다. 대신 **부트스트랩(bootstrap)** 이라는 새로운 표본 데이터에서 중복이 허용된 재추출을 통해 새로운 n개의 데이터를 생성한다. 그리고 만들어진 인공 데이터로 중앙값을 계산해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Callable\n",
    "\n",
    "X = TypeVar('X')  # 데이터를 위한 일반 타입\n",
    "Stat = TypeVar('Stat')  # 통계치를 위한 일반 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data: List[X]) -> List[X]:\n",
    "    \"\"\"len(data)개의 항목을 중복을 허용한 무작위 재추출\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data: List[X],\n",
    "                      stats_fn: Callable[[List[X]], Stat],\n",
    "                      num_samples: int) -> List[Stat]:\n",
    "    \"\"\"num_samples개의 부트스트랩된 표본에 대해 stats_fn을 적용\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 다음과 같은 두 가지 데이터를 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101개의 데이터가 모두 100에 인접\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101개의 데이터 중 50개는 0에 인접, 50개는 200에 인접\n",
    "far_from_100 = ([99.5 + random.random()] + \n",
    "               [random.random() for _ in range(50)] +\n",
    "               [200 + random.random() for _ in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 데이터의 중앙값을 계산해 보면 둘 다 대략 100에 가까운 것을 확인할 수 있다. 하지만 다음과 같이 부트스트랩을 적용해 보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w),  \"vectors must be same length\"\n",
    "    \n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))\n",
    "\n",
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"v_1 * v_1 + ... v_n * v_n\"\"\"\n",
    "    return dot(v,v)\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"편차의 제곱의 평균\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "    \n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def _median_odd(xs: List[float]) -> float:\n",
    "    \"\"\"len(xs)가 홀수면 중앙값을 반환\"\"\"\n",
    "    return sorted(xs)[len(xs) // 2]\n",
    "\n",
    "def _median_even(xs: List[float]) -> float:\n",
    "    \"\"\"len(xs)가 짝수면 두 중앙값의 평균을 반환\"\"\"\n",
    "    sorted_xs = sorted(xs)\n",
    "    hi_midpoint = len(xs) // 2  # e.g. length = 4 => hi_midpoint = 2\n",
    "    return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2\n",
    "\n",
    "def median(v: List[float]) -> float:\n",
    "    \"\"\"v의 중앙값을 계산\"\"\"\n",
    "    return _median_even(v) if len(v) % 2 == 0 else _median_odd(v)\n",
    "\n",
    "import math\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"표준편차는 분산의 제곱근\"\"\"\n",
    "    return math.sqrt(variance(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians_close = bootstrap_statistic(close_to_100, median, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 데이터는 대부분의 중앙값이 대략 100인 것을 확인할 수 있다. 하지만 다음과 같은 경우에는 대부분의 중앙값이 0에 가깝거나 200에 가까운 것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians_far = bootstrap_statistic(far_from_100, median, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 데이터의 중앙값들의 표준편차는 0에 가까운 것을 확인할 수 있지만 두 번째 데이터의 경우, 중앙값의 표준편차가 100에 가까운 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert standard_deviation(medians_close) < 1\n",
    "assert standard_deviation(medians_far) > 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 이렇게 극단적인 경우에는 데이터를 직접 살펴보면 문제를 쉽게 파악 할 수 있지만, 대부분의 경우 데이터만 살펴보는 것으로는 부족하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.7 계수의 표준 오차\n",
    "계수의 표준 오차를 추정할 때도 부트스트랩을 적용할 수 있다. 주어진 데이터를 bootstrap_sample 함수에 반복적으로 넣어서 표본을 생성하고, 각각의 표본으로 부터 beta를 추정해 볼 수 있다. 예를 들어 모든 표본 데이터에서 친구 수에 해당되는 독립 변수의 계수가 크게 변하지 않는다면 추정된 계수가 꽤 정확하다고 볼 수 있다. 하지만 만약 계수가 표본에 따라 크게 변한다면 추정된 계수를 그다지 신뢰할 수 없다. \\\n",
    "이 과정에서 한 가지 유의해야 할 점은 하나의 데이터에 속하는 x와 y를 zip으로 묶어서 짝이 되는 독립 변수와 종속 변수가 함께 추출될 수 있게 해줘야 한다는 점이다. 즉, bootstrap_sample은 (x_i, y_i) 형태의 리스트를 반환해 줄 것이다. 그리고 반환된 데이터를 x_sample과 y_sample로 다시 나눠 줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sample_beta(pairs: List[Tuple[Vector, float]]):\n",
    "    x_sample = [x for x, _ in pairs]\n",
    "    y_sample = [y for _, y in pairs]\n",
    "    beta = least_squares_fit(x_sample, y_sample, learning_rate, 5000,25)\n",
    "    print(\"bootstrap sample\", beta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)  # so that you get the same results as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8f68f720ca6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 이것은 몇 분 걸릴 것이다!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m bootstrap_betas = bootstrap_statistic(list(zip(inputs, daily_minutes_good)),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                      estimate_sample_beta, 100)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# 이것은 몇 분 걸릴 것이다!\n",
    "bootstrap_betas = bootstrap_statistic(list(zip(inputs, daily_minutes_good)),\n",
    "                                     estimate_sample_beta, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 계수의 표준 오차를 추정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bootstrap_betas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-dda09961da09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m bootstrap_standard_errors = [\n\u001b[1;32m      2\u001b[0m     \u001b[0mstandard_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbootstrap_betas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                       for i in range(4)]\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap_standard_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-dda09961da09>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m bootstrap_standard_errors = [\n\u001b[1;32m      2\u001b[0m     \u001b[0mstandard_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbootstrap_betas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                       for i in range(4)]\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap_standard_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bootstrap_betas' is not defined"
     ]
    }
   ],
   "source": [
    "bootstrap_standard_errors = [\n",
    "    standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "                      for i in range(4)]\n",
    "print(bootstrap_standard_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.218 보충"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8 Regularization\n",
    "실제로 데이터를 분석할 때는 변수가 굉장히 많은 데이터에 회귀 분석 모델을 적용해야 하는 경우가 자주 발생. 하지만 변수가 너무 많다면 다양한 문제가 발생할 수 있다. 첫 번째로 변수가 많아질수록 모델이 학습 데이터에 오버피팅하기 쉬워진다. 두 번째로 0이 아닌 계수가 많아질수록 모델을 해석하기 어려워니다. 만약 어떤 현상을 설명하는 것이 목표라면 수백 개의 변수로 모델을 만드는 것보다는 세 개 정도의 변수로 작은 모델을 만드는 것이 더 나을 수 있다. \\\n",
    "**Regularization** 은 beta가 커디면 커질수록 해당 모델에게 패널티를 주는 방법이다. \\\n",
    "그리고 오류와 패널티를 동시에 최소화하는 최적의 모델을 만들 수 있다. \\\n",
    "패널티를 더욱 강조할수록 값이 큰 계수에 대한 제한이 더욱 커진다. \\\n",
    "예를 들어 **리지 회귀(ridge regression)** 의 경우 상수인 beta_0를 제외한 beta_i의 각각을 제곱한 값의 합에 비례하는 패널티를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha는 패널티의 강도를 조절하는 *하이퍼 파라미터*\n",
    "# 보통 'lambda'라고 표현하지만 파이썬에서는 이미 사용 중인 키워드 이다.\n",
    "def ridge_penalty(beta: Vector, alpha: float) -> float:\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared_error_ridge(x: Vector,\n",
    "                       y: float,\n",
    "                       beta: Vector,\n",
    "                       alpha: float) -> float:\n",
    "    \"\"\"beta를 사용할 때 오류와 패널티의 합을 추정\"\"\"\n",
    "    return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 동일하게 경사하강법을 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"각 성분끼리 더한다.\"\"\"\n",
    "    assert len(v) == len(w), \"vector must be the same length\"\n",
    "    \n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_ridge_gradient(x: Vector,\n",
    "                          y: float,\n",
    "                          beta: Vector,\n",
    "                          alpha: float) -> Vector:\n",
    "    \"\"\"\n",
    "    i번째 오류 제곱 값과 리지 패널티 합의 기울기\n",
    "    \"\"\"\n",
    "    return add(sqerror_ridge_gradient(x, y, beta),\n",
    "              ridge_penalty_gradient(beta, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.220 내용 추가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
