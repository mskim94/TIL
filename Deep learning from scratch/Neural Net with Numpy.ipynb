{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNet Multi-Layer Perceptron!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function: ReLU\n",
    "# Loss Function: Cross-Entropy Error\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwoLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# TwoLayer\n",
    "class NeuralNetMLP:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "#     def numerical_gradient(self, x, t):\n",
    "#         loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "#         grads = {}\n",
    "#         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "#         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "#         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "#         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "#         return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 / Loss : 1.9066 / Train Acc : 76.0000% / Test Acc : 77.1500%\n",
      "Epoch : 1 / Loss : 0.8359 / Train Acc : 85.0517% / Test Acc : 85.8100%\n",
      "Epoch : 2 / Loss : 0.5351 / Train Acc : 87.5600% / Test Acc : 88.1800%\n",
      "Epoch : 3 / Loss : 0.4402 / Train Acc : 88.7400% / Test Acc : 89.3400%\n",
      "Epoch : 4 / Loss : 0.3935 / Train Acc : 89.4483% / Test Acc : 89.7900%\n",
      "Epoch : 5 / Loss : 0.3653 / Train Acc : 89.9167% / Test Acc : 90.2700%\n",
      "Epoch : 6 / Loss : 0.3459 / Train Acc : 90.3933% / Test Acc : 90.8600%\n",
      "Epoch : 7 / Loss : 0.3310 / Train Acc : 90.6717% / Test Acc : 91.1400%\n",
      "Epoch : 8 / Loss : 0.3187 / Train Acc : 90.9583% / Test Acc : 91.4400%\n",
      "Epoch : 9 / Loss : 0.3084 / Train Acc : 91.2850% / Test Acc : 91.6500%\n",
      "Epoch : 10 / Loss : 0.2992 / Train Acc : 91.4967% / Test Acc : 91.8200%\n",
      "Epoch : 11 / Loss : 0.2908 / Train Acc : 91.8117% / Test Acc : 92.0900%\n",
      "Epoch : 12 / Loss : 0.2831 / Train Acc : 91.9633% / Test Acc : 92.3400%\n",
      "Epoch : 13 / Loss : 0.2757 / Train Acc : 92.2533% / Test Acc : 92.5600%\n",
      "Epoch : 14 / Loss : 0.2688 / Train Acc : 92.4033% / Test Acc : 92.6900%\n",
      "Epoch : 15 / Loss : 0.2624 / Train Acc : 92.6067% / Test Acc : 92.9800%\n",
      "Epoch : 16 / Loss : 0.2562 / Train Acc : 92.8100% / Test Acc : 93.0100%\n",
      "Epoch : 17 / Loss : 0.2501 / Train Acc : 92.9600% / Test Acc : 93.2200%\n",
      "Epoch : 18 / Loss : 0.2443 / Train Acc : 93.1183% / Test Acc : 93.2900%\n",
      "Epoch : 19 / Loss : 0.2387 / Train Acc : 93.2983% / Test Acc : 93.4700%\n",
      "Epoch : 20 / Loss : 0.2333 / Train Acc : 93.4767% / Test Acc : 93.6000%\n",
      "Epoch : 21 / Loss : 0.2285 / Train Acc : 93.6617% / Test Acc : 93.7400%\n",
      "Epoch : 22 / Loss : 0.2235 / Train Acc : 93.8033% / Test Acc : 93.7700%\n",
      "Epoch : 23 / Loss : 0.2188 / Train Acc : 93.8700% / Test Acc : 93.8700%\n",
      "Epoch : 24 / Loss : 0.2141 / Train Acc : 94.0867% / Test Acc : 93.9900%\n",
      "Epoch : 25 / Loss : 0.2098 / Train Acc : 94.1683% / Test Acc : 94.0800%\n",
      "Epoch : 26 / Loss : 0.2054 / Train Acc : 94.2350% / Test Acc : 94.1700%\n",
      "Epoch : 27 / Loss : 0.2015 / Train Acc : 94.3517% / Test Acc : 94.2400%\n",
      "Epoch : 28 / Loss : 0.1976 / Train Acc : 94.4467% / Test Acc : 94.3000%\n",
      "Epoch : 29 / Loss : 0.1936 / Train Acc : 94.5750% / Test Acc : 94.3500%\n",
      "Epoch : 30 / Loss : 0.1896 / Train Acc : 94.6833% / Test Acc : 94.4400%\n",
      "Epoch : 31 / Loss : 0.1861 / Train Acc : 94.7750% / Test Acc : 94.6600%\n",
      "Epoch : 32 / Loss : 0.1826 / Train Acc : 94.8783% / Test Acc : 94.6900%\n",
      "Epoch : 33 / Loss : 0.1792 / Train Acc : 94.9700% / Test Acc : 94.8400%\n",
      "Epoch : 34 / Loss : 0.1757 / Train Acc : 95.1033% / Test Acc : 94.8400%\n",
      "Epoch : 35 / Loss : 0.1726 / Train Acc : 95.1533% / Test Acc : 94.9200%\n",
      "Epoch : 36 / Loss : 0.1694 / Train Acc : 95.2433% / Test Acc : 95.1200%\n",
      "Epoch : 37 / Loss : 0.1662 / Train Acc : 95.2900% / Test Acc : 95.0700%\n",
      "Epoch : 38 / Loss : 0.1635 / Train Acc : 95.4117% / Test Acc : 95.1500%\n",
      "Epoch : 39 / Loss : 0.1605 / Train Acc : 95.4933% / Test Acc : 95.2400%\n",
      "Epoch : 40 / Loss : 0.1576 / Train Acc : 95.5167% / Test Acc : 95.3200%\n",
      "Epoch : 41 / Loss : 0.1550 / Train Acc : 95.6283% / Test Acc : 95.4100%\n",
      "Epoch : 42 / Loss : 0.1524 / Train Acc : 95.7033% / Test Acc : 95.3700%\n",
      "Epoch : 43 / Loss : 0.1499 / Train Acc : 95.7617% / Test Acc : 95.5200%\n",
      "Epoch : 44 / Loss : 0.1473 / Train Acc : 95.8167% / Test Acc : 95.5500%\n",
      "Epoch : 45 / Loss : 0.1449 / Train Acc : 95.9450% / Test Acc : 95.5500%\n",
      "Epoch : 46 / Loss : 0.1425 / Train Acc : 96.0050% / Test Acc : 95.5800%\n",
      "Epoch : 47 / Loss : 0.1404 / Train Acc : 96.0183% / Test Acc : 95.6600%\n",
      "Epoch : 48 / Loss : 0.1378 / Train Acc : 96.0967% / Test Acc : 95.7500%\n",
      "Epoch : 49 / Loss : 0.1358 / Train Acc : 96.1683% / Test Acc : 95.8100%\n",
      "Epoch : 50 / Loss : 0.1336 / Train Acc : 96.2350% / Test Acc : 95.8400%\n",
      "Epoch : 51 / Loss : 0.1316 / Train Acc : 96.3233% / Test Acc : 95.8900%\n",
      "Epoch : 52 / Loss : 0.1297 / Train Acc : 96.3500% / Test Acc : 96.0100%\n",
      "Epoch : 53 / Loss : 0.1277 / Train Acc : 96.4233% / Test Acc : 96.0100%\n",
      "Epoch : 54 / Loss : 0.1257 / Train Acc : 96.4767% / Test Acc : 96.1100%\n",
      "Epoch : 55 / Loss : 0.1239 / Train Acc : 96.5183% / Test Acc : 96.1300%\n",
      "Epoch : 56 / Loss : 0.1222 / Train Acc : 96.5683% / Test Acc : 96.1500%\n",
      "Epoch : 57 / Loss : 0.1203 / Train Acc : 96.6283% / Test Acc : 96.2600%\n",
      "Epoch : 58 / Loss : 0.1185 / Train Acc : 96.6900% / Test Acc : 96.2100%\n",
      "Epoch : 59 / Loss : 0.1170 / Train Acc : 96.7317% / Test Acc : 96.2300%\n",
      "Epoch : 60 / Loss : 0.1153 / Train Acc : 96.7983% / Test Acc : 96.3100%\n",
      "Epoch : 61 / Loss : 0.1137 / Train Acc : 96.8267% / Test Acc : 96.3200%\n",
      "Epoch : 62 / Loss : 0.1123 / Train Acc : 96.8883% / Test Acc : 96.3200%\n",
      "Epoch : 63 / Loss : 0.1107 / Train Acc : 96.9267% / Test Acc : 96.3500%\n",
      "Epoch : 64 / Loss : 0.1092 / Train Acc : 96.9683% / Test Acc : 96.4000%\n",
      "Epoch : 65 / Loss : 0.1078 / Train Acc : 97.0283% / Test Acc : 96.4400%\n",
      "Epoch : 66 / Loss : 0.1063 / Train Acc : 97.0733% / Test Acc : 96.4900%\n",
      "Epoch : 67 / Loss : 0.1050 / Train Acc : 97.0833% / Test Acc : 96.5600%\n",
      "Epoch : 68 / Loss : 0.1035 / Train Acc : 97.1650% / Test Acc : 96.5300%\n",
      "Epoch : 69 / Loss : 0.1021 / Train Acc : 97.1917% / Test Acc : 96.5100%\n",
      "Epoch : 70 / Loss : 0.1010 / Train Acc : 97.2167% / Test Acc : 96.6300%\n",
      "Epoch : 71 / Loss : 0.0997 / Train Acc : 97.2733% / Test Acc : 96.6600%\n",
      "Epoch : 72 / Loss : 0.0985 / Train Acc : 97.3200% / Test Acc : 96.6300%\n",
      "Epoch : 73 / Loss : 0.0973 / Train Acc : 97.3433% / Test Acc : 96.7200%\n",
      "Epoch : 74 / Loss : 0.0960 / Train Acc : 97.3733% / Test Acc : 96.7300%\n",
      "Epoch : 75 / Loss : 0.0949 / Train Acc : 97.4183% / Test Acc : 96.7200%\n",
      "Epoch : 76 / Loss : 0.0937 / Train Acc : 97.4267% / Test Acc : 96.7900%\n",
      "Epoch : 77 / Loss : 0.0927 / Train Acc : 97.4500% / Test Acc : 96.7900%\n",
      "Epoch : 78 / Loss : 0.0916 / Train Acc : 97.4817% / Test Acc : 96.9000%\n",
      "Epoch : 79 / Loss : 0.0906 / Train Acc : 97.5700% / Test Acc : 96.8600%\n",
      "Epoch : 80 / Loss : 0.0895 / Train Acc : 97.5783% / Test Acc : 96.9200%\n",
      "Epoch : 81 / Loss : 0.0885 / Train Acc : 97.5900% / Test Acc : 96.9500%\n",
      "Epoch : 82 / Loss : 0.0875 / Train Acc : 97.6100% / Test Acc : 96.9000%\n",
      "Epoch : 83 / Loss : 0.0866 / Train Acc : 97.6617% / Test Acc : 96.9900%\n",
      "Epoch : 84 / Loss : 0.0856 / Train Acc : 97.6933% / Test Acc : 96.9700%\n",
      "Epoch : 85 / Loss : 0.0847 / Train Acc : 97.7317% / Test Acc : 97.0200%\n",
      "Epoch : 86 / Loss : 0.0837 / Train Acc : 97.7217% / Test Acc : 96.9900%\n",
      "Epoch : 87 / Loss : 0.0827 / Train Acc : 97.7550% / Test Acc : 97.0100%\n",
      "Epoch : 88 / Loss : 0.0819 / Train Acc : 97.7850% / Test Acc : 97.1000%\n",
      "Epoch : 89 / Loss : 0.0810 / Train Acc : 97.8083% / Test Acc : 97.1100%\n",
      "Epoch : 90 / Loss : 0.0802 / Train Acc : 97.8667% / Test Acc : 97.1500%\n",
      "Epoch : 91 / Loss : 0.0793 / Train Acc : 97.8700% / Test Acc : 97.1300%\n",
      "Epoch : 92 / Loss : 0.0786 / Train Acc : 97.8900% / Test Acc : 97.2000%\n",
      "Epoch : 93 / Loss : 0.0777 / Train Acc : 97.9250% / Test Acc : 97.1500%\n",
      "Epoch : 94 / Loss : 0.0769 / Train Acc : 97.9300% / Test Acc : 97.1600%\n",
      "Epoch : 95 / Loss : 0.0763 / Train Acc : 97.9467% / Test Acc : 97.2900%\n",
      "Epoch : 96 / Loss : 0.0753 / Train Acc : 97.9800% / Test Acc : 97.2500%\n",
      "Epoch : 97 / Loss : 0.0747 / Train Acc : 97.9867% / Test Acc : 97.2600%\n",
      "Epoch : 98 / Loss : 0.0739 / Train Acc : 98.0400% / Test Acc : 97.2700%\n",
      "Epoch : 99 / Loss : 0.0732 / Train Acc : 98.0317% / Test Acc : 97.2800%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = NeuralNetMLP(input_size=784, hidden_size=512, output_size=10)\n",
    "\n",
    "epochs = 100\n",
    "idx_arr = np.arange(len(x_train))\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    train_loss_list = []\n",
    "    np.random.shuffle(idx_arr)\n",
    "    for idx in idx_arr.reshape(-1, batch_size):\n",
    "        \n",
    "        x_batch, t_batch = x_train[idx], t_train[idx]\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "        \n",
    "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "            \n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "    \n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    print('Epoch : {} / Loss : {:.4f} / Train Acc : {:.4f}% / Test Acc : {:.4f}%'.format(epoch_i, np.mean(train_loss_list), train_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreeLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ThreeLayer\n",
    "class NeuralNetMLP:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)\n",
    "        self.params['b1'] = np.zeros(hidden_size1)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params['b2'] = np.zeros(hidden_size2)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        grads['W3'] = self.layers['Affine3'].dW\n",
    "        grads['b3'] = self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 / Loss : 2.3005 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 1 / Loss : 2.2940 / Train Acc : 16.3700% / Test Acc : 16.4200%\n",
      "Epoch : 2 / Loss : 2.2467 / Train Acc : 28.4750% / Test Acc : 28.6000%\n",
      "Epoch : 3 / Loss : 1.7936 / Train Acc : 63.5467% / Test Acc : 64.1300%\n",
      "Epoch : 4 / Loss : 0.9625 / Train Acc : 76.9017% / Test Acc : 77.3700%\n",
      "Epoch : 5 / Loss : 0.6752 / Train Acc : 80.7883% / Test Acc : 81.2000%\n",
      "Epoch : 6 / Loss : 0.5573 / Train Acc : 84.3633% / Test Acc : 84.8900%\n",
      "Epoch : 7 / Loss : 0.4816 / Train Acc : 86.3883% / Test Acc : 86.5200%\n",
      "Epoch : 8 / Loss : 0.4361 / Train Acc : 87.9833% / Test Acc : 87.8800%\n",
      "Epoch : 9 / Loss : 0.4033 / Train Acc : 88.6150% / Test Acc : 88.7000%\n",
      "Epoch : 10 / Loss : 0.3772 / Train Acc : 89.2900% / Test Acc : 89.1300%\n",
      "Epoch : 11 / Loss : 0.3562 / Train Acc : 90.0133% / Test Acc : 89.6800%\n",
      "Epoch : 12 / Loss : 0.3386 / Train Acc : 90.2300% / Test Acc : 90.3600%\n",
      "Epoch : 13 / Loss : 0.3241 / Train Acc : 90.7550% / Test Acc : 90.7400%\n",
      "Epoch : 14 / Loss : 0.3112 / Train Acc : 91.1567% / Test Acc : 91.0400%\n",
      "Epoch : 15 / Loss : 0.2990 / Train Acc : 91.4483% / Test Acc : 91.4900%\n",
      "Epoch : 16 / Loss : 0.2874 / Train Acc : 91.7850% / Test Acc : 91.7900%\n",
      "Epoch : 17 / Loss : 0.2766 / Train Acc : 91.9400% / Test Acc : 92.0500%\n",
      "Epoch : 18 / Loss : 0.2663 / Train Acc : 92.3683% / Test Acc : 92.3500%\n",
      "Epoch : 19 / Loss : 0.2561 / Train Acc : 92.5950% / Test Acc : 92.6300%\n",
      "Epoch : 20 / Loss : 0.2461 / Train Acc : 93.0367% / Test Acc : 92.9200%\n",
      "Epoch : 21 / Loss : 0.2371 / Train Acc : 93.1900% / Test Acc : 93.1700%\n",
      "Epoch : 22 / Loss : 0.2281 / Train Acc : 93.4883% / Test Acc : 93.3700%\n",
      "Epoch : 23 / Loss : 0.2198 / Train Acc : 93.6617% / Test Acc : 93.6300%\n",
      "Epoch : 24 / Loss : 0.2115 / Train Acc : 93.9583% / Test Acc : 93.9900%\n",
      "Epoch : 25 / Loss : 0.2039 / Train Acc : 94.1533% / Test Acc : 93.9600%\n",
      "Epoch : 26 / Loss : 0.1970 / Train Acc : 94.3067% / Test Acc : 94.2500%\n",
      "Epoch : 27 / Loss : 0.1901 / Train Acc : 94.4817% / Test Acc : 94.5600%\n",
      "Epoch : 28 / Loss : 0.1838 / Train Acc : 94.6767% / Test Acc : 94.6000%\n",
      "Epoch : 29 / Loss : 0.1778 / Train Acc : 94.7700% / Test Acc : 94.5400%\n",
      "Epoch : 30 / Loss : 0.1724 / Train Acc : 95.0883% / Test Acc : 94.8500%\n",
      "Epoch : 31 / Loss : 0.1670 / Train Acc : 95.1717% / Test Acc : 95.0000%\n",
      "Epoch : 32 / Loss : 0.1618 / Train Acc : 95.3867% / Test Acc : 95.2800%\n",
      "Epoch : 33 / Loss : 0.1573 / Train Acc : 95.4767% / Test Acc : 95.2300%\n",
      "Epoch : 34 / Loss : 0.1525 / Train Acc : 95.5467% / Test Acc : 95.3700%\n",
      "Epoch : 35 / Loss : 0.1487 / Train Acc : 95.6617% / Test Acc : 95.5100%\n",
      "Epoch : 36 / Loss : 0.1443 / Train Acc : 95.8183% / Test Acc : 95.6700%\n",
      "Epoch : 37 / Loss : 0.1407 / Train Acc : 95.9250% / Test Acc : 95.6600%\n",
      "Epoch : 38 / Loss : 0.1369 / Train Acc : 95.9850% / Test Acc : 95.7000%\n",
      "Epoch : 39 / Loss : 0.1332 / Train Acc : 96.1250% / Test Acc : 95.9000%\n",
      "Epoch : 40 / Loss : 0.1297 / Train Acc : 96.2450% / Test Acc : 95.9900%\n",
      "Epoch : 41 / Loss : 0.1262 / Train Acc : 96.3500% / Test Acc : 96.1600%\n",
      "Epoch : 42 / Loss : 0.1232 / Train Acc : 96.4300% / Test Acc : 96.0800%\n",
      "Epoch : 43 / Loss : 0.1199 / Train Acc : 96.5267% / Test Acc : 96.1800%\n",
      "Epoch : 44 / Loss : 0.1171 / Train Acc : 96.6300% / Test Acc : 96.1800%\n",
      "Epoch : 45 / Loss : 0.1143 / Train Acc : 96.6550% / Test Acc : 96.2200%\n",
      "Epoch : 46 / Loss : 0.1114 / Train Acc : 96.6883% / Test Acc : 96.2300%\n",
      "Epoch : 47 / Loss : 0.1088 / Train Acc : 96.8583% / Test Acc : 96.4300%\n",
      "Epoch : 48 / Loss : 0.1061 / Train Acc : 96.9067% / Test Acc : 96.3700%\n",
      "Epoch : 49 / Loss : 0.1034 / Train Acc : 96.9467% / Test Acc : 96.4300%\n",
      "Epoch : 50 / Loss : 0.1011 / Train Acc : 97.0967% / Test Acc : 96.4800%\n",
      "Epoch : 51 / Loss : 0.0986 / Train Acc : 97.1517% / Test Acc : 96.5000%\n",
      "Epoch : 52 / Loss : 0.0964 / Train Acc : 97.2717% / Test Acc : 96.6800%\n",
      "Epoch : 53 / Loss : 0.0941 / Train Acc : 97.2900% / Test Acc : 96.7100%\n",
      "Epoch : 54 / Loss : 0.0921 / Train Acc : 97.3767% / Test Acc : 96.6600%\n",
      "Epoch : 55 / Loss : 0.0899 / Train Acc : 97.3617% / Test Acc : 96.7400%\n",
      "Epoch : 56 / Loss : 0.0880 / Train Acc : 97.4083% / Test Acc : 96.7300%\n",
      "Epoch : 57 / Loss : 0.0858 / Train Acc : 97.4683% / Test Acc : 96.7700%\n",
      "Epoch : 58 / Loss : 0.0839 / Train Acc : 97.5333% / Test Acc : 96.7800%\n",
      "Epoch : 59 / Loss : 0.0821 / Train Acc : 97.5350% / Test Acc : 96.6200%\n",
      "Epoch : 60 / Loss : 0.0802 / Train Acc : 97.6567% / Test Acc : 96.8300%\n",
      "Epoch : 61 / Loss : 0.0786 / Train Acc : 97.7683% / Test Acc : 96.9200%\n",
      "Epoch : 62 / Loss : 0.0766 / Train Acc : 97.7367% / Test Acc : 97.0100%\n",
      "Epoch : 63 / Loss : 0.0752 / Train Acc : 97.8367% / Test Acc : 96.9100%\n",
      "Epoch : 64 / Loss : 0.0735 / Train Acc : 97.8433% / Test Acc : 97.0000%\n",
      "Epoch : 65 / Loss : 0.0720 / Train Acc : 97.9083% / Test Acc : 97.0500%\n",
      "Epoch : 66 / Loss : 0.0707 / Train Acc : 97.9217% / Test Acc : 97.0200%\n",
      "Epoch : 67 / Loss : 0.0689 / Train Acc : 98.0033% / Test Acc : 97.1600%\n",
      "Epoch : 68 / Loss : 0.0676 / Train Acc : 98.0517% / Test Acc : 97.1300%\n",
      "Epoch : 69 / Loss : 0.0662 / Train Acc : 98.1133% / Test Acc : 97.0400%\n",
      "Epoch : 70 / Loss : 0.0648 / Train Acc : 98.1167% / Test Acc : 97.2300%\n",
      "Epoch : 71 / Loss : 0.0637 / Train Acc : 98.1600% / Test Acc : 97.1500%\n",
      "Epoch : 72 / Loss : 0.0622 / Train Acc : 98.2400% / Test Acc : 97.2000%\n",
      "Epoch : 73 / Loss : 0.0611 / Train Acc : 98.2183% / Test Acc : 97.1700%\n",
      "Epoch : 74 / Loss : 0.0600 / Train Acc : 98.2617% / Test Acc : 97.2700%\n",
      "Epoch : 75 / Loss : 0.0588 / Train Acc : 98.2817% / Test Acc : 97.1800%\n",
      "Epoch : 76 / Loss : 0.0575 / Train Acc : 98.3750% / Test Acc : 97.2900%\n",
      "Epoch : 77 / Loss : 0.0565 / Train Acc : 98.4167% / Test Acc : 97.3200%\n",
      "Epoch : 78 / Loss : 0.0553 / Train Acc : 98.4617% / Test Acc : 97.3400%\n",
      "Epoch : 79 / Loss : 0.0544 / Train Acc : 98.4750% / Test Acc : 97.4100%\n",
      "Epoch : 80 / Loss : 0.0533 / Train Acc : 98.4983% / Test Acc : 97.3900%\n",
      "Epoch : 81 / Loss : 0.0523 / Train Acc : 98.5283% / Test Acc : 97.3000%\n",
      "Epoch : 82 / Loss : 0.0514 / Train Acc : 98.5967% / Test Acc : 97.4800%\n",
      "Epoch : 83 / Loss : 0.0503 / Train Acc : 98.5417% / Test Acc : 97.4000%\n",
      "Epoch : 84 / Loss : 0.0495 / Train Acc : 98.6200% / Test Acc : 97.3800%\n",
      "Epoch : 85 / Loss : 0.0485 / Train Acc : 98.6500% / Test Acc : 97.4600%\n",
      "Epoch : 86 / Loss : 0.0477 / Train Acc : 98.6583% / Test Acc : 97.3800%\n",
      "Epoch : 87 / Loss : 0.0468 / Train Acc : 98.7017% / Test Acc : 97.4200%\n",
      "Epoch : 88 / Loss : 0.0459 / Train Acc : 98.6867% / Test Acc : 97.4800%\n",
      "Epoch : 89 / Loss : 0.0451 / Train Acc : 98.7767% / Test Acc : 97.4700%\n",
      "Epoch : 90 / Loss : 0.0443 / Train Acc : 98.7483% / Test Acc : 97.5400%\n",
      "Epoch : 91 / Loss : 0.0434 / Train Acc : 98.8283% / Test Acc : 97.5200%\n",
      "Epoch : 92 / Loss : 0.0426 / Train Acc : 98.7550% / Test Acc : 97.4100%\n",
      "Epoch : 93 / Loss : 0.0421 / Train Acc : 98.8283% / Test Acc : 97.5800%\n",
      "Epoch : 94 / Loss : 0.0410 / Train Acc : 98.9050% / Test Acc : 97.6100%\n",
      "Epoch : 95 / Loss : 0.0405 / Train Acc : 98.9167% / Test Acc : 97.5800%\n",
      "Epoch : 96 / Loss : 0.0397 / Train Acc : 98.9417% / Test Acc : 97.5900%\n",
      "Epoch : 97 / Loss : 0.0391 / Train Acc : 98.9450% / Test Acc : 97.6000%\n",
      "Epoch : 98 / Loss : 0.0384 / Train Acc : 98.9283% / Test Acc : 97.6500%\n",
      "Epoch : 99 / Loss : 0.0376 / Train Acc : 99.0233% / Test Acc : 97.6300%\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = NeuralNetMLP(input_size=784, hidden_size1=256, hidden_size2=256, output_size=10)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "idx_arr = np.arange(len(x_train))\n",
    "\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    train_loss_list = []\n",
    "    np.random.shuffle(idx_arr)\n",
    "    for idx in idx_arr.reshape(-1, batch_size):\n",
    "        \n",
    "        x_batch, t_batch = x_train[idx], t_train[idx]\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "        \n",
    "        for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "            \n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "    \n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    \n",
    "    print('Epoch : {} / Loss : {:.4f} / Train Acc : {:.4f}% / Test Acc : {:.4f}%'.format(epoch_i, np.mean(train_loss_list),\n",
    "                                                                                         train_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FiveLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FiveLayer\n",
    "class NeuralNetMLP:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5, \n",
    "                 output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)\n",
    "        self.params['b1'] = np.zeros(hidden_size1)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params['b2'] = np.zeros(hidden_size2)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, hidden_size3)\n",
    "        self.params['b3'] = np.zeros(hidden_size3)\n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size3, hidden_size4)\n",
    "        self.params['b4'] = np.zeros(hidden_size4)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size4, hidden_size5)\n",
    "        self.params['b5'] = np.zeros(hidden_size5)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Relu3'] = Relu()\n",
    "        self.layers['Affine4'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Relu4'] = Relu()\n",
    "        self.layers['Affine5'] = Affine(self.params['W5'], self.params['b5'])\n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        grads['W4'] = numerical_gradient(loss_W, self.params['W4'])\n",
    "        grads['b4'] = numerical_gradient(loss_W, self.params['b4'])\n",
    "        grads['W5'] = numerical_gradient(loss_W, self.params['W5'])\n",
    "        grads['b5'] = numerical_gradient(loss_W, self.params['b5'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        grads['W3'] = self.layers['Affine3'].dW\n",
    "        grads['b3'] = self.layers['Affine3'].db\n",
    "        grads['W4'] = self.layers['Affine4'].dW\n",
    "        grads['b4'] = self.layers['Affine4'].db\n",
    "        grads['W5'] = self.layers['Affine5'].dW\n",
    "        grads['b5'] = self.layers['Affine5'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 / Loss : 2.3019 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 1 / Loss : 2.3013 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 2 / Loss : 2.3012 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 3 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 4 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 5 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 6 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 7 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 8 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 9 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 10 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 11 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 12 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n",
      "Epoch : 13 / Loss : 2.3011 / Train Acc : 11.2367% / Test Acc : 11.3500%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-eeb1d6014665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-1e9c04fbb5c6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# x: 입력 데이터, t: 정답 레이블\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-1e9c04fbb5c6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-29ed4e235d87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = NeuralNetMLP(input_size=784, hidden_size1=256, hidden_size2=128, hidden_size3=98, hidden_size4=50, hidden_size5=10, \n",
    "                       output_size=10)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "idx_arr = np.arange(len(x_train))\n",
    "\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    train_loss_list = []\n",
    "    np.random.shuffle(idx_arr)\n",
    "    for idx in idx_arr.reshape(-1, batch_size):\n",
    "        \n",
    "        x_batch, t_batch = x_train[idx], t_train[idx]\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "        \n",
    "        for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4', 'W5', 'b5'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "            \n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "    \n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    \n",
    "    print('Epoch : {} / Loss : {:.4f} / Train Acc : {:.4f}% / Test Acc : {:.4f}%'.format(epoch_i, np.mean(train_loss_list),\n",
    "                                                                                         train_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
