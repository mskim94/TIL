{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 05. 오차역전파법\n",
    "\n",
    "Chapter 04. 신경망 학습에서는 신경망의 가중치 매개변수에 대한 손실함수의 기울기를 구하는 것에 대해 배웠다. \n",
    "\n",
    "- 장점: 구현이 쉽다.\n",
    "- 단점: 계산 시간이 오래 걸린다.\n",
    "\n",
    "$\\Rightarrow$ **오차역전파법(backpropagation)**\n",
    "\n",
    "\n",
    "## 5.1 계산 그래프\n",
    "- 계산 그래프(computational graph): 계산 과정을 그래프로 나타낸 것\n",
    "\n",
    "- Graph = node + edge(node 사이의 직선)\n",
    "\n",
    "\n",
    "### 5.1.1 계산 그래프로 풀다\n",
    "\n",
    "Q1. 1개 100원 사과 2개, 소비세 10%, 지불 금액은?\n",
    "\n",
    "- 사과 $\\rightarrow^{100}$ x2 $\\rightarrow^{200}$ x1.1 $\\rightarrow^{220}$ L \n",
    "\n",
    "\"x\"만 연산으로 생각하면 \"사과 개수\", \"소비세\"를 변수로 취급\n",
    "\n",
    "**문제 풀이 Flow**\n",
    "1. 계산 그래프 구성\n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행 (**순전파(forwardpropagation)**)\n",
    "\n",
    "\n",
    "### 5.1.2 국소적 계산\n",
    "\n",
    "그래프 계산의 특징: **국소적 계산** 전파함으로 최종 결과를 얻음 \\\n",
    "(국소적: 자신과 관계된 작은 범위) (국소적 계산: 자신과 관계된 정보만으로 결과 출력)\n",
    "\n",
    "\n",
    "### 5.1.3 왜 계산 그래프로 푸는가?\n",
    "\n",
    "계산 그래프의 이점\n",
    "1. 국소적 계산: 전체 복잡해도, 각 노드서는 단순 계산 집중, 문제 단순화 가능\n",
    "2. 중간 계산 결과 보관\n",
    "3. **역전파 통해 '미분' 효율적 계산**\n",
    "\n",
    "e.g) 사과 가격이 오르면 최종 금액 어떤 영향?\n",
    "\n",
    "'사과 가격($x$)'에 대한 '지불 금액($L$)'의 미분 $\\Rightarrow$ $\\frac{\\partial L}{\\partial x}$\n",
    "\n",
    "\n",
    "## 5.2 연쇄 법칙(Chain Rule)\n",
    "\n",
    "계산 그래프\n",
    "- 순전파(forward propagation): left $\\rightarrow$ right\n",
    "- 역전파(backward propagation): **국소적 미분**을 right $\\rightarrow$ left\n",
    "    - 국소적 미분 전달 원리: **연쇄법칙(chain rule)**\n",
    "    \n",
    "    \n",
    "### 5.2.1 계산 그래프의 역전파\n",
    "\n",
    "e.g) $y = f(x)$\n",
    "\n",
    "- $\\overset{x}{\\rightarrow}$ $f$ $\\overset{y}{\\rightarrow}$\n",
    "- $\\underset{E \\frac{\\partial y}{\\partial x}}{\\leftarrow}$ $f$ $\\underset{E}{\\leftarrow}$\n",
    "\n",
    "계산 그래프를 이용하여 역전파를 계산하면 **미분값을 효율적으로 계산함**\n",
    "\n",
    "### 5.2.2 연쇄법칙이란?\n",
    "\n",
    "연쇄법칙: 합성 함수의 미분에 대한 성질\n",
    "\n",
    "\"\"함성함수의 미분은 합성 함수를 구성하는 **각 함수**의 **미분의 곱**으로 나타낼 수 있다.\"\"\n",
    "\n",
    "$z = (x + y)^2$ \n",
    "- $z = t^2$ \n",
    "- $t = x+ y$\n",
    "\n",
    "- $\\frac{\\partial z}{\\partial t} = 2t$\n",
    "- $\\frac{\\partial t}{\\partial x} = 1$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x} (x에 대한 z의 미분) = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t = 2(x+y)$\n",
    "\n",
    "\n",
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "\n",
    "역전파가 하는일 = 연쇄법칙의 원리\n",
    "\n",
    "\n",
    "## 5.3 역전파\n",
    "\n",
    "### 5.3.1 덧셈 노드의 역전파\n",
    "\n",
    "e.g) $z = x + y$\n",
    "- $\\frac{\\partial z}{\\partial x} = 1$\n",
    "- $\\frac{\\partial z}{\\partial y} = 1$\n",
    "\n",
    "$\\Rightarrow$ 상류에 전해진 미분($\\frac{\\partial L}{\\partial z}$)에 **1**곱해 하류로 흘린다.\n",
    "$\\Rightarrow$ 입력값을 그대로 다음 노드로 보낸다.\n",
    "\n",
    "### 5.3.2 곱셈 노드의 역전파\n",
    "\n",
    "e.g) $z = xy$\n",
    "- $\\frac{\\partial z}{\\partial x} = y$\n",
    "- $\\frac{\\partial z}{\\partial y} = x$\n",
    "\n",
    "$\\Rightarrow$ 상류에 순전파 때의 **입력 신호** 들을 **서로 바꾼 값**을 곱해 하류로 흘린다.\n",
    "\n",
    "### 정리)\n",
    "- 덧셈의 역전파: 상류 값 그대로 흘려, 순방향 **입력 신호 필요 없다**\n",
    "- 곱셈의 역전파: 입력 신호를 서로 바꾼 값을 곱하여 하류로 흘린다. **순방향 입력 신호 값 필요**, 곱셈 노드 구현 시, 순전파 입력 신호 변수 저장\n",
    "\n",
    "\n",
    "## 5.4 단순한 계측 구현하기\n",
    "\n",
    "### 5.4.1 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y  # x, y를 교환\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward() 호출 순서는 forward() 때와는 반대. backward()가 받는 인수는 '순전파의 출력에 대한 미분'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "2.2 110.00000000000001 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price , dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple, dapple_num, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.5 활성화 함수 계층 구현하기\n",
    "\n",
    "ReLU, Sigmoid 계층 구현\n",
    "\n",
    "### 5.5.1 ReLU 계층\n",
    "\n",
    "활성화 함수 ReLU 수식\n",
    "- $y = x (x > 0)$\n",
    "- $y = 0 (x \\leq 0)$\n",
    "\n",
    "- $\\frac{\\partial y}{\\partial x} = 1 (x > 0)$\n",
    "- $\\frac{\\partial y}{\\partial x} = 0(x \\leq 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask instance 변수는 T/F 구성된 numpy array, 순전파의 입력 $x$의 원소 값이 0 이하인 인덱스는 True, 그 외는 (0보다 큰 원소)는 False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = (x <=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순전파 때의 입력이 0 이하면 역전파 때의 값은 0이 돼야 한다. 그래서 역전파 때는 순전파 때 만들어둔 mask를 사용해 mask의 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층\n",
    "\n",
    "Sigmoid 수식: $y = \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "#### 1단계\n",
    "상류에서 흘러온 값에 $-y^2$을 곱해 하류로 전달\n",
    "\n",
    "'/'노드, $y = \\frac{1}{x}$\n",
    "- $\\frac{\\partial y}{\\partial x} =- \\frac{1}{{x}^{2}} = -y^2$\n",
    "\n",
    "\n",
    "$\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $/$ $\\underset{\\frac{\\partial L}{\\partial y}}{\\leftarrow}$\n",
    "\n",
    "#### 2단계\n",
    "'+'노드, 상류의 값을 그대로 하류로 내보낸다. \n",
    "\n",
    "$\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $+$ $\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $/$ $\\underset{\\frac{\\partial L}{\\partial y}}{\\leftarrow}$\n",
    "\n",
    "\n",
    "#### 3단계\n",
    "'exp' 노드는 $y = exp(x)$ 연산을 수행\n",
    "- $\\frac{\\partial y}{\\partial x} = \\exp(x)$\n",
    "\n",
    "$\\underset{- \\frac{\\partial L}{\\partial y} y^{2} exp(-x)}{\\leftarrow}$ $exp$ $\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $+$ $\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $/$ $\\underset{\\frac{\\partial L}{\\partial y}}{\\leftarrow}$\n",
    "\n",
    "#### 4단계 \n",
    "'x'노드, 순전파 때의 값을 '서로 바꿔' 곱한다. \n",
    "\n",
    "$\\underset{\\frac{\\partial L}{\\partial y} y^{2} exp(-x)}{\\leftarrow}$ $'\\times'$ $\\underset{- \\frac{\\partial L}{\\partial y} y^{2} exp(-x)}{\\leftarrow}$ $'exp'$ $\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $'+'$ $\\underset{- \\frac{\\partial L}{\\partial y} y^{2}}{\\leftarrow}$ $'/'$ $\\underset{\\frac{\\partial L}{\\partial y}}{\\leftarrow}$\n",
    "\n",
    "\n",
    "#### 간소화 버전\n",
    "\n",
    "$\\underset{\\frac{\\partial L}{\\partial y} y^{2} exp(-x)}{\\leftarrow}$ $sigmoid$ $\\underset{\\frac{\\partial L}{\\partial y}}{\\leftarrow}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} y^{2} exp(-x) = \\frac{\\partial L}{\\partial y} \\frac{1}{(1+\\exp(-x))^2}\\exp(-x) = \\frac{\\partial L}{\\partial y} \\frac{1}{(1+\\exp(-x))^2} \\frac{exp(-x)}{(1+\\exp(-x))^2} = \\frac{\\partial L}{\\partial y}y(1-y)$\n",
    "\n",
    "sigmoid 계층의 역전파는 순전파의 출력($y$)만으로 계산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 구현에서 순전파의 출력을 out에 보관, 역전파 계산 때 그 값을 사용\n",
    "\n",
    "## 5.6 Affine/Softmax 계층 구현하기\n",
    "\n",
    "### 5.6.1 Affine 계층\n",
    "\n",
    "신경망 순전파에서는 가중치 신호의 총합을 계산하기 위해 행렬의 곱 (np.dot())을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[1.94606992 0.96905767 1.67903606]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2)  # 입력\n",
    "W = np.random.rand(2,3)  # 가중치\n",
    "B = np.random.rand(3)  # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**어파인 변환(Affine transformation)**: 기하학에서 신경망의 순전파 때 수행하는 행렬의 곱\n",
    "\n",
    "### 5.6.2 배치용 Affine 계층\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)\n",
    "\n",
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "softmax function은 입력값을 정규화하여 출력한다. \n",
    "\n",
    "신경망에서 수행하는 작업은 **학습**, **추론** 두 가지이다. 추론시에는 Softmax를 사용하지 않고 Affine 계층의 출력을 인식 결과로 이용한다.\n",
    "신경망 추론에서는 답을 하나만 내는 경우, 가장 높은 점수만 알면 되니 Softmax 계층이 필요 없다.\n",
    "\n",
    "신경망에서 정규화하지 않은 출력 결과를 **점수(score)** 라고 한다. 신경망 학습에는 Softmax 계층이 필요하다.\n",
    "\n",
    "softmax 함수의 손실 함수로 **교차 엔트로피 오차**를 사용, 항등함수의 손실 함수로 **오차제곱합**을 이용. (3.5 출력층 설계하기 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블(one-hot-vector)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기\n",
    "\n",
    "지금까지 구현한 계층을 조합하여 신경망 구축\n",
    "\n",
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "신경망 학습 순서\n",
    "\n",
    "#### 전제\n",
    "신경망에는 적응 가능한 **가중치**, **편향** 존재, 이 가중치와 편향이 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 함.\n",
    "\n",
    "#### 1단계 - 미니배치\n",
    "\n",
    "훈련 데이터 중 일부 무작위로 가져옴, 이 데이터를 미니배치라고 하고, 그 미니배치의 손실함수 값을 줄이는 것이 목표\n",
    "\n",
    "#### 2단계 - 기울기 산출\n",
    "\n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함, 기울기는 손실 함수의 값을 가장 자게 하는 방향을 제시\n",
    "\n",
    "#### 3단계 - 매개변수 갱신\n",
    "\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "\n",
    "#### 4단계 - 반복\n",
    "\n",
    "1~3단계를 반복\n",
    "\n",
    "\n",
    "앞장의 기울기 산출에서는 **수치 미분**을 사용, 구현이 쉽지만 계산 시간이 오래 걸렸다. 하지만, 오차역전파법을 사용하면 기울기를 효율적으로 구할 수 있다.\n",
    "\n",
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "\n",
    "2층 신경망을 TwoLayerNet 클래스로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "\n",
    "수치미분은 구현하기 쉽지만 계산 시간이 오래 걸려, 오차역접파법을 이용해 기울기를 구하였다.\n",
    "\n",
    "수치미분의 결과와 오차역전파법의 결과를 비교해 오차역전파법을 제대로 구현했는지 **검증** 하곤 한다.\n",
    "\n",
    "이 처럼 두 가지 방식으로 구한 기울기가 일치함을 확인하는 작업을 **기울기 확인(gradient check)** 이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.0009675287498732883\n",
      "b1:0.005918786815175965\n",
      "W2:0.013700021152345702\n",
      "b2:0.3060766584070989\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "올바르게 구했다면 0에 아주 가까운 작은 값이 된다. 그 값이 크면 오차역전파법을 잘못 구현했다고 의심해야한다.\n",
    "\n",
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12123333333333333 0.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-c91d0e8dd0af>:2: RuntimeWarning: overflow encountered in exp\n",
      "  exp_a = np.exp(a)\n",
      "<ipython-input-12-c91d0e8dd0af>:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y = exp_a / sum_exp_a\n",
      "<ipython-input-5-7958be17b382>:6: RuntimeWarning: invalid value encountered in less_equal\n",
      "  self.mask = (x <= 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n",
      "0.09871666666666666 0.098\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다.\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 정리\n",
    "\n",
    "동작을 계층으로 모듈화하여, 신경망의 계층을 자유롭게 조합하여 원하는 신경망을 쉽게 만들 수 있다.\n",
    "\n",
    "- 계산 그래프를 사용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
    "- 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.\n",
    "- 계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다. \n",
    "- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효울적으로 계산할 수 있다.(오차역전파법)\n",
    "- 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다.(기울기 확인(gradient check))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
