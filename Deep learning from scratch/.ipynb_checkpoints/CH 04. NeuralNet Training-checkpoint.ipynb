{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04. 신경망 학습\n",
    "\n",
    "신경망 학습에 대해 배운다.\n",
    "\n",
    "학습이란? 데이터로부터 가중치 매개변수의 최적값을 자동으로 얻는것\n",
    "\n",
    "신경망이 학습할 수 있도록 하는 **지표** 인 **손실 함수(Loss Function)** 소개한다.\n",
    "\n",
    "학습 목표: 손실함수의 결괏값을 가장 작게하는 가중치 매개변수를 찾는 것\n",
    "\n",
    "손실 함수의 값을 작게 만드는 기법으로 **경사 하강법(Gradient Descent)** 가 있다.\n",
    "\n",
    "## 4.1 데이터에서 학습한다!\n",
    "\n",
    "신경망의 특징: 데이터를 보고 학습한다. $\\rightarrow$ 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.\n",
    "\n",
    "### 4.1.1 데이터 주도 학습\n",
    "기계학습에서 **데이터**는 매우 중요\n",
    "\n",
    "문제를 풀때, 특히 패턴을 찾을 때, \n",
    "- **사람**은 경험, 직관을 이용하여 시행착오를 거듭하면서 문제 해결을 진행\n",
    "- **기계학습**은 사람 개입을 최소화, 수집한 데이터로 부터 패턴을 찾으려 시도\n",
    "- **신경망, 딥러닝**은 기계학습 보다 더 사랑ㅁ의 개입을 배제\n",
    "\n",
    "e.g) MNIST 문제를 풀때 Algorithm or program을 처음부터 만들기 어려워 $\\rightarrow$ image에서 **특징** 추출, 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.\n",
    "\n",
    "- 특징(feature): 입력 데이터에서 본질적인 데이터(중요한 데이터) 추출하는 **변환기**, 이 특징은 **벡터(vector)** 로 기술\n",
    "    - Computer Vision 분야에서는 SIFT, SURF, HOG 등 특징 활용\n",
    "\n",
    "특징을 사용해 데이터를 벡터로 변환, 이 변환된 데이터를 지도학습 대표 기법인 SVM, KNN 등으로 학습 가능하다.\n",
    "\n",
    "모아진 데이터를 기계가 규칙을 찾는다. 처음부터 Algorithm을 설계하는 것보다 효율이 좋지만, 이 **특징** 을 **사람이 설계(개입)**, 문제에 따라 사람이 특징을 생각해야 한다.\n",
    "\n",
    "문제 $\\rightarrow$ 사람이 생각한 Algorithm $\\rightarrow$ 결과\n",
    "\n",
    "$(개선)\\Rightarrow$ 문제 $\\rightarrow$ 사람이 생각한 특징(SIFT, SURF, HOG etc...) $\\rightarrow$ 기계학습(SVM, KNN) $\\rightarrow$ 결과\n",
    "\n",
    "$(개선)\\Rightarrow$ 문제 $\\rightarrow$ 신경망(딥러닝) $\\rightarrow$ 결과\n",
    "\n",
    "신경망(딥러닝)에서는 이미지를 '있는 그대로' 학습, 특징까지 기계가 스스로 학습(기계학습, 신경망(딥러닝) 과정에서는 사람의 개입이 없다.)\n",
    "\n",
    "cf) 딥러닝을 **종단간 기계학습(end-to-end machine learning)** 이라고도 함 $\\Rightarrow$ 데이터(입력)부터 결과(출력)까지 사람의 개입이 없다.\n",
    "\n",
    "**신경망의 이점**: 모든 문제 같은 맥락에서 푼다 $\\Rightarrow$ 모든 문제를 주어진 데이터 그대로 사용, 입력 데이터 활용 'end-to-end'로 학습 가능\n",
    "\n",
    "### 4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "기계학습에서는 데이터를 훈련 데이터와 시험 데이터로 나눈다. \n",
    "\n",
    "Why Split? **범용 능력** 평가를 위해 \n",
    "- 범용 능력: 아직 보지 못한 data(training data에 없는)로 문제를 옳바르게 풀어내는 능력\n",
    "\n",
    "한 데이터에만 지나치게  최적화 된 상태: **오버피팅(overfitting)**\n",
    "\n",
    "## 4.2 손실 함수(Loss Function)\n",
    "\n",
    "신경망 학습에서 현재 상태를 '하나의 지표'로 표현, 이 지표를 가장 좋게 하는 가중치 매개변수 값을 탐색하는 것이 목적\n",
    "\n",
    "**손실 함수**: 신경망 학습에 사용되는 지표\n",
    "\n",
    "일반적으로 손실함수로 \"오차제곱합\", \"교차 엔트로피 오차\" 를 사용\n",
    "\n",
    "### 4.2.1 오차제곱합(Sum of Squares Error, SSE)\n",
    "\n",
    "가장 많이 사용하는 손실 함수 오차제곱합(Sum of Squares Error, SSE)\n",
    "\n",
    "각 원소의 출력(추정값)과 정답 레이블(참 값)의 차($y_k - t_k$)를 제곱(Squares) 후, 그 총합($\\sum$)\n",
    "\n",
    "- $E = \\frac{1}{2} \\sum_{k}(y_k - t_k)^2$\n",
    "    - $y_k$: 신경망의 출력(신경망이 추정한 값)\n",
    "    - $t_k$: 정답 레이블(One-hot-encoding)\n",
    "    - $k$: 데이터의 차원 수\n",
    "    - 신경망 출력 $y$는 Softmax function의 출력(확률로 해석 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작은 0.097이 정답에 더 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차(Cross-Entropy Error, CEE)\n",
    "\n",
    "CEE도 자주 사용\n",
    "\n",
    "- $E = -\\sum_k t_k \\log{y_k}$\n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "$\\Rightarrow$ 실질적으로 정답 시의 추정($t_k = 1$일때의 $y_k$) (y_k는 확률적인 값이고 그 값들의 총합은 1이다 (Softmax function의 출력이기때문에) $t_k$는 one-hot-encoding되어 정답 레이블인데 정답은 1 나머지는 0으로 수식에서 계산되면 정답 외의 값들은 0으로 처리된다.)\n",
    "\n",
    "$\\Rightarrow$ 교차 엔트로피 오차는 정답일때 **출력이 전체 값을 정한다.**\n",
    "($y_k$의 값이 작다(확률이 작다)면 log가 취해서 전체 CEE의 값 자체가 커진다  $\\rightarrow$ 오차가 커진다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-e47f3d2f8152>:4: RuntimeWarning: divide by zero encountered in log\n",
      "  y = np.log(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.001)\n",
    "y = np.log(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-5.1, 0.1)  # y축의 범위를 지정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답일 때의 출력(확률로 해석)이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "# 첫번째 결괏값이 더 작다(오차가 작다). 정답일 가능성이 높다고 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "\n",
    "기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고 그 값을 최소화하는 매개변수를 찾는다. $\\rightarrow$ 결국, 모든 훈련 데이터에 대한 손실 함수를 구해야한다.\n",
    "\n",
    "- $E = -\\sum_k t_k \\log{y_k}$ (데이터 1개에 대한 Loss Function)\n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "(N개의 데이터로 확장, N으로 나누어 정규화) $\\Rightarrow$\n",
    "\n",
    "- $E = -\\frac{1}{N}\\sum_{n} \\sum_k t_k \\log{y_k}$ \n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "N으로 나눠 **평균 손실 함수** 구해, But! 데이터가 커지면 시간 오래 걸린다.\n",
    "\n",
    "$\\Rightarrow$ 데이터 일부 추려 **근사치**를 이용 (일부: 미니배치(Mini-batch))\n",
    "\n",
    "**미니배치 학습**: Training data 중 무작위로 뽑아 학습하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import pickle\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)  # 훈련 데이터 60,000개, 입력 데이터 784(28 x 28)\n",
    "print(t_train.shape)  # 정답 레이블 (0 ~ 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 0 ~ train_size(60,000)에서 무작위로 batch_size(10)개를 뽑음\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 '2', '7'과 같이 주어졌을 때, cross-entropy error\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실함수를 설정하는가?\n",
    "\n",
    "왜 손실함수 사용, '정확도'라는 지표를 놔두고 우회적인 방법을 사용?\n",
    "\n",
    "정확도를 지표로 하면 매개변수의 미분이 대부분 장소에서 0이되기 때문\n",
    "\n",
    "정확도 지표를 사용시 매개변수의 미분이 대부분 0인 이유?\n",
    "\n",
    "만약 MNIST와 같은 이미지 데이터에서 100장의 training data 중 32장 올바르게 인식 $\\rightarrow$ 정확도: 32%\n",
    "\n",
    "if 정확도 지표를 사용시, 매개변수 값 조금 변해도 그대로 32%이다. 개선된다고 해도 연속적인 값이 아닌 불연속적 값이 나오기때문\n",
    "\n",
    "신경망 학습서 손실함수의 미분하여 매개변수 값을 서서히 갱신하는 과정을 반복\n",
    "- 미분 값이 음수 $\\rightarrow$ 가중치 매개변수를 양의 방향으로 변화시켜 loss function 값을 줄인다.\n",
    "- 미분 값이 양수 $\\rightarrow$ 가중치 매개변수를 음의 방향으로 변화시켜 loss function 값을 줄인다.\n",
    "- 미분 값이 0이면 $\\rightarrow$ 갱신 Stop\n",
    "\n",
    "정확도는 **매개변수의 미소한 변화에는 거의 반응하지 못한다.** 반응하더라도 불연속적 변화 (활성화 함수로 step function을 사용하지 않는 이유와 유사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분\n",
    "\n",
    "경사법에서 **기울기**(경사) 값을 기준으로 나아갈 방향을 정한다.\n",
    "\n",
    "### 4.3.1 미분\n",
    "미분: 한 순간의 변화량(순간 변화량)\n",
    "\n",
    "$\\frac{df(x)}{dx} = \\lim_{x\\to0} \\frac{f(x+h) - f(x)}{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h에 최대한 작은 값을 대입 위해 h = 1e-50 하지만, **반올림 오차(rounding error)** 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 반올림 오차 \n",
    "np.float32(1e-50)  # 0.0으로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$10^{-4}$ 정도 값 사용 시 좋은 결과 얻는다고 알려져 있다.\n",
    "\n",
    "$f$의 차분\n",
    "\n",
    "$f(x+h) - f(x)$는 $x+h$ 와 $x$ 사이의 차분\n",
    "\n",
    "**진정한 미분**: $x$ 위치에서 함수의 기울기(접선), 위 코드는 $(x_h)$와 $x$ 사이의 기울기에 해당\n",
    "\n",
    "h를 무한히 0으로 좁히는 것이 불가능하기에 생기는 한계\n",
    "\n",
    "**수치 미분**에는 **오차**가 포함된다. 이 오차를 줄이기 위해 $(x+h)$와 $(x-h)$일때 함수 $f$의 차분을 계산하는 방법도 쓰인다.\n",
    "\n",
    "이 차분은 $x$를 중심으로 그 전후의 차분을 계산한다는 의미에서 **중심 차분** or **중앙 차분**이라 한다. ($(x+h)$와 $x$의 차분을 **전방 차분**이라고 한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **수치 미분**: 차분으로 미분을 하는 것, 오차가 포함\n",
    "- **해적적 미분**: 수식을 전개하여 미분하는 것, 오차가 포함되지 않음, 진정한 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 수치 미분의 예\n",
    "$y = 0.01x^2 + 0.1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01 * x**2  + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5))  # x = 5\n",
    "print(numerical_diff(function_1, 10))  # x = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분\n",
    "\n",
    "**편미분**: 변수가 여럿인 함수에 대한 미분\n",
    "\n",
    "$f(x_0, x_1) = x_0^2 + x_1^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2  # np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기\n",
    "\n",
    "**기울기(Gradient)**: $(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1})$ 처럼 모든 변수의 편미분을 벡터로 정리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]  \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)\n",
    "\n",
    "경사법: 기울기 이용 손실함수의 min(or max)를 찾으려는 것\n",
    "\n",
    "기울기는 각 지점에서 함수값을 줄이는 방안을 제시하는 지표, 하지만, 실제로 방향이 맞는지는 보장 못한다.\n",
    "\n",
    "함수가 극대, 극소, **안장점(saddle point)** 에서 기울기가 **0**이다. \n",
    "- 극솟값: 국소적인 최솟값, 한정된 범위에서의 최솟값인 점\n",
    "- 안정점: 어느방향에서는 최솟값, 어느 방향에서는 극댓값이 되는 점\n",
    "\n",
    "$\\rightarrow$ 기울기 0이라고 해서 최솟값이라고 할 수 **없다**. 그점이 극솟값이나 안장점일 수 도 있기때문이다.\n",
    "\n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그리고 다음 이동한 곳에서 다시 기울기 구하고, 또 기울어진 방향으로 이동 반복\n",
    "- 경사 하강법(Gradient Descent): 최솟값을 찾음\n",
    "- 경사 상승법(Gradient Ascent): 최댓값을 찾음\n",
    "\n",
    "- $x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0}$\n",
    "- $x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}$\n",
    "\n",
    "$\\eta$(eta, 에타): **학습률(learning rate)** : 매개변수의 값을 얼마나 갱신하느냐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f: 최적화하려는 함수\n",
    "- init_x: 초깃값\n",
    "- lr: learning rate\n",
    "- step_num: 경사법에 따른 반복 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사법으로 $f(x_0, x_1) = x_0^2 + x_1^2$의 최솟값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, lr=0.1, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 (0, 0)에 가까운 결과, 실제 최솟값은 (0, 0)이므로 거의 정확한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=10.0, step_num = 100))\n",
    "\n",
    "# 학습률이 너무 작은 예\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=1e-10, step_num = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 크면 발산, 너무 작으면 거의 갱신되지 않고 끝난다.\n",
    "\n",
    "learning rate와 같은 파라메터를 **하이퍼파라미터(hyperparameter)** 라고 한다. 이 하이퍼파라미터는 사람이 직접 설정해야 하며, 여러 하이퍼파리미터 후보 값 중 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다.\n",
    "\n",
    "### 4.4.2 신경망에서 기울기\n",
    "\n",
    "간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)  # 정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"예측을 수행하는 메서드\"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수의 값 구하는 메서드\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.85241277  0.82516951  1.58759752]\n",
      " [-0.95713278 -1.56922664 -0.72568907]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.37286717 -0.91720227  0.29943835]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39476605229356426"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "print(np.argmax(p))  # p의 최댓값\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07593349  0.11976395 -0.19569743]\n",
      " [ 0.11390023  0.17964592 -0.29354615]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "- 전제: 신경망엔 적응 가능한 가중치와 편향있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. 4단계로 구성\n",
    "\n",
    "1단계 - 미니배치\n",
    "\n",
    "- 훈련 데이터 중 일부를 무작위로 가져옴, 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 주이는 것을 목표로 한다.\n",
    "\n",
    "2단계 - 기울기 산출\n",
    "\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "3단계 - 매개변수 갱신\n",
    "\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "4단계 - 반복\n",
    "\n",
    "- 1~3 단계를 반복한다.\n",
    "\n",
    "위 단계는 경사 하강법으로 매개변수를 갱신하는 방법이다. 이때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(Stochastic gradient descent)** 라고 부른다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 사용, 2층 신경망 구현(은닉층 1개인 네트워크)\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = sigmoid(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x: 입력 데이터, y: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(y, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    " \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet class가 사용하는 변수\n",
    "- params: 신경망의 매개변수를 보관하는 dict변수(인스턴스 변수)\n",
    "    - params['W1']: 1번째 층의 가중치, params['b1']: 1번째 층의 편향\n",
    "    - params['W2']: 2번째 층의 가중치, params['b2']: 2번째 층의 편향\n",
    "\n",
    "- grads: 기울기를 보관하는 dict변수(numerical_gradient() 메서드의 반환 값)\n",
    "    - grads['W1']: 1번째 층의 가중치의 기울기, grads['b1']: 1번째 층의 편향의 기울기\n",
    "    - grads['W2']: 2번째 층의 가중치의 기울기, grads['b2']: 2번째 층의 편향의 기울기\n",
    "    \n",
    "    \n",
    "TwoLayerNet class가 사용하는 메서드\n",
    "- \\__init__(self, input_size, hidden_size, output_size): 초기화를 수행, 인수는 순서대로 **입력층의 뉴런의 수, 은닉층의 뉴런의 수, 출력층의 뉴런의 수**\n",
    "- predidct(self, x): 예측(추론)을 수행, 인수 x는 이미지 데이터\n",
    "- loss(self, x, t): 손실함수의 값을 구한다. 인수 x는 이미지 데이터, t는 정답 레이블\n",
    "- accuracy(self, x, t): 정확도를 구한다.\n",
    "- numerical_gradient(self, x, t): 가중치 매개변수의 기울기를 구한다.\n",
    "- gradient(self, x, t): 가중치 매개변수의 기울기를 구한다. numerical_gradient()의 성능 개선판, 구현은 다음장에.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
