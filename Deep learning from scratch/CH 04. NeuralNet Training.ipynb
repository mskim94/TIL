{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04. 신경망 학습\n",
    "\n",
    "신경망 학습에 대해 배운다.\n",
    "\n",
    "학습이란? 데이터로부터 가중치 매개변수의 최적값을 자동으로 얻는것\n",
    "\n",
    "신경망이 학습할 수 있도록 하는 **지표** 인 **손실 함수(Loss Function)** 소개한다.\n",
    "\n",
    "학습 목표: 손실함수의 결괏값을 가장 작게하는 가중치 매개변수를 찾는 것\n",
    "\n",
    "손실 함수의 값을 작게 만드는 기법으로 **경사 하강법(Gradient Descent)** 가 있다.\n",
    "\n",
    "## 4.1 데이터에서 학습한다!\n",
    "\n",
    "신경망의 특징: 데이터를 보고 학습한다. $\\rightarrow$ 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.\n",
    "\n",
    "### 4.1.1 데이터 주도 학습\n",
    "기계학습에서 **데이터**는 매우 중요\n",
    "\n",
    "문제를 풀때, 특히 패턴을 찾을 때, \n",
    "- **사람**은 경험, 직관을 이용하여 시행착오를 거듭하면서 문제 해결을 진행\n",
    "- **기계학습**은 사람 개입을 최소화, 수집한 데이터로 부터 패턴을 찾으려 시도\n",
    "- **신경망, 딥러닝**은 기계학습 보다 더 사랑ㅁ의 개입을 배제\n",
    "\n",
    "e.g) MNIST 문제를 풀때 Algorithm or program을 처음부터 만들기 어려워 $\\rightarrow$ image에서 **특징** 추출, 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.\n",
    "\n",
    "- 특징(feature): 입력 데이터에서 본질적인 데이터(중요한 데이터) 추출하는 **변환기**, 이 특징은 **벡터(vector)** 로 기술\n",
    "    - Computer Vision 분야에서는 SIFT, SURF, HOG 등 특징 활용\n",
    "\n",
    "특징을 사용해 데이터를 벡터로 변환, 이 변환된 데이터를 지도학습 대표 기법인 SVM, KNN 등으로 학습 가능하다.\n",
    "\n",
    "모아진 데이터를 기계가 규칙을 찾는다. 처음부터 Algorithm을 설계하는 것보다 효율이 좋지만, 이 **특징** 을 **사람이 설계(개입)**, 문제에 따라 사람이 특징을 생각해야 한다.\n",
    "\n",
    "문제 $\\rightarrow$ 사람이 생각한 Algorithm $\\rightarrow$ 결과\n",
    "\n",
    "$(개선)\\Rightarrow$ 문제 $\\rightarrow$ 사람이 생각한 특징(SIFT, SURF, HOG etc...) $\\rightarrow$ 기계학습(SVM, KNN) $\\rightarrow$ 결과\n",
    "\n",
    "$(개선)\\Rightarrow$ 문제 $\\rightarrow$ 신경망(딥러닝) $\\rightarrow$ 결과\n",
    "\n",
    "신경망(딥러닝)에서는 이미지를 '있는 그대로' 학습, 특징까지 기계가 스스로 학습(기계학습, 신경망(딥러닝) 과정에서는 사람의 개입이 없다.)\n",
    "\n",
    "cf) 딥러닝을 **종단간 기계학습(end-to-end machine learning)** 이라고도 함 $\\Rightarrow$ 데이터(입력)부터 결과(출력)까지 사람의 개입이 없다.\n",
    "\n",
    "**신경망의 이점**: 모든 문제 같은 맥락에서 푼다 $\\Rightarrow$ 모든 문제를 주어진 데이터 그대로 사용, 입력 데이터 활용 'end-to-end'로 학습 가능\n",
    "\n",
    "### 4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "기계학습에서는 데이터를 훈련 데이터와 시험 데이터로 나눈다. \n",
    "\n",
    "Why Split? **범용 능력** 평가를 위해 \n",
    "- 범용 능력: 아직 보지 못한 data(training data에 없는)로 문제를 옳바르게 풀어내는 능력\n",
    "\n",
    "한 데이터에만 지나치게  최적화 된 상태: **오버피팅(overfitting)**\n",
    "\n",
    "## 4.2 손실 함수(Loss Function)\n",
    "\n",
    "신경망 학습에서 현재 상태를 '하나의 지표'로 표현, 이 지표를 가장 좋게 하는 가중치 매개변수 값을 탐색하는 것이 목적\n",
    "\n",
    "**손실 함수**: 신경망 학습에 사용되는 지표\n",
    "\n",
    "일반적으로 손실함수로 \"오차제곱합\", \"교차 엔트로피 오차\" 를 사용\n",
    "\n",
    "### 4.2.1 오차제곱합(Sum of Squares Error, SSE)\n",
    "\n",
    "가장 많이 사용하는 손실 함수 오차제곱합(Sum of Squares Error, SSE)\n",
    "\n",
    "각 원소의 출력(추정값)과 정답 레이블(참 값)의 차($y_k - t_k$)를 제곱(Squares) 후, 그 총합($\\sum$)\n",
    "\n",
    "- $E = \\frac{1}{2} \\sum_{k}(y_k - t_k)^2$\n",
    "    - $y_k$: 신경망의 출력(신경망이 추정한 값)\n",
    "    - $t_k$: 정답 레이블(One-hot-encoding)\n",
    "    - $k$: 데이터의 차원 수\n",
    "    - 신경망 출력 $y$는 Softmax function의 출력(확률로 해석 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작은 0.097이 정답에 더 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차(Cross-Entropy Error, CEE)\n",
    "\n",
    "CEE도 자주 사용\n",
    "\n",
    "- $E = -\\sum_k t_k \\log{y_k}$\n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "$\\Rightarrow$ 실질적으로 정답 시의 추정($t_k = 1$일때의 $y_k$) (y_k는 확률적인 값이고 그 값들의 총합은 1이다 (Softmax function의 출력이기때문에) $t_k$는 one-hot-encoding되어 정답 레이블인데 정답은 1 나머지는 0으로 수식에서 계산되면 정답 외의 값들은 0으로 처리된다.)\n",
    "\n",
    "$\\Rightarrow$ 교차 엔트로피 오차는 정답일때 **출력이 전체 값을 정한다.**\n",
    "($y_k$의 값이 작다(확률이 작다)면 log가 취해서 전체 CEE의 값 자체가 커진다  $\\rightarrow$ 오차가 커진다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minsungkim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.001)\n",
    "y = np.log(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-5.1, 0.1)  # y축의 범위를 지정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답일 때의 출력(확률로 해석)이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "# 첫번째 결괏값이 더 작다(오차가 작다). 정답일 가능성이 높다고 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "\n",
    "기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고 그 값을 최소화하는 매개변수를 찾는다. $\\rightarrow$ 결국, 모든 훈련 데이터에 대한 손실 함수를 구해야한다.\n",
    "\n",
    "- $E = -\\sum_k t_k \\log{y_k}$ (데이터 1개에 대한 Loss Function)\n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "(N개의 데이터로 확장, N으로 나누어 정규화) $\\Rightarrow$\n",
    "\n",
    "- $E = -\\frac{1}{N}\\sum_{n} \\sum_k t_k \\log{y_k}$ \n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "N으로 나눠 **평균 손실 함수** 구해, But! 데이터가 커지면 시간 오래 걸린다.\n",
    "\n",
    "$\\Rightarrow$ 데이터 일부 추려 **근사치**를 이용 (일부: 미니배치(Mini-batch))\n",
    "\n",
    "**미니배치 학습**: Training data 중 무작위로 뽑아 학습하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cbb4f55235a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 훈련 데이터 60,000개, 입력 데이터 784(28 x 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/TIL/Deep learning from scratch/dataset/mnist.py\u001b[0m in \u001b[0;36mload_mnist\u001b[0;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 5"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import pickle\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)  # 훈련 데이터 60,000개, 입력 데이터 784(28 x 28)\n",
    "print(t_train.shape)  # 정답 레이블 (0 ~ 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 0 ~ train_size(60,000)에서 무작위로 batch_size(10)개를 뽑음\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 '2', '7'과 같이 주어졌을 때, cross-entropy error\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실함수를 설정하는가?\n",
    "\n",
    "왜 손실함수 사용, '정확도'라는 지표를 놔두고 우회적인 방법을 사용?\n",
    "\n",
    "정확도를 지표로 하면 매개변수의 미분이 대부분 장소에서 0이되기 때문\n",
    "\n",
    "정확도 지표를 사용시 매개변수의 미분이 대부분 0인 이유?\n",
    "\n",
    "만약 MNIST와 같은 이미지 데이터에서 100장의 training data 중 32장 올바르게 인식 $\\rightarrow$ 정확도: 32%\n",
    "\n",
    "if 정확도 지표를 사용시, 매개변수 값 조금 변해도 그대로 32%이다. 개선된다고 해도 연속적인 값이 아닌 불연속적 값이 나오기때문\n",
    "\n",
    "신경망 학습서 손실함수의 미분하여 매개변수 값을 서서히 갱신하는 과정을 반복\n",
    "- 미분 값이 음수 $\\rightarrow$ 가중치 매개변수를 양의 방향으로 변화시켜 loss function 값을 줄인다.\n",
    "- 미분 값이 양수 $\\rightarrow$ 가중치 매개변수를 음의 방향으로 변화시켜 loss function 값을 줄인다.\n",
    "- 미분 값이 0이면 $\\rightarrow$ 갱신 Stop\n",
    "\n",
    "정확도는 **매개변수의 미소한 변화에는 거의 반응하지 못한다.** 반응하더라도 불연속적 변화 (활성화 함수로 step function을 사용하지 않는 이유와 유사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분\n",
    "\n",
    "경사법에서 **기울기**(경사) 값을 기준으로 나아갈 방향을 정한다.\n",
    "\n",
    "### 4.3.1 미분\n",
    "미분: 한 순간의 변화량(순간 변화량)\n",
    "\n",
    "$\\frac{df(x)}{dx} = \\lim_{x\\to0} \\frac{f(x+h) - f(x)}{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h에 최대한 작은 값을 대입 위해 h = 1e-50 하지만, **반올림 오차(rounding error)** 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 반올림 오차 \n",
    "np.float32(1e-50)  # 0.0으로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$10^{-4}$ 정도 값 사용 시 좋은 결과 얻는다고 알려져 있다.\n",
    "\n",
    "$f$의 차분\n",
    "\n",
    "$f(x+h) - f(x)$는 $x+h$ 와 $x$ 사이의 차분\n",
    "\n",
    "**진정한 미분**: $x$ 위치에서 함수의 기울기(접선), 위 코드는 $(x_h)$와 $x$ 사이의 기울기에 해당\n",
    "\n",
    "h를 무한히 0으로 좁히는 것이 불가능하기에 생기는 한계\n",
    "\n",
    "**수치 미분**에는 **오차**가 포함된다. 이 오차를 줄이기 위해 $(x+h)$와 $(x-h)$일때 함수 $f$의 차분을 계산하는 방법도 쓰인다.\n",
    "\n",
    "이 차분은 $x$를 중심으로 그 전후의 차분을 계산한다는 의미에서 **중심 차분** or **중앙 차분**이라 한다. ($(x+h)$와 $x$의 차분을 **전방 차분**이라고 한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **수치 미분**: 차분으로 미분을 하는 것, 오차가 포함\n",
    "- **해적적 미분**: 수식을 전개하여 미분하는 것, 오차가 포함되지 않음, 진정한 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 수치 미분의 예\n",
    "$y = 0.01x^2 + 0.1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01 * x**2  + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5))  # x = 5\n",
    "print(numerical_diff(function_1, 10))  # x = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분\n",
    "\n",
    "**편미분**: 변수가 여럿인 함수에 대한 미분\n",
    "\n",
    "$f(x_0, x_1) = x_0^2 + x_1^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2  # np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기\n",
    "\n",
    "**기울기(Gradient)**: $(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1})$ 처럼 모든 변수의 편미분을 벡터로 정리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]  \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)\n",
    "\n",
    "경사법: 기울기 이용 손실함수의 min(or max)를 찾으려는 것\n",
    "\n",
    "기울기는 각 지점에서 함수값을 줄이는 방안을 제시하는 지표, 하지만, 실제로 방향이 맞는지는 보장 못한다.\n",
    "\n",
    "함수가 극대, 극소, **안장점(saddle point)** 에서 기울기가 **0**이다. \n",
    "- 극솟값: 국소적인 최솟값, 한정된 범위에서의 최솟값인 점\n",
    "- 안정점: 어느방향에서는 최솟값, 어느 방향에서는 극댓값이 되는 점\n",
    "\n",
    "$\\rightarrow$ 기울기 0이라고 해서 최솟값이라고 할 수 **없다**. 그점이 극솟값이나 안장점일 수 도 있기때문이다.\n",
    "\n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그리고 다음 이동한 곳에서 다시 기울기 구하고, 또 기울어진 방향으로 이동 반복\n",
    "- 경사 하강법(Gradient Descent): 최솟값을 찾음\n",
    "- 경사 상승법(Gradient Ascent): 최댓값을 찾음\n",
    "\n",
    "- $x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0}$\n",
    "- $x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}$\n",
    "\n",
    "$\\eta$(eta, 에타): **학습률(learning rate)** : 매개변수의 값을 얼마나 갱신하느냐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f: 최적화하려는 함수\n",
    "- init_x: 초깃값\n",
    "- lr: learning rate\n",
    "- step_num: 경사법에 따른 반복 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사법으로 $f(x_0, x_1) = x_0^2 + x_1^2$의 최솟값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, lr=0.1, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 (0, 0)에 가까운 결과, 실제 최솟값은 (0, 0)이므로 거의 정확한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=10.0, step_num = 100))\n",
    "\n",
    "# 학습률이 너무 작은 예\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=1e-10, step_num = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 크면 발산, 너무 작으면 거의 갱신되지 않고 끝난다.\n",
    "\n",
    "learning rate와 같은 파라메터를 **하이퍼파라미터(hyperparameter)** 라고 한다. 이 하이퍼파라미터는 사람이 직접 설정해야 하며, 여러 하이퍼파리미터 후보 값 중 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다.\n",
    "\n",
    "### 4.4.2 신경망에서 기울기\n",
    "\n",
    "간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)  # 정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"예측을 수행하는 메서드\"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수의 값 구하는 메서드\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17810556 -2.06523344 -0.74287464]\n",
      " [-0.79707693 -0.23879957  0.64481973]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.82423257 -1.45405968  0.13461297]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4621802269922537"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "print(np.argmax(p))  # p의 최댓값\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14487967  0.07717507 -0.22205474]\n",
      " [ 0.21731951  0.1157626  -0.33308211]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "- 전제: 신경망엔 적응 가능한 가중치와 편향있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. 4단계로 구성\n",
    "\n",
    "1단계 - 미니배치\n",
    "\n",
    "- 훈련 데이터 중 일부를 무작위로 가져옴, 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 주이는 것을 목표로 한다.\n",
    "\n",
    "2단계 - 기울기 산출\n",
    "\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "3단계 - 매개변수 갱신\n",
    "\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "4단계 - 반복\n",
    "\n",
    "- 1~3 단계를 반복한다.\n",
    "\n",
    "위 단계는 경사 하강법으로 매개변수를 갱신하는 방법이다. 이때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(Stochastic gradient descent)** 라고 부른다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 사용, 2층 신경망 구현(은닉층 1개인 네트워크)\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = sigmoid(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x: 입력 데이터, y: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(y, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    " \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet class가 사용하는 변수\n",
    "- params: 신경망의 매개변수를 보관하는 dict변수(인스턴스 변수)\n",
    "    - params['W1']: 1번째 층의 가중치, params['b1']: 1번째 층의 편향\n",
    "    - params['W2']: 2번째 층의 가중치, params['b2']: 2번째 층의 편향\n",
    "\n",
    "- grads: 기울기를 보관하는 dict변수(numerical_gradient() 메서드의 반환 값)\n",
    "    - grads['W1']: 1번째 층의 가중치의 기울기, grads['b1']: 1번째 층의 편향의 기울기\n",
    "    - grads['W2']: 2번째 층의 가중치의 기울기, grads['b2']: 2번째 층의 편향의 기울기\n",
    "    \n",
    "    \n",
    "TwoLayerNet class가 사용하는 메서드\n",
    "- \\__init__(self, input_size, hidden_size, output_size): 초기화를 수행, 인수는 순서대로 **입력층의 뉴런의 수, 은닉층의 뉴런의 수, 출력층의 뉴런의 수**\n",
    "- predidct(self, x): 예측(추론)을 수행, 인수 x는 이미지 데이터\n",
    "- loss(self, x, t): 손실함수의 값을 구한다. 인수 x는 이미지 데이터, t는 정답 레이블\n",
    "- accuracy(self, x, t): 정확도를 구한다.\n",
    "- numerical_gradient(self, x, t): 가중치 매개변수의 기울기를 구한다.\n",
    "- gradient(self, x, t): 가중치 매개변수의 기울기를 구한다. numerical_gradient()의 성능 개선판, 구현은 다음장에.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params 변수에는 신경망에 필요한 매개변수가 모두 저장됩니다. params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)  # 더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100, 10)  # 더미 정답 레이블(100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)  # 기울기 계산\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\__init__(self, input_size, hidden_size, output_size): 클래스 초기화 (초기화 메서드는 TwoLayerNet을 생성할 때 불리는 메서드)\n",
    "\n",
    "가중치 초기화 메서드에서는 가중치 매개변수도 초기화 한다. 가중치 매개변수의 초깃값을 무엇으로 설정하냐가 신경망 학습의 성공을 좌우하기도 한다.\n",
    "\n",
    "정규분포를 따르는 난수로, 편향은 0으로 초기화 한다.\n",
    "\n",
    "numerical_gradient(self, x, t)는 **수치 미분 방식** 매개변수의 기울기를 계산, 이 기울기를 고속으로 수행하는 방법은 **오차역전파법**이다. 오차역전파법은 수치 미분을 사용할 때와 거의 같은 결과를 훨씬 빠르게 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# hyperparameter\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 시험 데이터로 평가하기\n",
    "\n",
    "손실함수의 값이란, 정확히는 **'훈련 데이터의 미니배치에 대한 손실 함수'의 값**\n",
    "\n",
    "훈현 데이터의 손실 함수의 값이 작아지는 것은 신경망이 잘 학습하고 있다는 방증이지만, 이 결과만으로는 다른 데이터셋에도 비슷한 실력을 발휘하는 지 확실하지 않다.\n",
    "\n",
    "신경망 학습에는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야한다. 즉, '오버피팅'을 확인해야 한다. \n",
    "\n",
    "신경망 학습의 **목표**는 **범용적인 능력**을 익히는 것.\n",
    "\n",
    "범용 능력을 평가하려면 훈련 데이터에 포함되지 않는 데이터를 사용해 평가해봐야한다. \n",
    "\n",
    "**에폭(epoch)**: 하나의 단위, **1에폭**은 **학습에서 훈련 데이터를 모두 소진했을 때의 횟수**에 해당합니다. \n",
    "\n",
    "e.g) 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '소진'하게 된다. 이 경우 100회가 1에폭이 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# hyperparameter\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learing_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuarcy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 정리\n",
    "신경망이 학습을 수행할 수 있도록 손실 함수라는 '지표'를 도입, 이 손실 함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수 값을 찾아내는 것이 신경망 학습의 목표이다. 그 손실 함수의 값이 가장 작아지게 만드는 매개변수를 찾는 기법 중 하나가 경사법이다. 경사법은 함수의 기울기를 이용하는 방법\n",
    "\n",
    "- 기계학습에서 데이터를 training data, test data로 나눠 사용한다.\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이로 평가한다\n",
    "- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.\n",
    "- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.\n",
    "- 아주 작은 값을 주었을 때 차분으로 미분하는 것을 수치 미분이라고 한다.\n",
    "- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.\n",
    "- 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 오차역전파법은 기우릭를 고속으로 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3",
   "language": "python",
   "name": "python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
