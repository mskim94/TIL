{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04. 신경망 학습\n",
    "\n",
    "신경망 학습에 대해 배운다.\n",
    "\n",
    "학습이란? 데이터로부터 가중치 매개변수의 최적값을 자동으로 얻는것\n",
    "\n",
    "신경망이 학습할 수 있도록 하는 **지표** 인 **손실 함수(Loss Function)** 소개한다.\n",
    "\n",
    "학습 목표: 손실함수의 결괏값을 가장 작게하는 가중치 매개변수를 찾는 것\n",
    "\n",
    "손실 함수의 값을 작게 만드는 기법으로 **경사 하강법(Gradient Descent)** 가 있다.\n",
    "\n",
    "## 4.1 데이터에서 학습한다!\n",
    "\n",
    "신경망의 특징: 데이터를 보고 학습한다. $\\rightarrow$ 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.\n",
    "\n",
    "### 4.1.1 데이터 주도 학습\n",
    "기계학습에서 **데이터**는 매우 중요\n",
    "\n",
    "문제를 풀때, 특히 패턴을 찾을 때, \n",
    "- **사람**은 경험, 직관을 이용하여 시행착오를 거듭하면서 문제 해결을 진행\n",
    "- **기계학습**은 사람 개입을 최소화, 수집한 데이터로 부터 패턴을 찾으려 시도\n",
    "- **신경망, 딥러닝**은 기계학습 보다 더 사랑ㅁ의 개입을 배제\n",
    "\n",
    "e.g) MNIST 문제를 풀때 Algorithm or program을 처음부터 만들기 어려워 $\\rightarrow$ image에서 **특징** 추출, 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.\n",
    "\n",
    "- 특징(feature): 입력 데이터에서 본질적인 데이터(중요한 데이터) 추출하는 **변환기**, 이 특징은 **벡터(vector)** 로 기술\n",
    "    - Computer Vision 분야에서는 SIFT, SURF, HOG 등 특징 활용\n",
    "\n",
    "특징을 사용해 데이터를 벡터로 변환, 이 변환된 데이터를 지도학습 대표 기법인 SVM, KNN 등으로 학습 가능하다.\n",
    "\n",
    "모아진 데이터를 기계가 규칙을 찾는다. 처음부터 Algorithm을 설계하는 것보다 효율이 좋지만, 이 **특징** 을 **사람이 설계(개입)**, 문제에 따라 사람이 특징을 생각해야 한다.\n",
    "\n",
    "문제 $\\rightarrow$ 사람이 생각한 Algorithm $\\rightarrow$ 결과\n",
    "\n",
    "$(개선)\\Rightarrow$ 문제 $\\rightarrow$ 사람이 생각한 특징(SIFT, SURF, HOG etc...) $\\rightarrow$ 기계학습(SVM, KNN) $\\rightarrow$ 결과\n",
    "\n",
    "$(개선)\\Rightarrow$ 문제 $\\rightarrow$ 신경망(딥러닝) $\\rightarrow$ 결과\n",
    "\n",
    "신경망(딥러닝)에서는 이미지를 '있는 그대로' 학습, 특징까지 기계가 스스로 학습(기계학습, 신경망(딥러닝) 과정에서는 사람의 개입이 없다.)\n",
    "\n",
    "cf) 딥러닝을 **종단간 기계학습(end-to-end machine learning)** 이라고도 함 $\\Rightarrow$ 데이터(입력)부터 결과(출력)까지 사람의 개입이 없다.\n",
    "\n",
    "**신경망의 이점**: 모든 문제 같은 맥락에서 푼다 $\\Rightarrow$ 모든 문제를 주어진 데이터 그대로 사용, 입력 데이터 활용 'end-to-end'로 학습 가능\n",
    "\n",
    "### 4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "기계학습에서는 데이터를 훈련 데이터와 시험 데이터로 나눈다. \n",
    "\n",
    "Why Split? **범용 능력** 평가를 위해 \n",
    "- 범용 능력: 아직 보지 못한 data(training data에 없는)로 문제를 옳바르게 풀어내는 능력\n",
    "\n",
    "한 데이터에만 지나치게  최적화 된 상태: **오버피팅(overfitting)**\n",
    "\n",
    "## 4.2 손실 함수(Loss Function)\n",
    "\n",
    "신경망 학습에서 현재 상태를 '하나의 지표'로 표현, 이 지표를 가장 좋게 하는 가중치 매개변수 값을 탐색하는 것이 목적\n",
    "\n",
    "**손실 함수**: 신경망 학습에 사용되는 지표\n",
    "\n",
    "일반적으로 손실함수로 \"오차제곱합\", \"교차 엔트로피 오차\" 를 사용\n",
    "\n",
    "### 4.2.1 오차제곱합(Sum of Squares Error, SSE)\n",
    "\n",
    "가장 많이 사용하는 손실 함수 오차제곱합(Sum of Squares Error, SSE)\n",
    "\n",
    "각 원소의 출력(추정값)과 정답 레이블(참 값)의 차($y_k - t_k$)를 제곱(Squares) 후, 그 총합($\\sum$)\n",
    "\n",
    "- $E = \\frac{1}{2} \\sum_{k}(y_k - t_k)^2$\n",
    "    - $y_k$: 신경망의 출력(신경망이 추정한 값)\n",
    "    - $t_k$: 정답 레이블(One-hot-encoding)\n",
    "    - $k$: 데이터의 차원 수\n",
    "    - 신경망 출력 $y$는 Softmax function의 출력(확률로 해석 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작은 0.097이 정답에 더 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차(Cross-Entropy Error, CEE)\n",
    "\n",
    "CEE도 자주 사용\n",
    "\n",
    "- $E = -\\sum_k t_k \\log{y_k}$\n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "$\\Rightarrow$ 실질적으로 정답 시의 추정($t_k = 1$일때의 $y_k$) (y_k는 확률적인 값이고 그 값들의 총합은 1이다 (Softmax function의 출력이기때문에) $t_k$는 one-hot-encoding되어 정답 레이블인데 정답은 1 나머지는 0으로 수식에서 계산되면 정답 외의 값들은 0으로 처리된다.)\n",
    "\n",
    "$\\Rightarrow$ 교차 엔트로피 오차는 정답일때 **출력이 전체 값을 정한다.**\n",
    "($y_k$의 값이 작다(확률이 작다)면 log가 취해서 전체 CEE의 값 자체가 커진다  $\\rightarrow$ 오차가 커진다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minsungkim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.001)\n",
    "y = np.log(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-5.1, 0.1)  # y축의 범위를 지정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답일 때의 출력(확률로 해석)이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "# 첫번째 결괏값이 더 작다(오차가 작다). 정답일 가능성이 높다고 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "\n",
    "기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고 그 값을 최소화하는 매개변수를 찾는다. $\\rightarrow$ 결국, 모든 훈련 데이터에 대한 손실 함수를 구해야한다.\n",
    "\n",
    "- $E = -\\sum_k t_k \\log{y_k}$ (데이터 1개에 대한 Loss Function)\n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "(N개의 데이터로 확장, N으로 나누어 정규화) $\\Rightarrow$\n",
    "\n",
    "- $E = -\\frac{1}{N}\\sum_{n} \\sum_k t_k \\log{y_k}$ \n",
    "    - $y_k$: 신경망의 출력\n",
    "    - $t_k$: 정답 레이블(One-Hot-Encoding)\n",
    "\n",
    "N으로 나눠 **평균 손실 함수** 구해, But! 데이터가 커지면 시간 오래 걸린다.\n",
    "\n",
    "$\\Rightarrow$ 데이터 일부 추려 **근사치**를 이용 (일부: 미니배치(Mini-batch))\n",
    "\n",
    "**미니배치 학습**: Training data 중 무작위로 뽑아 학습하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import pickle\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)  # 훈련 데이터 60,000개, 입력 데이터 784(28 x 28)\n",
    "print(t_train.shape)  # 정답 레이블 (0 ~ 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 0 ~ train_size(60,000)에서 무작위로 batch_size(10)개를 뽑음\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 '2', '7'과 같이 주어졌을 때, cross-entropy error\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실함수를 설정하는가?\n",
    "\n",
    "왜 손실함수 사용, '정확도'라는 지표를 놔두고 우회적인 방법을 사용?\n",
    "\n",
    "정확도를 지표로 하면 매개변수의 미분이 대부분 장소에서 0이되기 때문\n",
    "\n",
    "정확도 지표를 사용시 매개변수의 미분이 대부분 0인 이유?\n",
    "\n",
    "만약 MNIST와 같은 이미지 데이터에서 100장의 training data 중 32장 올바르게 인식 $\\rightarrow$ 정확도: 32%\n",
    "\n",
    "if 정확도 지표를 사용시, 매개변수 값 조금 변해도 그대로 32%이다. 개선된다고 해도 연속적인 값이 아닌 불연속적 값이 나오기때문\n",
    "\n",
    "신경망 학습서 손실함수의 미분하여 매개변수 값을 서서히 갱신하는 과정을 반복\n",
    "- 미분 값이 음수 $\\rightarrow$ 가중치 매개변수를 양의 방향으로 변화시켜 loss function 값을 줄인다.\n",
    "- 미분 값이 양수 $\\rightarrow$ 가중치 매개변수를 음의 방향으로 변화시켜 loss function 값을 줄인다.\n",
    "- 미분 값이 0이면 $\\rightarrow$ 갱신 Stop\n",
    "\n",
    "정확도는 **매개변수의 미소한 변화에는 거의 반응하지 못한다.** 반응하더라도 불연속적 변화 (활성화 함수로 step function을 사용하지 않는 이유와 유사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분\n",
    "\n",
    "경사법에서 **기울기**(경사) 값을 기준으로 나아갈 방향을 정한다.\n",
    "\n",
    "### 4.3.1 미분\n",
    "미분: 한 순간의 변화량(순간 변화량)\n",
    "\n",
    "$\\frac{df(x)}{dx} = \\lim_{x\\to0} \\frac{f(x+h) - f(x)}{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h에 최대한 작은 값을 대입 위해 h = 1e-50 하지만, **반올림 오차(rounding error)** 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 반올림 오차 \n",
    "np.float32(1e-50)  # 0.0으로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$10^{-4}$ 정도 값 사용 시 좋은 결과 얻는다고 알려져 있다.\n",
    "\n",
    "$f$의 차분\n",
    "\n",
    "$f(x+h) - f(x)$는 $x+h$ 와 $x$ 사이의 차분\n",
    "\n",
    "**진정한 미분**: $x$ 위치에서 함수의 기울기(접선), 위 코드는 $(x_h)$와 $x$ 사이의 기울기에 해당\n",
    "\n",
    "h를 무한히 0으로 좁히는 것이 불가능하기에 생기는 한계\n",
    "\n",
    "**수치 미분**에는 **오차**가 포함된다. 이 오차를 줄이기 위해 $(x+h)$와 $(x-h)$일때 함수 $f$의 차분을 계산하는 방법도 쓰인다.\n",
    "\n",
    "이 차분은 $x$를 중심으로 그 전후의 차분을 계산한다는 의미에서 **중심 차분** or **중앙 차분**이라 한다. ($(x+h)$와 $x$의 차분을 **전방 차분**이라고 한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **수치 미분**: 차분으로 미분을 하는 것, 오차가 포함\n",
    "- **해적적 미분**: 수식을 전개하여 미분하는 것, 오차가 포함되지 않음, 진정한 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 수치 미분의 예\n",
    "$y = 0.01x^2 + 0.1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01 * x**2  + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5))  # x = 5\n",
    "print(numerical_diff(function_1, 10))  # x = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분\n",
    "\n",
    "**편미분**: 변수가 여럿인 함수에 대한 미분\n",
    "\n",
    "$f(x_0, x_1) = x_0^2 + x_1^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2  # np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기\n",
    "\n",
    "**기울기(Gradient)**: $(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1})$ 처럼 모든 변수의 편미분을 벡터로 정리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]  \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)\n",
    "\n",
    "경사법: 기울기 이용 손실함수의 min(or max)를 찾으려는 것\n",
    "\n",
    "기울기는 각 지점에서 함수값을 줄이는 방안을 제시하는 지표, 하지만, 실제로 방향이 맞는지는 보장 못한다.\n",
    "\n",
    "함수가 극대, 극소, **안장점(saddle point)** 에서 기울기가 **0**이다. \n",
    "- 극솟값: 국소적인 최솟값, 한정된 범위에서의 최솟값인 점\n",
    "- 안정점: 어느방향에서는 최솟값, 어느 방향에서는 극댓값이 되는 점\n",
    "\n",
    "$\\rightarrow$ 기울기 0이라고 해서 최솟값이라고 할 수 **없다**. 그점이 극솟값이나 안장점일 수 도 있기때문이다.\n",
    "\n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그리고 다음 이동한 곳에서 다시 기울기 구하고, 또 기울어진 방향으로 이동 반복\n",
    "- 경사 하강법(Gradient Descent): 최솟값을 찾음\n",
    "- 경사 상승법(Gradient Ascent): 최댓값을 찾음\n",
    "\n",
    "- $x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0}$\n",
    "- $x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}$\n",
    "\n",
    "$\\eta$(eta, 에타): **학습률(learning rate)** : 매개변수의 값을 얼마나 갱신하느냐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f: 최적화하려는 함수\n",
    "- init_x: 초깃값\n",
    "- lr: learning rate\n",
    "- step_num: 경사법에 따른 반복 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사법으로 $f(x_0, x_1) = x_0^2 + x_1^2$의 최솟값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, lr=0.1, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 (0, 0)에 가까운 결과, 실제 최솟값은 (0, 0)이므로 거의 정확한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=10.0, step_num = 100))\n",
    "\n",
    "# 학습률이 너무 작은 예\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, lr=1e-10, step_num = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 크면 발산, 너무 작으면 거의 갱신되지 않고 끝난다.\n",
    "\n",
    "learning rate와 같은 파라메터를 **하이퍼파라미터(hyperparameter)** 라고 한다. 이 하이퍼파라미터는 사람이 직접 설정해야 하며, 여러 하이퍼파리미터 후보 값 중 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다.\n",
    "\n",
    "### 4.4.2 신경망에서 기울기\n",
    "\n",
    "간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log 0은 -inf로 발산하게 되므로 아주 작은 값을 더했다.\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)  # 정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"예측을 수행하는 메서드\"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수의 값 구하는 메서드\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55280397 -0.10819087  0.17493032]\n",
      " [-0.30625524 -0.31785921  0.72351334]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05605267 -0.3509878   0.7561202 ]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.60271077585391"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "print(np.argmax(p))  # p의 최댓값\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16306534  0.1085391  -0.27160444]\n",
      " [ 0.24459801  0.16280866 -0.40740666]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "- 전제: 신경망엔 적응 가능한 가중치와 편향있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. 4단계로 구성\n",
    "\n",
    "1단계 - 미니배치\n",
    "\n",
    "- 훈련 데이터 중 일부를 무작위로 가져옴, 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 주이는 것을 목표로 한다.\n",
    "\n",
    "2단계 - 기울기 산출\n",
    "\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "3단계 - 매개변수 갱신\n",
    "\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "4단계 - 반복\n",
    "\n",
    "- 1~3 단계를 반복한다.\n",
    "\n",
    "위 단계는 경사 하강법으로 매개변수를 갱신하는 방법이다. 이때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(Stochastic gradient descent)** 라고 부른다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3",
   "language": "python",
   "name": "python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
