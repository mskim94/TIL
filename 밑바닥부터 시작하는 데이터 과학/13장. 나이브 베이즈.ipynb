{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이즈\n",
    "\n",
    "\n",
    "## 13.1 바보 스팸 필터\n",
    "메시지가 스팸인 경우를 $S$, 메시지에 '비트코인'이라는 단어가 포함되어 있는 경우 $B$라고 정의 \\\n",
    "메시지에 '비트코인'이라는 단어가 포함되었을 때(조건) 해당 메시지가 스팸일 확률 by. 베이즈 정리 \n",
    "\n",
    "$P(S|B) = [P(B|S)P(S)] / [P(B|S)P(S) + P(B|-S)P(-S)]$\n",
    "\n",
    "만약, 메시지가 스팸일 확률과 스팸이 아닐 확률이 동일($P(S) = P(-S) = 0.5$), 아래와 같이 정리할 수 있다.\n",
    "\n",
    "$P(S|B) = P(B|S) / [P(B|S) + P(B|-S)]$\n",
    "\n",
    "예를 들어 스팸 메시지 중 50%, 스팸이 아닌 메시지 중 1%만이 비트코인이라는 단어를 포함한다면, 비트코인이라는 단어를 포함하고 있는 메시지가 스팸일 확률은 \n",
    "\n",
    "0.5 / (0.5 + 0.01) = 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 조금 더 똑똑한 스팸 필터\n",
    "더 많은 단어 ${w}_1, {w}_2, ..., {w}_n$이 주어졌다고 하고, 단어 ${w}_i$가 메시지에 포함되는 경우를 ${X}_i$로 나타내자. \\\n",
    "스팸 메시지에 i번째 단어가 포함되어 있는 확률인 $P({X}_i | S)$와 스팸이 아닌 메시지에 i번째 단어가 포함되어 있는 확률인 $P({X}_i | -S)$가 주여졌다고 해보자. \\\n",
    "나이브 베이즈(Naive Bayes)의 핵심은 **메시지가 스팸이나 아니냐가 주어졌다는 조건하에 각 단어의 존재(혹은 부재)는 서로 조건부 독립이다.** 라는 가정에 기반을 둔다. $\\rightarrow$ 이를 직관적으로 이해하면, 어떤 스팸 메시지가 비트코인이라는 단어를 포함하고 있다는 점이 같음 메시지가 롤렉스라는 단어를 포함하고 있는지를 판단하는 데 도움을 주지 않는다는 것을 의미 \n",
    "\n",
    "$\\Rightarrow$ $P({X}_i = {x}_1,...,{X}_n = {x}_n | X) = P({X}_1 = {x}_1 | S) x ... x P({X}_n = {x}_n | S)$\n",
    "\n",
    "나이즈(단순한) 베이즈라는 이름에서 알 수 있듯이, 나이브 베이즈는 너무 극단적인 가정을 한다. 예를 들어 사전에 수록된 단어가 비트코인과 롤렉스뿐이며, 모든 스팸 메시지의 반은 '비트코인을 얻으세요'에 관한 메시지이고, 나머지 스팸 메시지는 '정품 롤렉스'에 대한 메시지라고 해보자. 이러한 경우 나이브 베이즈는 스팸 메시지에 비트코인과 롤렉스라는 단어가 포함된 확률을 다믕과 같이 추정한다. \n",
    "\n",
    "$P({X}_1 = 1, {X}_2 = 1 | S) = P({X}_1 = 1|S) P({X}_2 = 1|S) = .5 x .5 = .25$\n",
    "\n",
    "이 경우 비트코인과 롤렉스가 동시에 등장하지 않는다는 점을 무시하였다. 비록 말도 안 되는 가정으로 모델을 만들었지만 성능은 상당히 뛰어나며, 이는 실제 스팸 필터로 사용되어 왔다. \\\n",
    "\n",
    "'비트코인'이라는 단어만으로 스팸을 걸러내는 필터에서도 사용된 베이즈 정리를 통해 메시지가 스팸일 확률을 다음과 같이 계산할 수 있다. \\\n",
    "\n",
    "$P(S|X = x) = P(X = x|S) / [P(X = x|S) + P(X = x | -S)]$\n",
    "\n",
    "나이브 베이지 정리에 의하면, 각 단어가 메시지에 포함될 확률값을 모두 곱해서 위 식의 우변에 위치한 확률값을 모두 구할 수 있다. \\\n",
    "실제 구현시 끊임없이 확률값을 곱하는 것을 피하자. 컴퓨터가 0에 가까운 부동소수점(floating point number)을 제대로 처리하지 못해서 언더플로(underflow) 문제가 발생한다. 부동소수점 문제를 피하기 위해 $p_1 * ... * p_n$은 다음과 같이 계산한다. \n",
    "\n",
    "$exp(\\log(p_1) + ... + \\log(p_n))$ \n",
    "\n",
    "이제 스팸이나 스팸이나 아닌 메시지에 단어 ${w}_i$가 포함될 확률인 $P({X}_i | S)$와 $P({X}_i | -S)$의 값을 추정하는 일만 남았다. \\\n",
    "만약 이미 스팸과 스팸이 아닌 메시지로 분류된 '학습' 메시지가 충분히 주어졌다면 $P({X}_i | S)$를 추정할 수 있는 가장 간단한 방법은 스팸 메시지 중 ${w}_i$가 포함된 메시지의 비율을 사용하는 것이다. \\\n",
    "이 방법에는 단점이 있는데 주어진 학습 데이터에서 '데이터'라는 단어는 스팸이 아닌 메시지에만 포함되어 있다고 상상해보자. 즉, P('데이터'|S) = 0 이다. 그러면 아니브 베이즈 분류기는 데이터라는 단어가 들어간 메시지를 항상 스팸 메시지가 아니라고 예측할 것이다. 또한 '무료 비트코인과 롤렉스에 대한 데이터'라는 메시지도 스팸이 아니라고 예측할 것이다. 이 문제를 해결하기 위해 **스무딩(smoothing, 평활화)** 이 필요하다. \\\n",
    "스무딩을 위해 **가자 빈도수(pseudocount)** k를 결정, 스팸 메시지에서 i번째 단어가 나올 확률을 다음과 같이 추정 가능 \n",
    "\n",
    "$P(X_i | S) = (k + w_i를 포함하고 있는 스팸 수) / (2k + 스팸 수)$ \\\n",
    "\n",
    "$P(X_i | -S)$도 비슷하게 계산 가능하다. 스팸 메시지에서 i번째 단어가 나올 확률을 계산할 때 해당 단어가 포함된 스팸과 포함되지 않은 스팸이 이미 각각 k번씩 나왔다고 가정한다. \\\n",
    "예를 들어 데이터라는 단어는 98개의 스팸 문서에서 단 한 번도 나타나지 않았지만, k가 1이라면 P(데이터|S)를 1/100=0.01로 계산할 수 있다.\\\n",
    "즉, 데이터라는 단어가 포함된 메시지가 스팸 메시지일 확률을 0이 아닌 확률값으로 설정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()  # 소문자 변환\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)  # 단어 추출\n",
    "    return set(all_words)\n",
    "\n",
    "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 위해 타입도 정의\n",
    "from typing import NamedTuple\n",
    "    \n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기가 학습 데이터의 단어, 빈도, 레이블을 관리해야 하기 때문에 이를 클래스로 만들 것이다. \\\n",
    "이메일을 '햄(ham)'으로 표기 \\\n",
    "생성자는 가짜 빈도수를 유일한 인자로 받아 확률 계산에 활용한다. 생성자에서는 각 단어가 스팸과 햄 메시지에서 얼마나 자주 등장했는지 기록하기 위한 빈 집합을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # 스무딩 인수\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "        \n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # 메시지의 수를 증가시킨다.\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # 단어의 빈도를 증가시킨다.\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "                    \n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"P(단어|스팸)과 P(단어|햄)을 반환\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "    \n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # 모든 메시지 안의 각 단어를 순회한다.\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # 만약 *token*이 메시지에 나온다면\n",
    "            # 단어가 등장할 로그 확률값을 더한다.\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # 그게 아니라면 단어가 등장하지 않을 로그 확률을 더한다.\n",
    "            # 이는 log(1 - 등장할 확률)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메시지를 사용해서 학습할 수 있도록 메서드를 만든다. 일단 두 메시지의 빈도를 기록하는 spam_messages, ham_messages를 증가시킨다. 그리고 각 메시지를 단어 기준으로 나누며, 각 단어별로 메시지의 종류에 따라 token_spam_counts or token_ham_counts를 증가시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 $P(스팸|단어)$를 계산 \\\n",
    "위의 값을 계산 하기 위해서 베이즈 정리를 사용하고 이를 위해서 메시지 안의 모든 단어마다 $P(단어|스팸)$ 과 $P(단어|햄)$값이 필요하다. \\\n",
    "이를 위해 '프라이빗' 도우미 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "    \"\"\"P(단어|스팸)과 P(단어|햄)을 반환\"\"\"\n",
    "    spam = self.token_spam_counts[token]\n",
    "    ham = self.token_ham_counts[token]\n",
    "    \n",
    "    p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "    p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "    \n",
    "    return p_token_spam, p_token_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측을 수행하는 predict 메서드를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, text: str) -> float:\n",
    "    text_tokens = tokenize(text)\n",
    "    log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "    \n",
    "    # 모든 메시지 안의 각 단어를 순회한다.\n",
    "    for token in self.tokens:\n",
    "        prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "        \n",
    "        # 만약 *token*이 메시지에 나온다면\n",
    "        # 단어가 등장할 로그 확률값을 더한다.\n",
    "        if token in text_tokens:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_ham += math.log(prob_if_ham)\n",
    "            \n",
    "        # 그게 아니라면 단어가 등장하지 않을 로그 확률을 더한다.\n",
    "        # 이는 log(1 - 등장할 확률)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "    \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_ham = math.exp(log_prob_if_ham)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 모델 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [Message(\"spam rules\", is_spam=True),\n",
    "           Message(\"ham rules\", is_spam=False),\n",
    "           Message(\"hello ham\", is_spam=False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "assert model.spam_messages == 1\n",
    "assert model.ham_messages == 2\n",
    "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
    "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측을 해보자. 그리고 구현한 나이브 베이즈를 다라 손으로 값을 직접 계산해 보고 결과가 동일한지 비교해 볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello spam\"\n",
    "\n",
    "probs_if_spam = [\n",
    "    (1 + 0.5) / (1 + 2 * 0.5),  # \"spam\" (존재)\n",
    "    1 - (0 +  0.5) / (1 + 2 * 0.5),  # \"ham\" (존재하지 않음)\n",
    "    1 - (1 +  0.5) / (1 + 2 * 0.5),  # \"rules\" (존재하지 않음)\n",
    "    (0 + 0.5) / (1 + 2 * 0.5),  # \"hello\" (존재)\n",
    "]\n",
    "\n",
    "probs_if_ham = [\n",
    "    (0 + 0.5) / (2 + 2 * 0.5),  # \"spam\" (존재)\n",
    "    1 - (2 +  0.5) / (2 + 2 * 0.5),  # \"ham\" (존재하지 않음)\n",
    "    1 - (1 +  0.5) / (2 + 2 * 0.5),  # \"rules\" (존재하지 않음)\n",
    "    (1 + 0.5) / (2 + 2 * 0.5),  # \"hello\" (존재)\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "\n",
    "# 약 0.83 정도가 나와야 한다.\n",
    "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO  # 바이트를 파일로 다루기 위해 필요 \n",
    "import requests # 파일을 내려받기 위해 필요\n",
    "import tarfile  # 파일이 .tar.bz 형식이기 때문에 필요\n",
    "\n",
    "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\"\n",
    "FILES = [\"20021010_easy_ham.tar.bz2\",\n",
    "        \"20021010_hard_ham.tar.bz2\",\n",
    "        \"20021010_spam.tar.bz2\"]\n",
    "\n",
    "# 데이터는 최종적으로\n",
    "# /spam, /easy_ham, /hard_ham 하위 디렉터리에 놓이게 된다.\n",
    "# 데이터가 놓였으면 하는 위치로 변경하라.\n",
    "OUTPUT_DIR = 'spam_data'\n",
    "\n",
    "for filename in FILES:\n",
    "    # requests를 사용하여 각각의 URL에서 파일의 내용을 가져오자\n",
    "    content = requests.get(f\"{BASE_URL}/{filename}\").content\n",
    "    \n",
    "    # 메모리의 바이트를 감싸서 '파일'로 다룰 수 있도록 하자.\n",
    "    fin = BytesIO(content)\n",
    "    \n",
    "    # 그리고 모든 파일을 지정된 출력 디렉터리로 풀도록 하자\n",
    "    with tarfile.open(fileobj=fin, mode='r:bz2') as tf:\n",
    "        tf.extractall(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "# 실제 파일의 저장한 경로로 path를 바꿔주자.\n",
    "path = 'spam_data/*/*'\n",
    "\n",
    "data: List[Message] = []\n",
    "    \n",
    "# glob.glob는 주어진 경로에 해당하는 모든 파일 이름을 반환\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "    \n",
    "    # 메일에 잘못된 문자가 들어 있는 경우가 있다.\n",
    "    # errors = 'ignore'는 exception을 반환하는 대신 건너뛰도록 해준다.\n",
    "    with open(filename, errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break  # done with this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 학습 데이터와 평가 데이터로 나누면 분류기를 만들 준비가 끝난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "X = TypeVar('X')  # 데이터를 표현하기 위한 일반적인 타입\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"데이터를 [prob, 1 - prob]의 비율로 나눈다.\"\"\"\n",
    "    data = data[:]  # 얕은 복사본을 만든다.\n",
    "    random.shuffle(data)  # shuffle이 리스트 내용을 바꾸기 때문\n",
    "    cut = int(len(data) * prob)  # prob을 사용하여 자를 위치를 선택하고\n",
    "    return data[:cut], data[cut:]  # 섞인 리스트를 자른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)  # 예시와 동일한 결과를 얻기 위해서 설정\n",
    "train_messages, test_messages = split_data(data, 0.75)\n",
    "\n",
    "model = NaiveBayesClassifier()\n",
    "model.train(train_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 661, (True, True): 102, (True, False): 37, (False, True): 25})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "predictions = [(message, model.predict(message.text)) for message in test_messages]\n",
    "\n",
    "# 메시지가 스팸일 확률이 0.5보다 크면 스팸이라고 하자.\n",
    "# 그리고 예측된 스팸 메시지가 실제 스팸인 경우를 세어 보자\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n",
    "                          for message, spam_probability in predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 80.31496062992126\n",
      "recall: 13.368283093053734\n"
     ]
    }
   ],
   "source": [
    "print(\"precision:\", 102 / (102 + 25) * 100)\n",
    "print(\"recall:\", 102 / (102 + 661) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 단어가 가장 스팸을 잘 대표하고 그렇지 않은지 살펴보는 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spammiest_words ['assistance', 'per', 'clearance', 'sale', 'mortgage', 'only', 'systemworks', 'adv', 'rates', 'money']\n",
      "hammies_words ['spambayes', 'was', 'users', 'razor', 'zzzzteana', 'apt', 'sadev', 'perl', 'ouch', 'spamassassin']\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
    "    # 프라이빗 메서드를 호출하는 것은 피해야 하지만 이번에만 이렇게 사용하자. \n",
    "    prob_if_spam, probs_if_ham = model._probabilities(token)\n",
    "    \n",
    "    return prob_if_spam / (prob_if_spam + probs_if_ham)\n",
    "\n",
    "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))\n",
    "\n",
    "print(\"spammiest_words\", words[-10:])\n",
    "print(\"hammies_words\", words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떻게 하면 더 좋은 성능을 낼 수 있을까? \n",
    "- 메시지의 제목뿐 아니라 내용을 활용할 수 있다. 하지만 메시지의 헤더를 어떻게 처리할지 잘 고민해 봐야 한다.\n",
    "- 지금 만들어진 분류기는 단어가 학습 데이터에서 단 한 번이라도 나왔다면 모두 사용한다. 단어의 최소 빈도수 min_count를 설정해서 기준보자 적게 나온 단어를 무시할 수 있도록 분류기를 수정할 수 있다.\n",
    "- 현재는 동의어('cheap'과 'cheapest')를 고려하지 않고 메시지를 단어 단위로 자르고 있다. 간단한 어간 추출기(stemmer)를 분류기에 추가하면 비슷한 의미의 단어를 동일한 그룹으로 묶어 줄 수 있다. 예를 들어 다음과 같이 굉장히 단순한 어간 추출기를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋은 어간 추출기를 만드는 것은 굉장히 어렵고, 보통은 이미 구현된 Porter stemmer를 사용한다.\n",
    "- 우리는 모델의 변수로 '메시지가 단어 ${w}_i$를 포함한다'의 꼴만 사용했지만 다른 변수를 사용해도 무관하다. 예를 들어 tokenize 함수를 수정해서 메시지에 숫자가 포함되어 있는 경우 contains:number 같은 문자열을 반환하도록 하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
