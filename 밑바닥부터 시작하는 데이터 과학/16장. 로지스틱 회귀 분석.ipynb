{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀 분석\n",
    "어떤 사용자들이 유료 계정으로 전환될지 예측하는 문제 다시 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 문제\n",
    "데이터 과학자로서의 경력(experience), 소득(salary) 그리고 소득(salary) 그리고 유료 계정으로 등록했는지 여부(paid_account)등을 익명화해서 저장한 200명의 사용자 데이터가 있따. 이때 종속 변수(dependent variable)는 유료 계정 등록 여부다. 범주형(categorical)이므로 0(유료 계정이 아니다)과 1(유료 계정이다)의 값을 가진다. \\\n",
    "\n",
    "데이터는 각 [experience, salary, paid_account] 정보가 담긴 리스트이다. 이를 다시 우리가 필요한 형태로 바꿔보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0),(8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0),(1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),(6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),(8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),(0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),(0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),(8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0),(1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),(7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),(8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),(4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),(8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),(2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),(8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),(2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),(2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),(7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),(8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),(5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),(4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),(4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),(3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),(8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),(1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),(9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),(7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),(2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),(5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "data = [list(row) for row in tuples]\n",
    "\n",
    "xs = [[1.0] + row[:2] for row in data]  # 여기서 원소는 [1, experience, salary]\n",
    "ys = [row[2] for row in data]  # 여기서 각 원소는 paid_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 선형 회귀 분석으로 가장 좋은 모델을 찾아볼 수 있다.\n",
    "\n",
    "유료 계정 등록 여부 = $\\beta_0$ + $\\beta_1$ 경력 + $\\beta_2$ 소득 + $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "from typing import Callable\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Vector = List[float]\n",
    "Matrix = List[List[float]]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w),  \"vectors must be same length\"\n",
    "    \n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))\n",
    "\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"모든 성분을 c로 곱하기\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"x의 모든 데이터 포인트에서 평균을 뺌(평균을 0으로 만들기 위해)\"\"\"\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"각 성분별 평균을 계산\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"표준편차는 분산의 제곱근\"\"\"\n",
    "    return math.sqrt(variance(xs))\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"편차의 제곱의 평균\"\"\"\n",
    "#     assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "    \n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"모든 벡터의 각 성분들끼리 더한다.\"\"\"\n",
    "    # vectors가 비어있는지 확인\n",
    "    assert vectors, \"no vectors provided\"\n",
    "    \n",
    "    # 모든 벡터의 길이가 동일한지 확인\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "    \n",
    "    # i번째 결과값은 모든 벡터의 i번째 성분을 더한 값\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scale(data: List[Vector]) -> Tuple[Vector, Vector]:\n",
    "    \"\"\"각 열의 평균과 표준편차를 반환\"\"\"\n",
    "    dim = len(data[0])\n",
    "    \n",
    "    means = vector_mean(data)\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data] for i in range(dim))]\n",
    "    \n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data: List[Vector]) -> List[Vector]:\n",
    "    \"\"\"각 열의 평균을 0, 표준편차를 1로 볁환하면서\n",
    "    입력되는 데이터의 척도를 조절\n",
    "    편차가 없는 열은 그래로 유지\"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    # 각 벡터에 복사본을 생성\n",
    "    rescaled = [v[:] for v in data]\n",
    "    \n",
    "    for v in rescaled:\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i] / stdevs[i])\n",
    "    return rescaled\n",
    "\n",
    "def least_squares_fit(xs: List[Vector],\n",
    "                     ys: List[float],\n",
    "                     learning_rate: float = 0.001,\n",
    "                     num_steps: int = 1000,\n",
    "                     batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    오류의 제곱 합을 최소화하는 beta를 찾자.\n",
    "    모델은 y = dot(x, beta)라 가정하자.\n",
    "    \"\"\"\n",
    "    # 임의의 위치에서 출발\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start: start+batch_size]\n",
    "            batch_ys = ys[start: start+batch_size]\n",
    "            \n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "    \n",
    "    return guess\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"각 x_i의 첫 번째 항목은 1이라고 가정\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"v에서 step_size만큼 이동하기\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2a84b9fb9f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrescaled_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# [0.26, 0.43, -0.43]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrescaled_xs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cf28b6aa9569>\u001b[0m in \u001b[0;36mrescale\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     69\u001b[0m     편차가 없는 열은 그래로 유지\"\"\"\n\u001b[1;32m     70\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# 각 벡터에 복사본을 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cf28b6aa9569>\u001b[0m in \u001b[0;36mscale\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mstdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandard_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdevs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cf28b6aa9569>\u001b[0m in \u001b[0;36mstandard_deviation\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandard_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m\"\"\"표준편차는 분산의 제곱근\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cf28b6aa9569>\u001b[0m in \u001b[0;36mvariance\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#     assert len(xs) >= 2, \"variance requires at least two elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mdeviations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mde_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum_of_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeviations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "rescaled_xs = rescale(xs)\n",
    "beta = least_squares_fit(rescaled_xs, ys, learning_rate, 1000, 1)\n",
    "# [0.26, 0.43, -0.43]\n",
    "predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "\n",
    "plt.scatter(predictions, ys)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 접근법은 몇 가지 문제를 야기시킨다.\n",
    "- 예측값이 0 또는 1로 나와서 어떤 클래스에 속하는지 나타냈으면 한다. 숫자가 0과 1사이로 나와도 확률로 해석할 수 있으니 괜찮다. 가령 예측값이 0.25이면 유료 계정 사용자일 확률이 25%라고 해석할 수 있다. 하지만 선형 회귀 모델의 결괏값은 아주 큰 양수값 도는 음수값일 수 있기 때문에 해석이 어렵다. 실제로 여기서도 상당히 많은 예측값이 음수이다. \n",
    "- 선형 회귀 분석은 오류값이 x의 열과 아무런 상관 관계가 없다고 가정한다. 하지만 여기서는 경력(experience)에 대한 계수가 0.43이기 때문에 경력이 많은 사람이 유료 계정을 등록할 가능성이 더 많음을 암시한다. 이는 사용자의 경력이 많은 경우 모델이 아주 큰 예측값을 생성한다는 것을 의미한다. 하지만 실제 y의 최댓값은 1이므로. 아주 큰 예측값은(아주 큰 경험의 계수와 함께)큰 음의 오류를 발생시킨다. 즉, 이 경우 beta에 관한 추정이 상당히 편향되어 있다. \n",
    "\n",
    "이렇게 하는 대신 우리는 dot(x_i, beta)의 값이 큰 양수이면 예측값이 1에 가깝고, 큰 음수이면 예측값이 0에 가깝도록 설정해볼 수 있다. 이는 현재의 예측값에 어떤 함수를 적용하면 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 로지스틱 함\n",
    "로지스틱 회귀 분석은 로지스틱 함수(logistic function)을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x: float) -> float:\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 함수는 입력값이 양의 방향으로 커질수록 출력값이 1에 가까워진다. \\\n",
    "반대로 입력값이 음의 방향으로 커질수록 출력값이 0에 가까워진다. 그와 더불어 로지스틱 함수는 다음과 같이 간단하게 미분할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_prime(x: float) -> float:\n",
    "    y = logistic(x)\n",
    "    return y * (1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 미분식은 쓸모가 많다. 이 식으로 모델을 학습시킬 것이다.\n",
    "\n",
    "$ y_i = f(x_i \\beta) + \\epsilon_i$ \n",
    "\n",
    "여기서 $f$는 logistic 함수이다. \\\n",
    "선형 회귀 분석을 할 때 오차의 제곱 합(sum of squared error)을 최소화해서 모델 파라미터 $\\beta$를 학습했는데, 그 모델 파마리터 $\\beta$가 결국 데이터의 가능도(likelihood)를 최대화하는 값이었다는 것을 기억하는가? \n",
    "\n",
    "그런데 로지스틱 회귀 분석에서 이 두 가지 경우는 동치가 아니다. 그러므로 여기서는 경사 하강법(gradient descent)을 사용해서 가능도를 직접 최대화시켜보자. 이를 위해 가능도 함수와 그 그래디언트를 직접 계산해 보겠다.\n",
    "\n",
    "앞서 언급한 모델에서, $\\beta$가 주어졌을 때 각 $y_i$는 $f(x_i \\beta)$의 확률로 1, 1 - $f(x_i \\beta)$의 확률로 0이 되어야 한다. \n",
    "\n",
    "그리고 $y_i$의 확률 밀도 함수(probability density function)는 다음과 같다.\n",
    "\n",
    "$p(y_i|x_i, \\beta) = f(x_i \\beta)^{y_i}(1 - f(x_i \\beta))^{1 - y_i}$\n",
    "\n",
    "만약 $y_i$ 가 0이라면 이 식은 다음과 같다. \n",
    "\n",
    "$1 - f(x_i \\beta)$\n",
    "\n",
    "또한 $y_i$가 1이라면 다음과 같다.\n",
    "\n",
    "$f(x_i \\beta)$ \n",
    "\n",
    "따라서 **로그 가능도(log likelihood)** 를 최대화하는 것은 생각보다 간단하다.\n",
    "\n",
    "$logL(\\beta|x_i, y_i) = y_i logf(x_i \\beta) + (1 - y_i)log(1-f(x_i \\beta))$\n",
    "\n",
    "로그(log)함수는 단조 증가 함수이므로 로그 가능도를 최대화하는 beta는 동시에 가능도 또한 최대화하며, 그 역도 성립한다. 경사 하강법은 함수를 최소화하는데 사용하므로, 실제로는 로그 가능도의 부호를 바꾼 **네거티브(negative)** 로 그 가능도를 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w),  \"vectors must be same length\"\n",
    "    \n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_likelihood(x: Vector, y: float, beta: Vector) -> float:\n",
    "    \"\"\"데이터 포인트의 네거티브 로그 가능도\"\"\"\n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만일 데이터 포인트끼리 서로 독립이라면 데이터 전체의 가능도는 개별 테이터 포인트의 가능도의 단순 곱이 된다. 바꿔 말햐면 데이터 전체에 대한 로그 가능도는 개별 데이터 포인트의 로그 가능도의 단순 합이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def negative_log_likelihood(xs: List[Vector],\n",
    "                           ys: List[float],\n",
    "                           beta: Vector) -> float:\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "              for x, y, in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에 약간의 미적분을 이용하면 그래디언트를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"모든 벡터의 각 성분들끼리 더한다.\"\"\"\n",
    "    # vectors가 비어있는지 확인\n",
    "    assert vectors, \"no vectors provided\"\n",
    "    \n",
    "    # 모든 벡터의 길이가 동일한지 확인\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "    \n",
    "    # i번째 결과값은 모든 벡터의 i번째 성분을 더한 값\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> float:\n",
    "    \"\"\"\n",
    "    데이터 포인트의 j번째 편미분\n",
    "    여기서 i는 데이터 포인트의 인덱스를 의미\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    \"\"\"\n",
    "    데이터 포인트의 그레디언트\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j) for j in range(len(beta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_gradient(xs: List[Vector],\n",
    "                         ys: List[float],\n",
    "                         beta: Vector) -> Vector:\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                      for x, y in zip(xs, ys)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 모델 적용하기\n",
    "데이터를 학습 데이터와 테스트 데이터로 분할하고 경사 하강법(batch gradient)을 적용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "import tqdm\n",
    "\n",
    "X = TypeVar('X')  # 데이터를 표현하기 위한 일반적인 타입\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"데이터를 [prob, 1 - prob]의 비율로 나눈다.\"\"\"\n",
    "    data = data[:]  # 얕은 복사본을 만든다.\n",
    "    random.shuffle(data)  # shuffle이 리스트 내용을 바꾸기 때문\n",
    "    cut = int(len(data) * prob)  # prob을 사용하여 자를 위치를 선택하고\n",
    "    return data[:cut], data[cut:]  # 섞인 리스트를 자른다.\n",
    "\n",
    "Y = TypeVar('Y')  # 출력 변수를 표현하기 위한 일반적인 타입\n",
    "\n",
    "def train_test_split(xs: List[X],\n",
    "                    ys: List[Y],\n",
    "                    test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    # 인덱스를 생성하여 분할\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "    \n",
    "    return ([xs[i] for i in train_idxs],  # x_train\n",
    "           [xs[i] for i in test_idxs],  # x_test\n",
    "           [ys[i] for i in train_idxs],  # y_train\n",
    "           [ys[i] for i in test_idxs])  # y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rescaled_xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5279436590ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rescaled_xs' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 임의의 시작점을 정하자\n",
    "beta = [random.random() for _ in range(3)]\n",
    "\n",
    "with tqdm.trange(5000) as t:\n",
    "    for epoch in t:\n",
    "        gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "        beta = gradient_step(beta, gradient, -learning_rate)\n",
    "        loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "        t.set_description(f\"loss: {loss:.3f} beta: {beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 통해 얻은 beta는 대략 다음과 같다.\n",
    "\n",
    "[-2.0, 4.7, -4.5]\n",
    "\n",
    "이는 rescale을 통해 척도가 조절된 데이터의 계수이므로 이를 원해 데이터의 단위로 다시 변환해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# 앞장에서 만들어 놓은 함수\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"모든 성분을 c로 곱하기\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"각 성분별 평균을 계산\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"표준편차는 분산의 제곱근\"\"\"\n",
    "    return math.sqrt(variance(xs))\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"편차의 제곱의 평균\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "    \n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"모든 벡터의 각 성분들끼리 더한다.\"\"\"\n",
    "    # vectors가 비어있는지 확인\n",
    "    assert vectors, \"no vectors provided\"\n",
    "    \n",
    "    # 모든 벡터의 길이가 동일한지 확인\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "    \n",
    "    # i번째 결과값은 모든 벡터의 i번째 성분을 더한 값\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scale(data: List[Vector]) -> Tuple[Vector, Vector]:\n",
    "    \"\"\"각 열의 평균과 표준편차를 반환\"\"\"\n",
    "    dim = len(data[0])\n",
    "    \n",
    "    means = vector_mean(data)\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data] for i in range(dim))]\n",
    "    \n",
    "    return means, stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-947fe54d6e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1]\n\u001b[1;32m      3\u001b[0m                  - beta[2] * means[2] /stdevs[2]),\n\u001b[1;32m      4\u001b[0m                  \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstdevs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  beta[2] / stdevs[2]]\n",
      "\u001b[0;32m<ipython-input-10-a21f7146fa8c>\u001b[0m in \u001b[0;36mscale\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mstdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandard_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdevs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a21f7146fa8c>\u001b[0m in \u001b[0;36mstandard_deviation\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandard_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"\"\"표준편차는 분산의 제곱근\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a21f7146fa8c>\u001b[0m in \u001b[0;36mvariance\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"\"\"편차의 제곱의 평균\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"variance requires at least two elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "means, stdevs = scale(xs)\n",
    "beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1]\n",
    "                 - beta[2] * means[2] /stdevs[2]),\n",
    "                 beta[1] / stdevs[1],\n",
    "                 beta[2] / stdevs[2]]\n",
    "\n",
    "# [8.9, 1.6, -0.000288]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀의 계수와는 달리 이들의 의미를 해석하기는 쉽지 않다. 다른 모든 것이 동일하다면 경력이 1년 더 추가될 대 logistic의 입력값에 1.6이 더해진다. 다른 모든 것이 동일하다면 연봉이 $10,000 추가될 때마다 logistic의 입력값에서 2.88이 빠진다. \n",
    "\n",
    "하지만 최종 예측값을 결정하는 데는 다른 변수들의 영향도 크다. dot(beta, x_i)가 이미 충분히 크다면(즉 예측값이 이미 충분히 1에 가깝다면) 특정 변수의 입력값이 아주 많이 증가해도 예측값에는 거의 영향을 주지 못한다. 반면 dot(beta, x_i)가 0에 가깝다면 특정 변수의 값을 조금만 키워도 예측값에는 꽤 큰 영향을 미칠 수 있다.\n",
    "\n",
    "한 가지 확실한 것은, 다른 모든 것이 동일하다면 경력이 많을수록 유료 계정으로 등록할 가능성이 높고, 다른 모든 것이 동일하다면 월급이 높을수록 유료 계정으로 등록할 가능성이 낮다는 것이다.(데이터를 시각화 했을 때도 이와 유사한 결론을 얻을 수 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4 적합성(Goodness of fit)\n",
    "지금까지는 평가용으로 따로 빼 놓았던 데이터를 써볼 기회가 없었다. 예측값이 0.5를 초과할 때마다 사용자가 유료 계정으로 등록한다고 예측해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f99285119e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrue_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloegistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    prediction = loegistic(dot(beta, x_i))\n",
    "    \n",
    "    if y_i == 1 and prediction >= 0.5:  # TP: 실제 유로 계정, 예측도 유료 계정\n",
    "        true_positives += 1\n",
    "    elif y_i == 1:  # FN: 실제 유료 계정, 예측은 무료 계정\n",
    "        false_negatives += 1\n",
    "    elif prediction >= 0.5:  # FP: 실제 무료 계정, 예측은 유료 계정\n",
    "        false_positives += 1\n",
    "    else:  # TN: 실제 무료 계정, 예측도 무료 계정\n",
    "        true_negatives += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5e793cac5c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_positives\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfalse_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_positives\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfalse_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 하면 정밀도가 75%이고(유료 계정이라고 예측, 실제 유료 계정 확률이 75%), 재현율은 80%이다(유료 계정 이용자, '유료 계정' 이용자라고 예측할 확률 80%). 이 정도면 적은 데이터에 비해 나쁘지 않은 결과이다. \n",
    "\n",
    "예측값과 실제값을 시각화할 수도 있다. 이 결과를 봐도 모델링이 비교적 잘 되었음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-36c81c80f26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actual outcome\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression Predicted vs. Actual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = [logistic(dot(beta, x_i)) for x_i in x_test]\n",
    "plt.scatter(predictions, y_test, marker='+')\n",
    "plt.xlabel(\"predicted probability\")\n",
    "plt.ylabel(\"actual outcome\")\n",
    "plt.title(\"Logistic Regression Predicted vs. Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5 서포트 벡터 머신\n",
    "dot(beta_hat, x_i)의 값이 0이 되는 부분이 두 클래스의 경계면이다. 모델이 어떻게 작동하는지 확인하기 위해 이 경계면을 시각화해 볼 수 있다. \\\n",
    "이렇게 전체 파라미터 공간을 '유료 계정'과 '유료 계정이 아닌' 두 개의 부분 공간으로 나누는 경계면을 **초평면(hyperplane)** 이라고 부른다. \\\n",
    "이 초평면은 가능도를 최대화한 결과의 부산물로 얻을 수 있다. \\\n",
    "분류 문제를 푸는 다른 방법 중에는 애초에 각 클래스를 가장 잘 분류하는 초평면을 찾는 것을 목적으로 하는 것도 있다. **서포트 벡터 머신(support vector machine, SVM)** 이 그중 하나인데, 이는 각 클래스 안의 초평면과 가장 가까운 점의 거리를 최대화하는 방식으로 초평면을 찾는다. \n",
    "\n",
    "초평면을 찾는 것은 생각보다 꽤 복잡한 최적화 방법론을 필요로 한다. 그뿐 아니라, 데이터를 분류할 수 있는 초평면이 아예 존재하지 않을 수도 있다. 앞서 다룬 유료 회원 문제에서조차 유료 회원과 무료 회원 사이를 완벽하게 가를 수 있는 직선이 존재하지 않는다.\n",
    "\n",
    "이 문제를 우회하는 방법 중 하나는 데이터가 선형적으로 구분될 수 있는, 조금 더 차원이 높은 공간으로 데이터를 변환하여 보내는 것이다.\n",
    "\n",
    "단순한 1차원 데이터를 예로 들면, negatives, positives를 구분할 수 있는 초평면이 존재하지 않는다. 하지만 이 데이터 포인트를 (x, x ** 2)로 매핑(mapping)하여 2차원 공간으로 보낸다면 데이터를 분리할 수 있는 초평면을 찾을 수 있게 된다.\n",
    "\n",
    "이를 보통 **커널 트릭(kernel trick)** 이라고 표현한다. 데이터 포인트를 더 높은 차원의 공간으로 직접 매핑하기 보다는(데이터가 많거나 복잡하면 계산량이 무척 많을 수 있기 때문에) **'커널(kernel) 함수'** 로 더 높은 차원에서의 내적을 계산하고 그것으로 초평면을 찾는다.\n",
    "\n",
    "최적화 코드를 제대로 작성할 줄 아는 전문가에게 의지하지 않고 직접 서포트 벡터 머신을 작성하는 것은 굉장히 힘든 일일뿐더러 별로 좋은 생각이 아닐 수 있으니, 서포트 벡터 머신에 대한 논의는 이쯤에서 마무리하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
