{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝\n",
    "**딥러닝(deep learning)** 은 (한 개 이상의 은닉층을 지닌) '깊은' 신경망을 의미한다. \n",
    "\n",
    "요즘에는 다양한 신경망 구조를 모두 아우르는 용어가 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.1 텐서\n",
    "1차원 배열은 벡터, 2차원 배열은 행렬으로 명명하였다. 복잡한 신경망 구조에서는 더 고차원 배열이 필요한데 대부분의 신경망 라이브러리의 명칭을 따라서 n차원 배열을 **텐서(tensor)**라고 하겠다.\n",
    "\n",
    "텐서를 리스트로 정의해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 텐서의 크기를 반환해 주는 도우미 함수(helper function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서마다 차원의 수가 다양하기  때문에 재귀적으로 텐서를 살펴봐야 한다. \n",
    "\n",
    "밑의 함수는 1차원 벡터부터 재귀적으로 고차원의 텐서까지 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    만약 tensor[0]이 리스트라면 고차원 텐서를 의미\n",
    "    그러지 않으면 1차원 벡터를 의미\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 함수를 사용해 텐서 안의 모든 값의 합을 반환해주는 tensor_sum 함수를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"텐서 안의 모든 값의 합을 반환\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # 벡터의 경우, 파이썬 기본 함수인 sum을 사용\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)  # 벡터가 아닌 경우, 각 행별 tensor_sum을 호출하고\n",
    "                  for tensor_i in tensor)  # 결과값을 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tensor_sum([1,2,3]) == 6\n",
    "assert tensor_sum([[1,2], [3,4]]) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 else statement에 tensor_sum이 재귀적으로 사용되었다. \n",
    "\n",
    "매번 이렇게 재귀적으로 구현하는 것은 번거로우니 이러한 기능을 대신해 주는 도우미 함수를 만들자. 먼저 텐서 안의 모든 값에 일괄적으로 함수를 적용할 수 있게 해주는 함수를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"텐서 안의 모든 값에 f를 적용\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 도우미 함수를 사용하면 주어진 크기와 동일한 0 텐서를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert zeros_like([1,2,3]) == [0, 0, 0]\n",
    "assert zeros_like([[1,2], [3, 4]]) == [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 두 텐서의 대칭되는 값에 일괄적으로 함수를 적용할 수 있게 해주는 함수를 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                  t1: Tensor,\n",
    "                  t2: Tensor) -> Tensor:\n",
    "    \"\"\"두 텐서의 대칭되는 t1과 t2에 일괄적으로 함수를 적용\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "               for t1_i, t2_i in zip(t1, t2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "assert tensor_combine(operator.add, [1,2,3], [4,5,6]) == [5,7,9]\n",
    "assert tensor_combine(operator.mul, [1,2,3], [4,5,6]) == [4,10,18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.2 층 추상화\n",
    "18장에서는 각각 sigmoid(dot(weights, inputs))를 계산해 주는 두 층으로 구성된 단순한 신경망을 만들어 보았다.\n",
    "\n",
    "이러한 구조는 실제 뉴런이 작동하는 방식과 비슷하지만, 딥러닝에서는 다양한 신경망 구조를 사용할 것이다. 가령 각 뉴런이 이전 입력값을 기억하도록 만들거나 시그모이드 대신 다른 활성화 함수(activation function)를 사용할 것이다. 또한 두 층보다 더 깊은 신경망을 만들 것이다. (feed_forward 함수로 두 층 이상의 신경망을 만드는 것은 구현했지만 그래디언트를 계산하는 것은 아직 구현하지 않았다.)\n",
    "\n",
    "이번 장에서는 이러한 다양한 신경망을 구현해 볼 것이다. 가장 핵심적인 **추상화** 는 신경망의 각 층을 나타내는 Layer이다. Layer에서는 입력값에 특정 함수를 적용하거나 역전파를 할 수 있어야 한다. 18장에서 구현한 신경망은 '선형'층 위에 '시그모이드' 층으로 구성된 신경망이라고 볼 수 있다.\n",
    "\n",
    "새로운 용어라 어색할 수도 있지만 이렇게 받아들이는 것이 다양한 신경망 구조를 이해하는 데 도움이 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    딥러닝 신경망은 Layer들로 구성되어 있다.\n",
    "    각 Layer별로 순방향으로 입력값에 어떤 계산을 하고\n",
    "    역방향으로 그래디언트를 전파해야 한다.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        타입이 명시되어 있지 않은 것을 유의하자.\n",
    "        입력층과 출력값의 타입의 제한하지 않을 것이다.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "            \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        역방향에서도 그래디언트의 타입을 제한하지 않을 것이다.\n",
    "        메서드를 호출할 때 유의하자.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        해당 층의 파라미터를 반환\n",
    "        기본적으로 아무것도 반환하지 않을 것이다.\n",
    "        만약 특정 층에서 반환할 파라미터가 없다면 구현할 필요가 없다.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        params()처럼 그래디언트를 반환\n",
    "        \"\"\"\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward, backward 메서드는 곧 구현할 것이다. 그리고 경사 하강법으로 신경망의 파라미터를 학습하기 위해 각 층의 파라미터와 그래디언트를 반환해 줄 수 있어야 한다. \n",
    "\n",
    "시그모이드 층과 비슷한 층에서는 파라미터를 업데이트할 필요가 없기 때문에 이러한 경우를 위해 기본값 설정 또한 해주었다.\n",
    "\n",
    "먼저 시그모이드 층을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        입력된 Tensor의 모든 값에 sigmoid를 계산\n",
    "        backpropagation을 위해 결괏값을 저장\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "    \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad, self.sigmoids, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 가지 유의할 점이 있다. 역전파를 위해 순방향에서 계산되는 시그모이드 값을 모두 저장할 것이다. 앞으로 층의 순방향에서 계산되는 값은 대부분 저장할 것이다. \n",
    "\n",
    "두 번째로 sig * (1 - sig) * grad가 어디서 나왔는지 궁금할 것이다. 이 값은 18장에서 다룬 미적분과 연쇄법칙으로 계산된 output * (1 - output) * (output - target)이다.]\n",
    "\n",
    "마지막으로 tensor_apply와 tensor_combine 함수를 사용했다. 앞으로 구현할 모든 층에서는 기본적으로 두 함수를 사용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.3 선형 층\n",
    "18장에서 구현한 신경망을 기반으로 뉴런의 dot(weights, inputs)를 나타내는 선형 층(linear layer)을 구현할 수 있다.\n",
    "\n",
    "선형 층의 초깃값은 임의로 생성할 것이다.\n",
    "\n",
    "사실 임의로 파라미터의 초깃값을 설정하는 것은 매우 중요하다. 파라미터의 초깃값에 따라 신경망의 학습 속도가 (혹은 학습 여부 자체가) 결정된다. 만약 초깃값이 너무 크면 활성화 함수의 그래디언트는 0에 가깝게 된다. 그래디언트가 0에 가까운 파라미터는 경사 하강법으로 학습을 할 수 없게 된다.\n",
    "\n",
    "따라서 파라미터를 임의로 생성해 주는 세 가지 방법을 구현할 것이다. 먼저 random.random()으로 균등 분포를 구현하여, 파라미터의 초깃값을 0과 1 사이 임의의 값으로 설정할 것이다. 두 번째 방법으로 표준정규분표에서 임의의 초깃값을 생성할 것이다. 마지막으로 딥러닝에서 자주 쓰이는 Xavier 초기화에서는 평균이 0이고 편차가 2 / (입력 값의 개수 + 출력 값의 개수)인 정규분포에서 임의의 초깃값을 생성할 것이다. 세 가지 방법 모두 random_uniform과 random_normal 함수로 구현할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def inverse_normal_cdf(p: float,\n",
    "                      mu: float = 0,\n",
    "                      sigma: float = 1,\n",
    "                      tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"이진 검색을 사용해 역함수를 근사\"\"\"\n",
    "    # 표준정규분포가 아니라면 표준정규분포로 변환\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    \n",
    "    low_z = -10.0  # normal_cdf(-10)은 0에 근접\n",
    "    hi_z = 10.0  # normal_cdf(10)은 1에 근접\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2  # 중간 값\n",
    "        mid_p = normal_cdf(mid_z)  # 중간 값의 누적분포 값을 계산\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z  # 중간 값이 너무 작다면 더 큰 값들을 검색\n",
    "        else:\n",
    "            hi_z = mid_z  # 중간 값이 너무 크다면 더 작은 값들을 검색\n",
    "    \n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_normal(*dims: int, \n",
    "                 mean: float = 0.0,\n",
    "                 variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "               for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "               for _ in range(dims[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10)) == [5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 random_tensor 함수로 감싸주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unkown init: {init}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 선형 층을 구현할 준비가 끝났다. 먼저 입력값의 차원 수(각 뉴런별 파라미터 개수), 출력값의 차원 수(뉴런의 개수), 초기화 방법을 명시해 줘야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w),  \"vectors must be same length\"\n",
    "    \n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self,\n",
    "                input_dim: int,\n",
    "                output_dim: int,\n",
    "                init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        output_dim개의 뉴런과 각 뉴런별 input_dim개의 파라미터로 (편향 제외)\n",
    "        구성된 층\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # self.w[o]는 o번째 뉴런의 파라미터\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "        \n",
    "        # self.b[o]는 o번째 뉴런의 편향\n",
    "        self.b = random_tensor(output_dim, init=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 층의 forward 메서드는 쉽게 구현할 수 있다. 뉴런별 한 개의 값을 벡터로 저장할 것이다. 각 뉴런의 입력값과 파라미터를 곱하고 편향을 더해주면 출력값이 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input: Tensor) -> Tensor:\n",
    "    # 역방향을 위해 input을 저장\n",
    "    self.input = input\n",
    "    \n",
    "    # 뉴런의 결괏값을 벡터로 반환\n",
    "    return [dot(input, self.w[o]) + self.b[o]\n",
    "           for o in range(self.output_dim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward 메서드는 조금 더 복잡하지만, 미적분을 알고 있다면 금방 이해할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, gradient: Tensor) -> Tensor:\n",
    "    # 각 b[o]는 output[o]에 더해진다.\n",
    "    # 즉, b의 그래디언트는 output의 그래디언트과 동일하다는 것을 의미\n",
    "    self.b_grad = gradient\n",
    "    \n",
    "    # 각 w[o][i]를 input[i]에 곱하고 output[o]에다 더해 준다.\n",
    "    # 즉, 그래디언트는 input[i] * gradient[o]\n",
    "    self.w_grad = [[self.input[i] * gradient[o]\n",
    "                   for i in range(self.input_dim)]\n",
    "                   for o in range(self.output_dim)]\n",
    "    # input[i]에 각 w[o][i]를 곱하고\n",
    "    # output[o]에 더해 주기 때문에 그래디언트는 w[o][i] * gradient[o]를\n",
    "    # 모두 더해 준 값\n",
    "    return [sum(self.w[o][i] * gradien[o] for o in range(self.output_dim))\n",
    "           for i in range(self.input_dim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 params와 grads 메서드를 만들어 보자. 선형 층에는 두 개의 파라미터와 그래디언트가 있다는 것을 기억하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(self) -> Iterable[Tensor]:\n",
    "    return [self.w, self.b]\n",
    "\n",
    "def grads(self) -> Iterable[Tensor]:\n",
    "    return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.4 순차적인 층으로 구성된 신경망\n",
    "신경망을 순차적인 층으로 구성되어 있다고 생각해 볼 수 있다. 그렇다면 여러층을 하나의 층으로 표현할 수도 있을 것이다. 즉, 하나의 신경망 자체를 Layer의 메서드를 활용해서 하나의 층으로 표현해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    하나의 Layer에는 실제 여러 층이 포함되어 있다. \n",
    "    각 층의 출력값이 \n",
    "    다음 층의 입력값이 된다는 것을 꼭 이해하고 넘어가자\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"순차적으로 각 층의 입력값을 전파\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"역방향으로 각 층의 그래디언트를 전파\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "    \n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"각 층별 파라미터를 반환\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"각 층별 그래디언트를 반환\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18장에서 XOR 게이트를 위해 만들었던 신경망을 다음과 같이 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_net = Sequential([\n",
    "    Linear(input_dim=2, output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2, output_dim=1),\n",
    "    Sigmoid()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.5 손실 함수와 최적화\n",
    "지금까지는 손실함수(loss function)와 그래디언트 함수를 직접 명시하였다. 이번에는 다양한 손실 함수를 살펴볼 것이며, 손실 함수와 그래디언트 계산을 loss라는 클래스로 추상화할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"예측값이 얼마나 정확한가?\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
